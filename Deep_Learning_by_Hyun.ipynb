{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "(원본) 혀누를 위한 Deep Learning",
      "provenance": [],
      "collapsed_sections": [
        "aFbtD4uKrv3W",
        "t88oG-qc-16q",
        "qO3dY4c56Gfw",
        "bJWmUSDlAyOK",
        "gTEdK2mn6fBk",
        "YfPXOpIa6fBh",
        "rqjDn-746e_d",
        "mT9PWpxM6e_O",
        "vg6i6rTL6e_F",
        "K_GS6M6WXx2W",
        "vTuW-lEIEhS7",
        "JZ0xmkTvLFQL",
        "BzTaKX4BKK_1",
        "8G8ym8Ra8u_v",
        "nU26nC0WFJfd",
        "lZ6HbDW9C905",
        "Nc2C22R1nDvn",
        "FvfPNPNXFrE-",
        "iOHgaMFVFspK",
        "irdcEEPdGjHH",
        "jOdk6QUccAwv",
        "4OSPeAmOhiOi",
        "nMtUYXOThd3v",
        "_bCY9Vv-jPWc",
        "GPrTfiO3j0gz",
        "UfvhxoLWmVPV",
        "2gajRY7FUUSv",
        "DGaQBIA8e4K-",
        "1PrBCXstnH0G",
        "KcKv7SepaVit",
        "vPmMl0PFahTU",
        "5ZEoizR0ApBl",
        "A8pURKEJPNKh",
        "8i5iJALPF4t1",
        "9pEZ84Kz8zfi",
        "1DFl_5y8H98K",
        "3dBC99juVk-j",
        "ZjH0kB6VZ_ta",
        "jQJZdFO4niOq",
        "a3eRh91BgIAZ",
        "YO2oZH7Twwsu",
        "9tyDWij89PWj",
        "5Q_1BKW792xq",
        "_7WiUoh7cYG6",
        "WvcNk-e3d3PP",
        "xUvS_mYYAsPV",
        "VUwJK1wC9nos",
        "E16QxFTBUrv_"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENYso35hnbpA",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# ┗(•̀へ •́ )ﾉ Hyun's Deep Learning\n",
        "---\n",
        "자 Deep Learning을 공부해봅시다 (´∇｀)  \n",
        "\n",
        "## Authored by. Hyun\n",
        "![미니언즈!](https://post-phinf.pstatic.net/MjAxNzA3MTBfMjIg/MDAxNDk5NjcxOTY1NDQw.Kz07JXiZg6AT6Y4PAZY7ubUNAr7rbDinLwFGuS0OOxcg.WVhpo8yfybUh0qImMGNAo1ucSUPuNOvQyzlO_vKlAlkg.JPEG/%EC%98%81%EC%A7%84%EC%9C%845.jpg?type=w1200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFbtD4uKrv3W",
        "colab_type": "text"
      },
      "source": [
        "# ┗(•̀へ •́  )ﾉ **이것만 돌리면 다 돌아간다~~~**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRO8gtPDrySn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # 기본\n",
        "# import numpy as np\n",
        "# from numpy.linalg import inv\n",
        "# from numpy.linalg import solve\n",
        "# import random\n",
        "\n",
        "# import pandas as pd\n",
        "# from pandas import Series, DataFrame\n",
        "# import pandas_profiling\n",
        "# import openpyxl\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# import urllib\n",
        "# import matplotlib.dates as mdates\n",
        "# from matplotlib.dates import bytespdate2num\n",
        "# import matplotlib.ticker as ticker\n",
        "# from matplotlib import font_manager, rc\n",
        "# from matplotlib import style\n",
        "# import seaborn as sns\n",
        "\n",
        "# import time\n",
        "\n",
        "# import itertools\n",
        "# import re\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# # 시계열 데이터 처리\n",
        "# import calendar\n",
        "# import dateutil\n",
        "# from dateutil.parser import parse\n",
        "# import datetime\n",
        "\n",
        "# # Network 분석\n",
        "# import networkx as nx\n",
        "\n",
        "# # 지도시각화\n",
        "# import folium\n",
        "# from folium import plugins\n",
        "# import html\n",
        "# import json\n",
        "# import geopy\n",
        "# from geopy.geocoders import Nominatim\n",
        "# import os\n",
        "# import requests\n",
        "# import ipywidgets\n",
        "# from IPython.display import Image\n",
        "# from ipywidgets import interact\n",
        "\n",
        "# # Clustering\n",
        "# from sklearn.cluster import KMeans\n",
        "# from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "\n",
        "# # Factor analysis\n",
        "# from factor_analyzer import FactorAnalyzer\n",
        "\n",
        "# # Machine learning 용도\n",
        "# from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import r2_score\n",
        "#from sklearn.metrics import classification_report\n",
        "# from sklearn.model_selection import PredefinedSplit\n",
        "# from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
        "# from sklearn.metrics import (roc_curve, auc, accuracy_score, roc_auc_score)\n",
        "# from sklearn.cluster import KMeans\n",
        "# from sklearn.decomposition import PCA\n",
        "# from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "# from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# from sklearn.preprocessing import RobustScaler\n",
        "# from sklearn.datasets import make_blobs\n",
        "# import scipy.cluster.hierarchy as shc\n",
        "# from sklearn.cluster import AgglomerativeClustering\n",
        "# from xgboost import XGBRegressor\n",
        "# from catboost import CatBoostRegressor\n",
        "# from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
        "# from lightgbm import LGBMClassifier, LGBMRegressor\n",
        "# import lightgbm as lgb\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn import metrics\n",
        "\n",
        "# # Deep learning 용도\n",
        "# import torch\n",
        "# import torchvision.datasets as dsets\n",
        "# import torchvision.transforms as transforms\n",
        "# import random\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import torch.optim as optim\n",
        "\n",
        "# # Wordcloud\n",
        "# import konlpy\n",
        "# from konlpy.tag import Okt\n",
        "# import collections\n",
        "# from collections import Counter\n",
        "# from wordcloud import WordCloud, STOPWORDS as stopwords\n",
        "# from PIL import Image, ImageFilter\n",
        "# from wordcloud import ImageColorGenerator\n",
        "# import pickle\n",
        "\n",
        "# # 한글 사용하기\n",
        "# import matplotlib.font_manager as fm\n",
        "# from matplotlib import rc\n",
        "# font_name = fm.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
        "# rc('font', family=font_name)\n",
        "\n",
        "\n",
        "# # 크롤링\n",
        "#  from selenium import webdriver\n",
        "# import requests\n",
        "# from bs4 import BeautifulSoup\n",
        "\n",
        "# # 기타\n",
        "# from urllib.request import urlopen\n",
        "# from tqdm import tqdm\n",
        "# import time\n",
        "# from zeep import Client\n",
        "# from collections import namedtuple\n",
        "# import sqlite3\n",
        "\n",
        "# # 한 번에 matplotlib 그림 띄우기\n",
        "# %matplotlib inline    \n",
        "# %config InlineBackend.figure_format = 'retina'  #%matplotlib 뒤에 써주면 그래프를 더 높은 해상도로 보여줌\n",
        "\n",
        "# # 설정 관련\n",
        "# pd.set_option('display.max_columns', 200)\n",
        "# pd.set_option('display.max_rows', 200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t88oG-qc-16q",
        "colab_type": "text"
      },
      "source": [
        "# 목차"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhANASX2-28p",
        "colab_type": "text"
      },
      "source": [
        "## 1. Deep Learning이 무엇일까???\n",
        "## 2. Deep Learning 관련 용어, 메소드 모음\n",
        "## 3. Linear Model (with Pytorch)\n",
        "## 4. AutoEncoder\n",
        "## 5. DNN (Deep Neural Network)\n",
        "## 6. CNN (Convolutional Neural Network)\n",
        "## 7. NLP (Natural Language Processing)\n",
        "## 8. RNN (Recurrent Neural Network)\n",
        "## 9. Transfer Learning\n",
        "## 10. GAN (Generative Adversial Network)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO3dY4c56Gfw",
        "colab_type": "text"
      },
      "source": [
        "# 1. Deep Learning이 무엇일까???"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IknJz98B6eFD",
        "colab_type": "text"
      },
      "source": [
        "![](https://www.filepicker.io/api/file/S4choBT5RyPBKeLqlWTG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ_B_6jK2O1L",
        "colab_type": "text"
      },
      "source": [
        "![](https://lh3.googleusercontent.com/HK5QOxzeXCF0CMWSTae2OkxXmxETt0lNlyTYrYhVTiNrGxFr33lLgCnjfGiSSQgVVmJhjlkx4gM8eo71BBV6HW_IAo1b-kzqJL4ZLcqeiUhXIYin0vjeTjxrJOQWRmNSve6TX-er2RQ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2wWk6km_CWp",
        "colab_type": "text"
      },
      "source": [
        "- 머신러닝(이하 ML)의 일부로 ML과 다르게 스스로 학습이 가능하다는 장점이 있다!\n",
        "- 크게 Deep Neural Network(DNN), Convolutional Neural Network(CNN), Recurrent Neural Network(RNN), Generative Adversarial Network(GAN) 등이 있다.\n",
        "- 간단하게 정리하면\n",
        "  1. DNN: Aritifical Nerual Network에서 hidden layer가 2개 이상인 모델\n",
        "  2. CNN: Convolution(합성곱) 연산과 Filter를 활용하여 이미지의 특징을 추출해내는 모델\n",
        "  3. RNN: 순서가 있는 데이터(문장, 주식종가, 음성인식 등)에서 특징을 추출해내는 모델\n",
        "  4. GAN: 가짜의 이미지를 만들어내는 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D_z5X14P1rd",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Deep learning method\n",
        "![](https://i.stack.imgur.com/b4sus.jpg)\n",
        "예시)\n",
        "- one to one: 기존 Neural Network\n",
        "- one to many: 이미지 -> 문장. 이미지를 가져와 이를 설명하는 문장을 출력\n",
        "- many to one: 문장 -> 감정 or 시계열 데이터 분석. 문장을 분석하여 감정을 출력\n",
        "- many to many: 영상 -> 문장 or 기계번역. 비디오 영상의 변화에 대한 분석 결과\n",
        "\n",
        "## 추가로 하면 좋을 것들\n",
        "- Classification: DenseNet, SENet, MobileNet, SqueezeNet, AutoML(NAS, NASNet)\n",
        "- Detection: Latest Object Detecetion 검색해봐라\n",
        "- Tracking: MDNet, GOTURN, CFNet, ROLO, Tracking the Untrackable\n",
        "- Segmentation: FCN, U-Net, Mask RCNN\n",
        "- Image Captioning\n",
        "- Super Resolution\n",
        "- Generative model(AutoEncoder, GAN)\n",
        "- OpenPose\n",
        "- SENet (2017 ILSVRC 우승 알고리즘)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJWmUSDlAyOK",
        "colab_type": "text"
      },
      "source": [
        "# 2. Deep Learning 관련 용어, 메소드 모음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu8qZChTmcZK",
        "colab_type": "text"
      },
      "source": [
        "## Deep learning 용어모음\n",
        " ```용어참조``` 부분"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epC3tuHrwRmF",
        "colab_type": "text"
      },
      "source": [
        "`A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxHP4wuyA0WY",
        "colab_type": "text"
      },
      "source": [
        "> ### Activation function\n",
        "- 신경망의 출력을 결정하는 식\n",
        "![Activation function](https://upload.wikimedia.org/wikipedia/commons/6/60/ArtificialNeuronModel_english.png) \n",
        "- Non-linear Activation Function을 쓰는 이유는, 활성화 함수가 선형일 경우 Perceptron이 여러 개라도, 즉, Layer을 깊게 쌓더라도 결국 선형 변환이기 때문.\n",
        "- Act_fun을 쓰는 이유\n",
        "  - input 사이의 non-linear 관계를 잡는다\n",
        "  - input을 유용한 output으로 바꿔준다.\n",
        "\n",
        "> ### Adam optimizer\n",
        "- 기울기의 일차 모멘트(평균), 이차 모멘트(분산)을 계산하여 변수를 업데이트하는 방식\n",
        "\n",
        "> ### Batch Normalization\n",
        "- Internal Covariate Shift(내부 공변량 변화.하나의 hidden layer에 여러 범위의 입력이 들어오는 현상)를 해결하기 위해 사용\n",
        "- 개념: 각 layer마다 normalization하는 layer을 둬서 변형된 분포가 나오지 않도록 방지하는 것. Batch마다 normalization을 해주겠다는 것\n",
        "- Overfitting을 방지하는 용도로 사용됨\n",
        "- DenseNet 구조: batch normalization -> activation function -> convolution\n",
        "- hidden layer마다 batch normalization을 넣어주면 internal cavariate shift를 방지하면서 더 큰 lr의 사용도 가능해짐.\n",
        "- batch normalization은 학습 시 배치 단위의 평균, 분산을 받아 이동평균, 이동분산을 저장한 뒤 test할 때 구해놓은 평균, 분산으로 정규화를 한다.\n",
        "\n",
        "> ### Batch size\n",
        "- Batch size: data를 나누는 것\n",
        "\n",
        "> ### Backpropagation(역전파)  \n",
        "[김성훈 교수님의 강의](https://www.youtube.com/watch?v=573EZkzfnZ0&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm&index=27)\n",
        "- loss를 각 변수들로 미분하여 기울기를 구하고 chain rule을 이용하여 입력 단까지 다시 전달하는 과정\n",
        "- 입력 X가 들어왔을 때, Neural Network를 통해서 output Y를 구하는데 loss를 계산해서 weight의 미분값을 계산해서 loss를 최소화하도록 weight를 업데이트하는 방식\n",
        "- 자유투를 던지는 과정은 순전파 과정(Feed Forward)라고 할 수 있고, 던진 공이 어느 지점에 도착했는지를 확인하고 던질 위치를 수정하는 과정이 역전파(Backpropagation)이다.\n",
        "- 내 생각: loss에 각 weight별로 어느 정도 영향을 미쳤는지를 고려했는지 알기 위해서 사용하는 방법\n",
        "\n",
        "> ### Binary cross entropy\n",
        "- $BCE(x) = -\\frac{1}{N} \\sum_{i=1}^N y_i \\log\\big(h(x_i; \\theta)\\big) + (1-y_i) \\log\\big(1- h(x_i; \\theta)\\big)$\n",
        "- 두 개의 class 중 하나를 예측하는 task에 대한 cross entropy의 special case\n",
        "\n",
        "> ### BPTT(Backpropagation throught time)\n",
        " - loss를 input과 hidden layer 사이의 gradient로 미분하여 loss에 대한 각각의 비중을 구해 업데이트하면 됨.\n",
        "  \n",
        "> ### CBOW(continuous bag-of-words)\n",
        "- embedding 방식 중 하나\n",
        "- 주변 단어들로부터 가운데 들어갈 단어가 나오도록 embedding하는 방식\n",
        "- 이렇게 하면 주위 단어들과 특정 단어와의 관계가 학습이 되고, 문장 또는 문서의 모든 문장을 위와 같은 방식으로 학습하면 그 문서에서 사용한 단어들이 의미적(semantcally)으로 embedding된다.\n",
        "\n",
        "> ### Convolution\n",
        "- 이미지 위에서 stride 값 만큼 filter(kernel)을 이동시키면서 겹쳐지는 부분의 각 원소의 값을 곱해서 모두 더한 값을 출력하는 연산\n",
        "  - stride: filter가 한번에 얼마나 이동할 것인가\n",
        "  - padding: 일정한 크기의 층으로 이미지를 감싸는 것. 입력 이미지에서 충분한 특성을 뽑을 수 있기 때문에 padding을 사용!!!\n",
        "- input type: Tensor형태이어야 함\n",
        "- input shape: (N x C x H x W) = (batch_size, channel, height, width)\n",
        "- $$Output size = \\frac{input size- filter size + (2 * padding)}{Stride} + 1$$\n",
        "\n",
        "> ### Cost function\n",
        "- Cost function은 loss function의 합이다. 즉 entire data set을 다루는 것이다.\n",
        "- 순간순간의 loss를 판단할 땐 loss function, 학습이 완료된 후에는 cost function!\n",
        "- A loss function **is a part of** a cost function **which is a type of** an objective function!!!\n",
        "\n",
        "> ### Cross-entropy  \n",
        "- $H_{p,q}(X) = - \\sum_{i=1}^N p(x_i) \\log q(x_i)$\n",
        "- 목표로 하는 최적의 확률분포 p와 이를 근사하려는 확률분포 q가 얼마나 다른지 측정하는 방법. 즉 원래 p였던 분포를 q로 표현했을 때 얼만큼의 비용이 드는지 측정하는 것\n",
        "- p는 true probability, true label에 대한 분, q는 현재 예측모델의 추정값에 대한 분포\n",
        "- Sigmoid 활성함수를 이용한 이진분류의 cost 함수를 MSE로 정의할 때보다 예측이 잘못될수록 cost가 더 크게 증가하여 학습 면에서도 cross entropy를 사용\n",
        "\n",
        "> ### Cross-entropy loss\n",
        "- categorical output 예측에 많이 쓰임\n",
        "\n",
        "> ### Cosine similarity\n",
        "- 의미적 연산의 대표적인 지표\n",
        "- 두 vector의 inner product으로 계산.\n",
        "- one-hot vector는 inner product이 0이 나와서 cosine similarity를 구할 수 없음\n",
        "- 이를 해결하기 위한 것이 embedding\n",
        "\n",
        "> ### cuDNN\n",
        "- CUDA를 이용해 딥러닝 연산을 가속해주는 라이브러리\n",
        "\n",
        "> ### CUDA\n",
        "- 엔비디아가 GPU를 통한 연산을 가능하게 만든 API 모델\n",
        "\n",
        "> ### Data augmentation\n",
        "- 데이터를 늘리는 방법으로 이미지에서 많이 사용\n",
        "- 간단하게 말하면 이미지를 돌리거나 뒤집기만 해도 데이터의 수를 늘리는 효과를 가져옴\n",
        "\n",
        "> ### Dropout\n",
        "- overfitting을 방지하는 방법 중 하나 (e.g. train data 늘리기, feature 줄이기, regularization)\n",
        "- 학습을 진행하면서 node, 뉴런을 무작위로 껐다 켰다 하는 것.모델의 수용력을 줄임으로써 overfitting을 방지\n",
        "- Network ensemble 효과를 얻을 수도 있음 -> 학습 때 생기는 낮은 수용력의 모델들이 test할 때는 드롭을 하지 않고 합쳐지기 때문 -> 차이를 맞춰주기 위해서 test 시에는 drop 확률을 곱해준다.\n",
        "\n",
        "> ### e (자연상수, exponential)\n",
        "- $\\lim_{n \\to \\infty} (1+\\frac{1}{n})^n$\n",
        "- 2.718281828459046..\n",
        "- \"성장\". 자연의 연속한 성장을 표현하기 위해 만든 수\n",
        "- 100%의 성장률을 가지고 1회 연속성장 할 때, 가질 수 있는 최대 성장량을 의미\n",
        "- 마법의 돼지 저금통을 생각!!\n",
        "- 예시) 전세계 인구가 x% 성장한다면 1년 뒤에 인구가 몇 명이 될지 e를 이용하면 정확하게 유추가 가능. 이 외 많은 부분에서 자연의 연속성장을 예측할 때 e를 쓸 수 있다.\n",
        "\n",
        "> ### Embedding\n",
        "- One-hot vector의 의미적 연산과 확장성의 한계(inner product이 항상 0이 나오기에 단어, 문장 간의 의미적 차이나 유사도를 구하는 것이 불가능)를 극복하기 위한 방법\n",
        "- 간단하게 말하면 단어, 알파벳 같은 기본 단위 요소들을 일정한 길이를 가지는 vector 공간에 투영하는 것\n",
        "- 단어를 벡터화하는 것이 word2vec\n",
        "- 대표적으로 CBOW, skip-gram\n",
        "\n",
        "> ### Entropy\n",
        "- 불확실성에 대한 척도.\n",
        "- Entropy 함수  \n",
        "  $H_p(X) = \\mathbb{E}\\big[I(X)\\big] = \\mathbb{E} \\big[ \\log (\\frac{1}{p(X)}) \\big] = -\\sum_{i=1}^{n} p(x_i)\\log(p(x_i))$  \n",
        "  C는 범주의 갯수, q는 사건의 확률질량함수(probability mass function)\n",
        "- 예측이 어려울수록 정보의 양은 더 많아지고 엔트로피는 커진다.\n",
        "- 확률적으로 발생하는 사건에 대한 **정보량의 평균**. 놀람의 정도를 나타낸다고 볼 수 있다.\n",
        "\n",
        "> ### Epoch\n",
        "- one epoch: 모든 training example을 도는 것!\n",
        "\n",
        "> ### Forward propagation\n",
        "- Neural Network에 입력값이 들어와 여러 개의 hidden layer을 순서대로 거쳐 결괏값을 내는 과정\n",
        "- $y = w_4(f(w_3(f(w_2(f(w_1*x+b_1))+b_2))+b_3))+b_4$  \n",
        "  이 과정이 forward propagation\n",
        "\n",
        "> ### Fully connected layer\n",
        "- 이전 layer의 모든 node가 다음 layer 모든 node에 연결된 layer를 fully connected layer(FC layer)라고 함.\n",
        "\n",
        "> ### Gradient\n",
        "- Parameter들의 편미분계수, 기울기\n",
        "\n",
        "> ### Gradient descent\n",
        "- $W_{t+1} = W_t - lr*\\frac{\\delta loss}{\\delta w}$\n",
        "- loss에서 weight가 차지하는 비중만큼 기존 weight를 바꿔주는 방법\n",
        "- $W_{t+1} = W_t - lr*\\frac{\\delta loss}{\\delta w}-lr*\\lambda*w$\n",
        "- gradient descent에 가중치 부식을 넣은 것\n",
        "\n",
        "> ### Gradient exploding\n",
        "- gradient vanishing과 반대\n",
        "- exploding은 gradient가 너무 크게 계산되어서 발생하는 문제\n",
        "- Solution\n",
        "  - Batch normalization\n",
        "  - Chaning activation function\n",
        "  - careful initialization\n",
        "  - small learning rate\n",
        "\n",
        "> ### Gram matrix\n",
        "- Gram matrix란 내적이 정의된 공간에서 벡터 $v_1$, $v_2$, ..., $v_n$이 있을 때 가능한 모든 경우의 내적을 행렬로 나타낸 것.\n",
        "- 벡타 하나하나 간의 내적으로 계산하지 않고 행렬곱으로 계산하면 한 번에 Gram matrix를 구할 수 있다.\n",
        "\n",
        "> ### Hidden state\n",
        "- RNN에서 출력되지 않고 다음 cell로 전달되는 값\n",
        "\n",
        "> ### Hyperbolic Tangent\n",
        "- $tanh(x) = \\frac{e^x-e^-x}{e^x+e^-x}$\n",
        "- -1 ~ +1 값을 가짐\n",
        "\n",
        "> ### Hyperparameter\n",
        "- 학습의 대상이 아니라 학습 이전에 정해놓은 변수\n",
        "\n",
        "> ### Initialization\n",
        "- 최적의 지점 자체에서 시작한다는 말이 성립할 수 없는 대신 모델이 학습되는 도중에 gradient vanishing / exploding 현상을 최소한 겪지 않게 하거나 loss 함수 공간을 최적화가 쉬운 형태로 바꾸는 방법\n",
        "- weight를 초기화해야 학습이 잘 되고 성능이 좋아진다.\n",
        "- RBM, Xavier, He initialization 등등을 사용한다.  \n",
        "  1) Xavier\n",
        "    - 핵심: 가중치의 초기값을 $N(0,var=2/(n_{in}+n_{out}))$에서 뽑는다. \n",
        "    - $n_{in}$: 해당 레이어에 들어오는 특성의 수 \n",
        "    - $n_{out}$: 해당 레이어에 나가는 특성의 수\n",
        "    - 이를 통해 데이터가 몇 개의 layer를 통과하더라도 활성화 값이 너무 커지거나 작아지지 않고 일정한 범위 안에 있도록 잡아주는 초기화 방법\n",
        "    - sigmoid, tanh 함수를 주로 사용하는 경우에 사용하는 initialization  \n",
        "  2) He \n",
        "    - 가중치를 $N(0,var=\\frac{2}{(1+a^2)*n_{in}})$에서 샘플링함\n",
        "    - a: ReLU, Leaky Rulu의 음수 부분의 기울기에 해당\n",
        "    - ReLU를 사용하는 경우에 사용하는 initialization\n",
        "\n",
        "> ### Internal Covariate Shift\n",
        "- Train set과 Test set의 분포에 차이가 있어서 문제를 발생시킨다는 개념\n",
        "- 이 문제를 해결하기 위해 batch normalization을 사용\n",
        "\n",
        "> ### Iteration\n",
        "- batch를 몇 번 학습에 사용했냐?\n",
        "- e.g. 1000개의 training set이 있고, batch size는 500이다. 그러므로 1 epoch 도는 동안 2 iteration이다.\n",
        "\n",
        "> ### KL(Kullback-Leibler) Divergence\n",
        "- $H(p,q) = H(p) + \\sum_x{p(x)log\\frac{p(x)}{q(x)}}$\n",
        "- 분포 p를 기준으로 q가 얼마나 다른지 측정하는 방법\n",
        "- cross entropy를 최소화한다는 것은 KLD를 최소화하여 q가 p의 분포와 최대한 같아지게 한다는 것.\n",
        "\n",
        "> ### Latent space interpolation (잠재 공간 보간)\n",
        "- 잠재 변수 z의 공간을 탐색하는 방식\n",
        "\n",
        "> ### L-BFGS(limited-memory BFGS)\n",
        "- style transfer를 구현할 때 쓰는 optimizer 알고리즘으로 2차 미분 값까지 이용한다는 특징이 있음.\n",
        "- 연산 속도는 1차 미분보다 느리지만 엄밀하게 계산하는 대신 근사하는 방식으로 바꿔서 연산 속도를 높이는 장점이 있음.\n",
        "\n",
        "> ### Leaky RELU\n",
        "- $f(x) = max(ax,x)$\n",
        "- dying neuron 현상을 해결해줌\n",
        "\n",
        "> ### Learning rate\n",
        "- 계산한 기울기에 비례하여 parameter를 얼마만큼 업데이트할지 결정하는 수치\n",
        "- 배우는 속도. 데이터와 모델에 따라서 최적값은 모두 다르다.\n",
        "- 보통 실무에서는 초기에 비교적 높은 lr로 시작하여 점차 낮추는 전략을 취함. 단 오히려 batch size를 늘리는 게 더 좋다는 연구도 있음\n",
        "- Learning rate가 너무 크면?? \n",
        "  - 발산해버리게 된다. Cost가 무진장 늘어난다.\n",
        "  - 데이터에 빨리 적응하지만 이전 데이터를 금방 까먹음\n",
        "- Learning rate가 너무 작으면? \n",
        "  - cost 값이 변함이 없는 것을 알 수 있다.\n",
        "  - 관성이 커져서 느리게 학습됨\n",
        "- lr_scheduler: StepLR, ExponentialLR, MultiStepLR이 있음. 밑에 pytorch method에 설명이 있음\n",
        "\n",
        "> ### Loss function\n",
        "- data point에서의 오차. single data set에서의 오차를 구하는 것\n",
        "- 순간순간의 loss를 판단할 땐 loss function, 학습이 완료된 후에는 cost function!\n",
        "- A loss function **is a part of** a cost function **which is a type of** an objective function!!!\n",
        "\n",
        "> ### Neural Network\n",
        "- 뇌에 있는 neuron의 모양을 본 따서 만든 신경망\n",
        "- 여러 자극 혹은 입력이 들어오면 각각 가중치를 곱해 더해주고 추가적으로 편차를 더한 값을 activation function을 통해 변형하여 전달하는 네트워크!\n",
        "- f는 activation function  \n",
        "$y = w_4(f(w_3(f(w_2(f(w_1*x+b_1))+b_2))+b_3))+b_4$\n",
        "\n",
        "> ### Normalization (정규화)\n",
        "- 데이터를 분포를 맞춰주는 역할을 한다.\n",
        "- 정규화 방법: Standardization(정규화), minmax(최소극대화)\n",
        "  - Standarization: 주어진 평균과 std로 데이터의 분포를 바꿈\n",
        "  - minmax: 모든 데이터를 0~1로 바꿔줌. 평균적 범위를 넘어서는 너무 작거나 큰 이상치가 있는 경우에는 오히려 학습에 방해가 되기도 함.\n",
        "- 정규화를 하면 일반적으로 학습이 더 잘된다 -> 원형에 가까운 형태로 손실 그래프가 바뀌어서 불필요한 업데이트가 줄어들고더 큰 lr을 적용할 수 있기 때문\n",
        "\n",
        "> ### Optimization\n",
        "- Regular equation을 쓰지 않고 optimization을 진행하는 이유: regular equation을 사용하면 데이터 크기가 커질수록 복잡도가 증가하여 계산이 비효율적이고 확정상이 좋지 않기 때문.\n",
        "[Optimization Algorithms](https://medium.com/analytics-vidhya/optimization-algorithms-for-deep-learning-1f1a2bd4c46b)\n",
        "- Given an algorithm f(x), an optimization algorithm help in either minimizing or maximizing the value of f(x). In the context of deep learning, we use optimization algorithms to train the neural network by optimizing the cost function J[J(W,b)]\n",
        "![optimization 변천사](https://image.slidesharecdn.com/random-170910154045/95/-49-638.jpg?cb=1505089848)\n",
        "- 스탭방향: Gradient, 스탭사이즈: Learning rate\n",
        "- GD: 모든 데이터를 검토한 뒤 방향을 찾자\n",
        "- SGD: 조금씩 데이터를 검토한 뒤 자주 방향을 찾자\n",
        "- Momentum: 관성 개념을 도입해서 덜 비틀거리면서 가보자\n",
        "- Adagrad: 처음엔 빠르게 학습하고 나중에 세밀하게 학습하자\n",
        "- NAG: 관성방향으로 먼저 움직인 뒤 계산한 방향으로 가보자\n",
        "- RMSProp: 세밀하게 학습하되 상황을 보며 정도를 정하자\n",
        "- AdaDelta: 세밀한 정도가 너무 작아져서 학습이 안되는 것을 막자\n",
        "- Adam: 둘 다 고려해서 방향을 찾자\n",
        "\n",
        "> ### Overshooting\n",
        "- Leaerning rate이 너무 크면 diverge 하면서 cost가 점점 늘어난다.\n",
        "\n",
        "> ### Padding Method\n",
        "- 여러 개의 sequence data를 하나의 batch로 묶는 방법 중 하나\n",
        "- 가장 긴 sequence의 길이에 맞춰 나머지 data의 뒷부분을 pad로 치환하는 방법\n",
        "- 단점: 계산하지 않아도 될 부분이 늘어난다.\n",
        "\n",
        "> ### Packing Method\n",
        "- 여러 개의 sequence data를 하나의 batch로 묶는 방법 중 하나\n",
        "- sequence 길이를 기억하는 방법으로 저장.\n",
        "- 단점: 길이 내림차순으로 정렬이 되어야 pytorch에서 동작할 수 있다. padding보다는 구현이 좀 더 복잡하다.\n",
        "\n",
        "> ### Perceptron\n",
        "- Neural Network의 한 종류\n",
        "- x가 입력되었을 때 x에 w(가중치)를 곱하고 bias를 더하고서, activation function(e.g. sigmoid function)을 거쳐서 최종적인 output을 만든다. \n",
        "- 초창기는 `linear classifier`을 위해서 만들어진 모델\n",
        "- AND gate & OR gate & XOR gate\n",
        "\n",
        "|A|B|AND|OR|XOR|\n",
        "|--|--|--|--|--|\n",
        "|0|0|0|0|0|\n",
        "|0|1|0|1|1|\n",
        "|1|0|0|1|1|\n",
        "|1|1|1|1|0|\n",
        "\n",
        "> ### Pooling\n",
        "- 상황에 따라 초고화질이 필요하지 않을 수도 있고, 좀 넓게 봐야 파악할 수 있는 특성이 있는데 그때 pooling을 사용!\n",
        "- 이미지 사이즈를 줄이기 위해서 사용되기도 하고, fully connected 연산을 대체하기 위해서 average pooling을 사용하기도 한다.\n",
        "- Max Pooling: n*n에서 가장 큰 값이 반환\n",
        "- Average Pooling: n*n안에서 평균이 반환\n",
        "- Pooling을 해주는 이유는 overfitting을 방지하기 위함이다.\n",
        "  너무 많은 layer를 통과하면 feature가 많아지기 때문에 overfitting이 된다.\n",
        "  이를 방지하고자 convolution을 진행한 뒤에 pooling을 통해서 특징을 줄여주게 되는 것!\n",
        "\n",
        "> ### Regularization\n",
        "- 제약 조건을 추가로 걸어줌으로써 overfitting을 해결하는 기법\n",
        "- MSE 말고도 람다와 변수의 합을 곱한 항(정형화 식)이 추가되어있음.\n",
        "- 전체 식을 최소화하려면 정형화 식 부분도 작아져야 함. Penalty를 주는 것 이를 통해 함수의 형태가 단순해지고 overfitting도 줄어들게 되는 것.\n",
        "- 한편 여기서 람다가 너무 커지면 w의 값이 매우 작아져 함수가 너무 단순해지는 underfitting 발생 -> 람다의 적절한 값을 찾아야 함.\n",
        "- L1 Regularization  \n",
        "$w^*=\\underset{w}{\\operatorname{argmin}}\\sum_{j}(t(x_j)-\\sum_{i}w_ih_i(x_j))^2 + \\lambda\\sum_{i=1}^k\\lvert{w_i}\\lvert$\n",
        "  - w 값들이 0이 되는 경우가 많다.\n",
        "  - 0이 나왔다는 것은 특정 feature가 오차를 줄이는 데 영향이 없다는 것을 의미\n",
        "  - 이를 활용해서 feature selection을 하기도 함.\n",
        "- L2 Regularization  \n",
        "$w^*=\\underset{w}{\\operatorname{argmin}}\\sum_{j}(t(x_j)-\\sum_{i}w_ih_i(x_j))^2 + \\lambda\\sum_{i=1}^k{w_i^2}$\n",
        "  - 수식의 해를 구할 수 있다. w값이 0이 되기 어렵다.\n",
        "\n",
        "> ### ReLU  \n",
        "- Vanishing Gradient 문제를 해결하는 activation function\n",
        "- 단점: 어느 순간 큰 손실이 발생해 w와 bias가 마이너스로 떨어지는 경우, 어떠한 입력값에도 활성화 값이 0이 되는 dying neuron 현상이 발생\n",
        "- $f(x) = max(0,x)$\n",
        "\n",
        "> ### Sequence Data\n",
        "- 순서가 존재하는 데이터\n",
        "\n",
        "> ### Sigmoid function\n",
        "- $S(x) = \\frac{1}{1 + e^{-x}}$\n",
        "- 실함수로써 유계이며 미분가능, 모든 점에서의 미분값은 양수이기에 역전파에 유용\n",
        "- 한계: Gradient를 계산하면서 문제가 발생. 양 끝 부분에서 gradient를 구하면 값이 매우 작아진다. backpropagation을 통해서 gradient를 전파시킬 때, activation function에서 gradient를 곱하게 된다. 작은 값이 곱해지면서 loss로부터 전파되는 gradient가 소멸되는 문제가 발생 (Vanishing Gradient)\n",
        "\n",
        "> ### Skip-gram\n",
        "- embedding의 대표적인 방식\n",
        "- CBOW와는 반대로 중심 단어로부터 주변 단어들이 나오도록 모델을 학습하여 embedding vector를 얻는 방식.\n",
        "\n",
        "> ### Softmax  \n",
        "- $softmax(y_i) = \\frac{exp(y_i)}{{\\sum_{j}exp(y_j)}}$\n",
        "- 입력받은 값을 출력으로 0~1 사이의 값으로 모두 정규화하여 출력값들의 총합은 항상 1이 되는 함수\n",
        "- Logistic regression이 binary class 예측이면 softmax는 더 다양한 class 예측가능\n",
        "- CNN 신경망의 결괏값을 확률로 바꿔줘야 할 때 사용하는 함수\n",
        "\n",
        "> ### Super resolution\n",
        "- 저화질의 이미지를 입력으로 받아서 고화질로 변환하는 작업\n",
        "\n",
        "> ### Tensor\n",
        "- n차원의 배열을 전부 포함하는 개념\n",
        "\n",
        "> ### Torchvision\n",
        "- 다양한 dataset과 model을 제공해주는 패키지다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uMGUztimedg",
        "colab_type": "text"
      },
      "source": [
        "## Pytorch 메소드 모음\n",
        "```메소드참조```부분"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTXVtk-mmk7f",
        "colab_type": "text"
      },
      "source": [
        "> 기초 모델 관련\n",
        "- **.ToTensor ( )** : 이미지 순서와 값들을 pytorch에 맞게 바꿔줌\n",
        "- **torch .tensor ( data = , requires_grad = True)** : requires_grad는 tensor에 대한 기울기를 저장할지 여부\n",
        "- **.item()** : Tensor 형태에서 앞에 item만을 반환\n",
        "- **.zero_grad ( )** : gradienet를 초기화\n",
        "- **.backward ( )** : Backpropagation 진행\n",
        "- **.step ( )** : parameter를 업데이트!\n",
        "- **torch .no_grad ( )** : 테스트를 진행하는데 기울기를 계산하지 않겠다는 것\n",
        "- **torch .optim .lr_scheduler .StepLR** : 정해진 step_size(epoch 수)마다 lr에 gamma를 곱해 lr을 감소시킴\n",
        "- **torch .optim .lr_scheduler .ExponentialLR** : 매 epoch 마다 lr에 gamma를 곱해 감소\n",
        "- **torch .optim .lr_scheduler .MultiStepLR** : step_size에 milestones 인수에 list로 받아서 원하는 지점마다 lr을 감소\n",
        "- **torch .nn .BatchNorm1d ( num_features , eps = 1e-05 , momentum = 0.1, affine = True, track_running_stats = True)**: batch normalization하는 code\n",
        "\n",
        "> CNN 관련\n",
        "- **torch .nn .Conv2d (in_channels, out_channels, kernel_size, stride = 1, padding = 0, bias  = True)** : 이런식으로 입력 채널, 출력채널, 커널 크기 등에 따라서 convolution을 진행할 수 있음\n",
        "- **torch .nn .MaxPool2d ( kernel_size, stride = None, padding = 0, dilation =1, return_indices = False, ceil_mode = False)** : MaxPool 설정하는 법\n",
        "- **torch .nn. ConTranspose2d (in_channels = 1, out_channels = 1, kernel_size = 3, stride = 1, padding = 0, output_padding = 0, bias = False)**: deconvolution 계산하는 법\n",
        "- input = (Batch_size, in_channel, height, width)\n",
        "- output = (Batch_size, out_channel, height, width)\n",
        "\n",
        "> RNN 관련\n",
        "- **rnn .init_hidden( )** : 학습을 시작하려면 순환 신경망 은닉층의 초깃값을 지정하기 위함."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gTEdK2mn6fBk"
      },
      "source": [
        "# 3. Linear model (with Pytorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YfPXOpIa6fBh"
      },
      "source": [
        "## 빈용 메소드 모음\n",
        "- **Basic**\n",
        "  - **torch .FloatTensor()** : matrix를 생성하는 메소드\n",
        "  - **.matmul( )** : 행렬간의 곱을 구해줌\n",
        "  - **.mul( )** : 행렬간의 사이즈를 늘려서 맞춘 다음 곱을 해줌\n",
        "\n",
        "    *: 같은 결과를 반환\n",
        "  - **.mul_()**: in-place operation. 기존 tensor를 변환시킨다.\n",
        "  - **.mean ( dim = )** : 평균을 구해줌. dimension을 선택해줄 수도 있음\n",
        "  - **.sum ( dim = )** : 합을 구해줌. dimension을 선택해줄 수도 있음\n",
        "  - **.max ( dim = )** : 최대값을 구해줌. dimension을 선택해줄 수도 있음\n",
        "  - **.max ( dim = )[ 1 ]**: argmax. 즉 최대값의 index를 반환해줌\n",
        "  - **.view ( )**: reshape과 비슷. 원하는 형태로 tensor를 바꿔줄 수 있다.\n",
        "  - **.squeeze ( )**: view와 비슷하나, dimension의 element의 개수가 1인 경우에 그 dimension을 없애준다.\n",
        "  - **.unsqueeze ( dim = )**: squeeze를 반대로 해줌. 원하는 dimension에 1을 넣어준다. 꼭 dimension을 명시해줘야 한다.\n",
        "  - **torch .ones_lie ( )** : 같은 사이즈이되 1로 찬 tensor를 반환\n",
        "  - **torch .zeros_lie ( )** : 같은 사이즈이되 0로 찬 tensor를 반환\n",
        "  \n",
        "- **Type Casting**\n",
        "   - **.Longtensor ( )** : int(?) 형태로 저장한다.\n",
        "   - **.ByteTensor ( )** : Boolean 형태로 저장한다.\n",
        "   - **.long ( )** : long tensor로 바꿔줌\n",
        "   - **.float ( )** : float tensor로 바꿔줌\n",
        "\n",
        "- **Concatenate, Stacking**\n",
        " - **torch .cat ( [ ,  ] , dim = )** : 2개의 tensor를 합쳐주는 방법\n",
        " - **torch . stack ( [ , ] , dim = )** : 쌓아라! list형태로 tensor를 쌓아라! dim은 어떻게 쌓을지를 말해준다.\n",
        " \n",
        "- **Linear( in_features, out_features )**: in_feature는 input sample의 사이즈, out_feature는 output sample의 사이즈"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3dedba00-76a1-49b9-cca4-4d0ec0695fed",
        "id": "KLBidX-v6fBY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "# in-place operation\n",
        "x = torch.FloatTensor([[1,2],[3,4]])\n",
        "print(x.mul(2.))\n",
        "print(x)\n",
        "print(x.mul_(2.))\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2., 4.],\n",
            "        [6., 8.]])\n",
            "tensor([[1., 2.],\n",
            "        [3., 4.]])\n",
            "tensor([[2., 4.],\n",
            "        [6., 8.]])\n",
            "tensor([[2., 4.],\n",
            "        [6., 8.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "45603a68-61e1-4ecb-dd7a-fa94768ccfd5",
        "id": "LH2di5iy6fBW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "m1 = torch.FloatTensor([[3,3]])\n",
        "m2 = torch.FloatTensor([[2,2]])\n",
        "print(m1 + m2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[5., 5.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "dddf62ac-884c-4700-b2a4-bbc11740854d",
        "id": "bxpyKGmD6fBS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# 사이즈가 맞지 않지만 차원을 알아서 늘려서 실행을 해준다.\n",
        "m1 = torch.FloatTensor([[1,2]])\n",
        "m2 = torch.FloatTensor([[3],[4]])\n",
        "print(m1+m2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4., 5.],\n",
            "        [5., 6.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "4671d164-6b38-4541-9c49-e1e52303c94f",
        "id": "T54o4pPo6fBO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# Multiplication vs Matrix Multiplication\n",
        "m1 = torch.FloatTensor([[1, 2],[3, 4]])\n",
        "m2 = torch.FloatTensor([[1],[2]])\n",
        "print(m1.matmul(m2))\n",
        "print(m1 * m2)\n",
        "print(m1.mul(m2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 5.],\n",
            "        [11.]])\n",
            "tensor([[1., 2.],\n",
            "        [6., 8.]])\n",
            "tensor([[1., 2.],\n",
            "        [6., 8.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "24ca01ea-a23d-475f-caf8-a356db08513a",
        "id": "63FT8LNO6fBI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# .mean() 예시1\n",
        "t = torch.FloatTensor([1,2])\n",
        "print(t.mean())\n",
        "\n",
        "# .mean() 예시2\n",
        "t = torch.FloatTensor([[1,2],[3,4]])\n",
        "print(t.mean())\n",
        "print(t.mean(dim=0))\n",
        "print(t.mean(dim=1))\n",
        "print(t.mean(dim=-1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.5000)\n",
            "tensor(2.5000)\n",
            "tensor([2., 3.])\n",
            "tensor([1.5000, 3.5000])\n",
            "tensor([1.5000, 3.5000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f38cbe02-4c19-46d8-ac24-661fb8cf8b3a",
        "id": "ARunkFlH6fBC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "print(t.sum())\n",
        "print(t.sum(dim=0))\n",
        "print(t.sum(dim=1))\n",
        "print(t.sum(dim=-1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(10.)\n",
            "tensor([4., 6.])\n",
            "tensor([3., 7.])\n",
            "tensor([3., 7.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9f9fcd97-a728-40c5-f785-a4a635983668",
        "id": "y-ncX52N6fA5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# argmax: 최대값의 index를 반환\n",
        "t = torch.FloatTensor([[1,2],[3,4]])\n",
        "print(t.max())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(4.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e4ab220f-72bf-4414-df7d-3b70866e2c83",
        "id": "FIg9V_s56fAz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "print(t.max(dim=0))\n",
        "print('Max: ',t.max(dim=0)[0])\n",
        "print('Argmax: ', t.max(dim=0)[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.return_types.max(\n",
            "values=tensor([3., 4.]),\n",
            "indices=tensor([1, 1]))\n",
            "Max:  tensor([3., 4.])\n",
            "Argmax:  tensor([1, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "062a6a0e-a500-4c1f-e6d4-0fc400d6e182",
        "id": "ZeylQC7s6fAv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# view 예시 1\n",
        "t = np.array([[[0,1,2],\n",
        "               [3,4,5]],\n",
        "              \n",
        "              [[6,7,8],\n",
        "               [9,10,11]]])\n",
        "ft = torch.FloatTensor(t)\n",
        "ft.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 2, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9dc3da0d-f896-40f7-f7df-ab423fa9e86c",
        "id": "kCLR4hpa6fAo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "print(ft.view([-1,3]))\n",
        "print(ft.view([-1,3]).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.,  1.,  2.],\n",
            "        [ 3.,  4.,  5.],\n",
            "        [ 6.,  7.,  8.],\n",
            "        [ 9., 10., 11.]])\n",
            "torch.Size([4, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "097734bc-1be9-471b-efda-a479392ca324",
        "id": "3dEroZjT6fAh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "print(ft.view([-1,1,3]))\n",
        "print(ft.view([-1,1,3]).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.,  1.,  2.]],\n",
            "\n",
            "        [[ 3.,  4.,  5.]],\n",
            "\n",
            "        [[ 6.,  7.,  8.]],\n",
            "\n",
            "        [[ 9., 10., 11.]]])\n",
            "torch.Size([4, 1, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f1f7b935-770b-4d68-e2d8-404cffd15c6d",
        "id": "ObEoLjA26fAb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# .squeeze() 예시\n",
        "ft = torch.FloatTensor([[0], [1], [2]])\n",
        "print(ft)\n",
        "print(ft.shape)\n",
        "print(ft.squeeze())\n",
        "print(ft.squeeze().shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [2.]])\n",
            "torch.Size([3, 1])\n",
            "tensor([0., 1., 2.])\n",
            "torch.Size([3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f8230d28-7ae4-46df-f9a9-f4e77204f919",
        "id": "88wPLB-i6fAV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# .unsqueeze() 예시\n",
        "ft = torch.Tensor([0,1,2,])\n",
        "print(ft.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "30ab6d05-ca94-4384-9f47-ef34774afd4e",
        "id": "0hfVY0Zb6fAQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(ft.unsqueeze(dim = 0))\n",
        "print(ft.unsqueeze(0).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 1., 2.]])\n",
            "torch.Size([1, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a4850460-2ecd-4091-a57a-55595a374b03",
        "id": "vLmihUJH6fAJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(ft.view(1,-1))\n",
        "print(ft.view(1,-1).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 1., 2.]])\n",
            "torch.Size([1, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "47c8ddc2-5377-4b5d-9318-5d2a33eebebb",
        "id": "CbHLway36fAB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "print(ft.unsqueeze(1))\n",
        "print(ft.unsqueeze(1).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [2.]])\n",
            "torch.Size([3, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2866f192-7c30-4dce-9ad9-c14b7dedb018",
        "id": "BTzknNLQ6e_-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "print(ft.unsqueeze(-1))\n",
        "print(ft.unsqueeze(-1).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [2.]])\n",
            "torch.Size([3, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7e1fc330-c995-4def-fe36-c44e8fa88879",
        "id": "_TkqXGHT6e_w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# concat, .cat() 예시\n",
        "x = torch.FloatTensor([[1,2], [3,4]])\n",
        "y = torch.FloatTensor([[5,6], [7,8]])\n",
        "print(torch.cat([x,y],dim=0))\n",
        "print(torch.cat([x,y],dim=1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 2.],\n",
            "        [3., 4.],\n",
            "        [5., 6.],\n",
            "        [7., 8.]])\n",
            "tensor([[1., 2., 5., 6.],\n",
            "        [3., 4., 7., 8.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d386dbce-7d79-4c5a-c198-df300fdae2d0",
        "id": "-v_XDhEo6e_m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "# .stack() 예시\n",
        "x = torch.FloatTensor([1,4])\n",
        "y = torch.FloatTensor([2,5])\n",
        "z = torch.FloatTensor([3,6])\n",
        "print(torch.stack([x,y,z]))\n",
        "print(torch.stack([x,y,z], dim=1))\n",
        "print(torch.cat([x.unsqueeze(0),y.unsqueeze(0),z.unsqueeze(0)], dim=0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 4.],\n",
            "        [2., 5.],\n",
            "        [3., 6.]])\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "tensor([[1., 4.],\n",
            "        [2., 5.],\n",
            "        [3., 6.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9631789f-c587-494c-9816-514933fa34a7",
        "id": "ul1w9HVY6e_f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# .ones_like(), .zeros_like() 예시\n",
        "x = torch.FloatTensor([[0,1,2],[2,1,0]])\n",
        "print(x)\n",
        "print(torch.ones_like(x))\n",
        "print(torch.zeros_like(x))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 1., 2.],\n",
            "        [2., 1., 0.]])\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rqjDn-746e_d"
      },
      "source": [
        "## Linear Regression with Pytorch\n",
        "- Basic\n",
        "  - **torch .zeros ( , requires_grad = True )** : 초기화 시킬 때 사용, requires_grad=True는 학습할 것이라고 pytorch에게 명시하는 것  \n",
        "- **torch.optim**: 모델을 개선시킬 때 사용하는 라이브러리\n",
        "  - **opotim.SGD( [ , ], lr = )** : Stochastic gradient descent를 사용하기\n",
        "  - **zero_grad ( )**: gradient를 초기화\n",
        "  - **backward ( )**: gradient를 계산\n",
        "  - **step ( )**: 개선된 gradient의 방향대로 W(weight), b(bias)를 개선한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1PQ47scB6e_U",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oX4FE47Q6e_P",
        "outputId": "0aa6369b-8294-44b6-eafb-272ccc0fa389",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "## Linear regression 예시\n",
        "# x: 공부시간, y: 시험성적\n",
        "x_train = torch.FloatTensor([[1],[2],[3]])\n",
        "y_train = torch.FloatTensor([[2],[4],[6]])\n",
        "\n",
        "# hypothesis를 설정하기\n",
        "W = torch.zeros(1, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "optimizer = optim.SGD([W,b], lr = 0.01)\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(1, nb_epochs+1):\n",
        "  hypothesis = x_train*W + b\n",
        "  # cost 구하기\n",
        "  cost = torch.mean((hypothesis - y_train)**2)\n",
        "  \n",
        "  # 모델 개선 by.stochastic gradient descent\n",
        "  optimizer.zero_grad()    # hypothesis 예측\n",
        "  cost.backward()    # Cost 계산\n",
        "  optimizer.step()    # optimizer로 학습\n",
        "  \n",
        "  if epoch % 100 == 0:\n",
        "      print(epoch, cost.item())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 0.048402994871139526\n",
            "200 0.029910147190093994\n",
            "300 0.018482649698853493\n",
            "400 0.011421176604926586\n",
            "500 0.007057555019855499\n",
            "600 0.004361126106232405\n",
            "700 0.0026949176099151373\n",
            "800 0.0016652889316901565\n",
            "900 0.0010290463687852025\n",
            "1000 0.0006358931423164904\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mT9PWpxM6e_O"
      },
      "source": [
        "## Gradient Descent with Pytorch\n",
        "- Basic\n",
        "  - **torch .zeros ( , requires_grad = True )** : 초기화 시킬 때 사용, requires_grad=True는 학습할 것이라고 pytorch에게 명시하는 것  \n",
        "- **torch.optim**: 모델을 개선시킬 때 사용하는 라이브러리\n",
        "  - **opotim.SGD( [ , ], lr = )** : Stochastic gradient descent를 사용하기\n",
        "  - **zero_grad ( )**: gradient를 초기화\n",
        "  - **backward ( )**: gradient를 계산\n",
        "  - **step ( )**: 개선된 gradient의 방향대로 W(weight), b(bias)를 개선한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "84Hglysp6e_J",
        "colab": {}
      },
      "source": [
        "## Gradient Descent 예시\n",
        "# 데이터\n",
        "x_train = torch.FloatTensor([[1],[2],[3]])\n",
        "y_train = torch.FloatTensor([[1],[2],[3]])\n",
        "\n",
        "# 모델 초기화\n",
        "W = torch.zeros(1, requires_grad  = True)\n",
        "# optimizer 설정\n",
        "optimizer = optim.SGD([W], lr=0.15)\n",
        "\n",
        "nb_epochs = 10\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "  # H(x) 계산\n",
        "  hypothesis = x_train * W\n",
        "\n",
        "  # cost 계산\n",
        "  cost = torch.mean((hypothesis - y_train)**2)\n",
        "\n",
        "  print('Epoch {:4d}/{} W: {:.3f} Cost: {;.6f}'.format(epoch, nb_epochs, W.titem(),cost.item()))\n",
        "\n",
        "  # cost로 H(x) 개선\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TmmghTaQJZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 예시 2\n",
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# 예시1\n",
        "x_train = torch.FloatTensor([[1],[2],[3]])\n",
        "y_train = torch.FloatTensor([[2],[4],[6]])\n",
        "\n",
        "W = torch.zeros(1, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "optimizer = optim.SGD([W,b], lr=0.01)\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(1, nb_epochs +1):\n",
        "  hypothesis = x_train*W + b\n",
        "  cost = torch.mean((hypothesis - y_train)**2)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imrnXamYQNBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 예시 3\n",
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# 예시2\n",
        "X = torch.FloatTensor([[0,0],[0,1],[1,0],[1,1]]).to(device)\n",
        "Y = torch.FloatTensor([[0],[1],[1],[0]]).to(device)\n",
        "\n",
        "## nn layers\n",
        "linear = torch.nn.Linear(2, 1, bias=True)\n",
        "sigmoid = torch.nn.Sigmoid()\n",
        "model = torch.nn.Sequential(linear, sigmoid).to(device)\n",
        "\n",
        "## define cost/loss & optimizer\n",
        "criterion = torch.nn.BCELoss().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
        "for step in range(10001):\n",
        "  optimizer.zero_grad()\n",
        "  hypothesis = model(X)\n",
        "  # cost/Loss function\n",
        "  cost = criterion(hypothesis, Y)\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  if step % 100 == 0:\n",
        "    print(step, cost.item())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vg6i6rTL6e_F"
      },
      "source": [
        "## Multivariate Linear Regression  with Pytorch\n",
        "- Basic\n",
        "  - **.matmul ( )** : x의 길이가 바뀌어도 코드를 바꿀 필요가 없고 속도도 더 빠르다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pU1TZqmZ6e-_",
        "colab": {}
      },
      "source": [
        "## Multivariate linear regression 예시\n",
        "# 데이터\n",
        "x_train = torch.FloatTensor([73, 80, 75],\n",
        "                            [93, 88, 93],\n",
        "                            [89, 91, 90],\n",
        "                            [96, 98, 100],\n",
        "                            [73, 66, 70])\n",
        "y_train = torch.FloatTensor([[152],[180],[196],[142]])\n",
        "\n",
        "# 모델 초기화\n",
        "W = torch.zeros((3,1), requires_grad = True)\n",
        "b = torch.zeros(1, requires_grad = True)\n",
        "\n",
        "# optimizer 설정\n",
        "optimizer = optim.SGD([W, b], lr=le-5)\n",
        "\n",
        "nb_epochs = 20\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "  # H(x) 계산\n",
        "  hypothesis = x_train.matmul(W) + b\n",
        "\n",
        "  # cost 계산\n",
        "  cost = torch.mean((hypothesis - y_train)**2)\n",
        "\n",
        "  # cost로 H(x) 개선\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  print('Epoch {:4d}/{} W: {:.3f} Cost: {;.6f}'.format(epoch, nb_epochs, W.titem(),cost.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_GS6M6WXx2W",
        "colab_type": "text"
      },
      "source": [
        "## 코드를 살펴봅시다~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTuW-lEIEhS7",
        "colab_type": "text"
      },
      "source": [
        "### Mnist에 적용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKsomrXTFq8C",
        "colab_type": "text"
      },
      "source": [
        "학습에 필요한 것들 setting하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec5Z1u4fEscw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 기본 세팅\n",
        "# Lab 10 MNIST and softmax\n",
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# for reproducibility\n",
        "random.seed(777)\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)\n",
        "\n",
        "# 데이터 불러오기\n",
        "# MNIST dataset\n",
        "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
        "                          train=True,\n",
        "                          transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
        "                         train=False,\n",
        "                         transform=transforms.ToTensor(),\n",
        "                         download=True)\n",
        "\n",
        "# dataset loader\n",
        "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          drop_last=True)\n",
        "\n",
        "# MNIST data image of shape 28 * 28 = 784, 10=0~9의 label, bias는 사용\n",
        "linear = torch.nn.Linear(784, 10, bias=True).to(device)    # 1개의 layer를 선언\n",
        "\n",
        "# parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\n",
        "optimizer = torch.optim.SGD(linear.parameters(), lr=learning_rate)    # parameter= W, b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEK2BIjwFvHq",
        "colab_type": "text"
      },
      "source": [
        "실제 학습하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80GF2OAaEsG3",
        "colab_type": "code",
        "outputId": "b5a4ab57-0343-4e1c-c1a7-552da098f226",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        }
      },
      "source": [
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = len(data_loader)\n",
        "\n",
        "    for X, Y in data_loader:\n",
        "        # reshape input image into [batch_size by 784]\n",
        "        # label is not one-hot encoded\n",
        "        X = X.view(-1, 28 * 28).to(device)\n",
        "        Y = Y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        hypothesis = linear(X)\n",
        "        cost = criterion(hypothesis, Y)\n",
        "        cost.backward()\n",
        "        optimizer.step() \n",
        "\n",
        "        avg_cost += cost / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning finished')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost = 0.535468459\n",
            "Epoch: 0002 cost = 0.359274209\n",
            "Epoch: 0003 cost = 0.331187516\n",
            "Epoch: 0004 cost = 0.316578060\n",
            "Epoch: 0005 cost = 0.307158142\n",
            "Epoch: 0006 cost = 0.300180763\n",
            "Epoch: 0007 cost = 0.295130193\n",
            "Epoch: 0008 cost = 0.290851474\n",
            "Epoch: 0009 cost = 0.287417054\n",
            "Epoch: 0010 cost = 0.284379572\n",
            "Epoch: 0011 cost = 0.281825274\n",
            "Epoch: 0012 cost = 0.279800713\n",
            "Epoch: 0013 cost = 0.277808994\n",
            "Epoch: 0014 cost = 0.276154339\n",
            "Epoch: 0015 cost = 0.274440885\n",
            "Learning finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "419mdqk_Xb6o",
        "colab_type": "text"
      },
      "source": [
        "Test 하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9yOEgScEsBc",
        "colab_type": "code",
        "outputId": "9c511286-c635-4b33-d1ad-cb11cdc4a558",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "# Test the model using test sets\n",
        "with torch.no_grad():    # gradient를 계산하지 않겠다는 의미\n",
        "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
        "    Y_test = mnist_test.test_labels.to(device)\n",
        "\n",
        "    prediction = linear(X_test)\n",
        "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "    accuracy = correct_prediction.float().mean()\n",
        "    print('Accuracy:', accuracy.item())\n",
        "\n",
        "    # Get one and predict\n",
        "    r = random.randint(0, len(mnist_test) - 1)\n",
        "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
        "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
        "\n",
        "    print('Label: ', Y_single_data.item())\n",
        "    single_prediction = linear(X_single_data)\n",
        "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8862999677658081\n",
            "Label:  8\n",
            "Prediction:  3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:58: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:48: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLSbv7yCEr8R",
        "colab_type": "code",
        "outputId": "f2671e30-1e3a-405f-da15-c1c113816e5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.8863, device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ0xmkTvLFQL",
        "colab_type": "text"
      },
      "source": [
        "### Mnist에 Linear 적용 (ReLU function)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuVXo-rPLJeG",
        "colab_type": "code",
        "outputId": "bafdbb32-1be4-4d3d-8e12-0228b11e298f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "source": [
        "# Lab 10 MNIST and softmax\n",
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# for reproducibility\n",
        "random.seed(111)\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)\n",
        "\n",
        "# parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "\n",
        "# MNIST dataset\n",
        "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
        "                          train=True,\n",
        "                          transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
        "                         train=False,\n",
        "                         transform=transforms.ToTensor(),\n",
        "                         download=True)\n",
        "\n",
        "# dataset loader\n",
        "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          drop_last=True)\n",
        "\n",
        "# nn layers              input output\n",
        "linear1 = torch.nn.Linear(784, 256, bias=True)\n",
        "linear2 = torch.nn.Linear(256, 256, bias=True)\n",
        "linear3 = torch.nn.Linear(256, 10, bias=True)    # 3개의 layer\n",
        "relu = torch.nn.ReLU()    # Activation funciton = ReLU\n",
        "\n",
        "# Initialization\n",
        "torch.nn.init.normal_(linear1.weight)\n",
        "torch.nn.init.normal_(linear2.weight)\n",
        "torch.nn.init.normal_(linear3.weight)\n",
        "\n",
        "# model\n",
        "model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3).to(device)\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "total_batch = len(data_loader)\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "\n",
        "    for X, Y in data_loader:\n",
        "        # reshape input image into [batch_size by 784]\n",
        "        # label is not one-hot encoded\n",
        "        X = X.view(-1, 28 * 28).to(device)\n",
        "        Y = Y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        hypothesis = model(X)\n",
        "        cost = criterion(hypothesis, Y)\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_cost += cost / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning finished')\n",
        "\n",
        "# Test the model using test sets\n",
        "with torch.no_grad():\n",
        "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
        "    Y_test = mnist_test.test_labels.to(device)\n",
        "\n",
        "    prediction = model(X_test)\n",
        "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "    accuracy = correct_prediction.float().mean()\n",
        "    print('Accuracy:', accuracy.item())\n",
        "\n",
        "    # Get one and predict\n",
        "    r = random.randint(0, len(mnist_test) - 1)\n",
        "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
        "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
        "\n",
        "    print('Label: ', Y_single_data.item())\n",
        "    single_prediction = model(X_single_data)\n",
        "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost = 129.354400635\n",
            "Epoch: 0002 cost = 36.216625214\n",
            "Epoch: 0003 cost = 23.102458954\n",
            "Epoch: 0004 cost = 16.053781509\n",
            "Epoch: 0005 cost = 11.673536301\n",
            "Epoch: 0006 cost = 8.583076477\n",
            "Epoch: 0007 cost = 6.432810783\n",
            "Epoch: 0008 cost = 4.828622818\n",
            "Epoch: 0009 cost = 3.637768030\n",
            "Epoch: 0010 cost = 2.729751825\n",
            "Epoch: 0011 cost = 2.157511234\n",
            "Epoch: 0012 cost = 1.715026379\n",
            "Epoch: 0013 cost = 1.249148488\n",
            "Epoch: 0014 cost = 0.994808137\n",
            "Epoch: 0015 cost = 0.837149620\n",
            "Learning finished\n",
            "Accuracy: 0.9459999799728394\n",
            "Label:  9\n",
            "Prediction:  9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:58: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:48: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzTaKX4BKK_1",
        "colab_type": "text"
      },
      "source": [
        "### Mnist에 softmax, Adam 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rprVWNvkJv_P",
        "colab_type": "code",
        "outputId": "bd76cf0e-3384-4016-c294-f7eaf5b51cde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        }
      },
      "source": [
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# for reproducibility\n",
        "random.seed(777)\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)\n",
        "\n",
        "\n",
        "# parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "\n",
        "\n",
        "# MNIST dataset\n",
        "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
        "                          train=True,\n",
        "                          transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
        "                         train=False,\n",
        "                         transform=transforms.ToTensor(),\n",
        "                         download=True)\n",
        "\n",
        "# dataset loader\n",
        "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          drop_last=True)\n",
        "\n",
        "# MNIST data image of shape 28 * 28 = 784\n",
        "linear = torch.nn.Linear(784, 10, bias=True).to(device)    # 1개의 layer를 선언\n",
        " \n",
        "# Initialization\n",
        "torch.nn.init.normal_(linear.weight)    # layer의 weight를 normal distribution으로 초기화 진행\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\n",
        "optimizer = torch.optim.Adam(linear.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train\n",
        "total_batch = len(data_loader)\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "\n",
        "    for X, Y in data_loader:    # data_loader로 Mnist 이미지와 (X) label (Y)을 불러옴\n",
        "        # reshape input image into [batch_size by 784]\n",
        "        # label is not one-hot encoded\n",
        "        X = X.view(-1, 28 * 28).to(device)\n",
        "        Y = Y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        hypothesis = linear(X)    # 불러온 이미지를 neural network에 통과시키기\n",
        "        cost = criterion(hypothesis, Y)    # NN을 통과시켜서 나온 결과와 정답의 cost를 계산\n",
        "        cost.backward()    # cost로부터 gradient를 계산해서\n",
        "        optimizer.step()    # parameter를 업데이트\n",
        "\n",
        "        avg_cost += cost / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning finished')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/9912422 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:00, 19573437.08it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to MNIST_data/MNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 292394.07it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 5649529.36it/s]                           \n",
            "8192it [00:00, 128854.66it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n",
            "Epoch: 0001 cost = 5.656054974\n",
            "Epoch: 0002 cost = 1.699803472\n",
            "Epoch: 0003 cost = 1.121566176\n",
            "Epoch: 0004 cost = 0.883357942\n",
            "Epoch: 0005 cost = 0.750486851\n",
            "Epoch: 0006 cost = 0.663246810\n",
            "Epoch: 0007 cost = 0.601343870\n",
            "Epoch: 0008 cost = 0.554267764\n",
            "Epoch: 0009 cost = 0.518746793\n",
            "Epoch: 0010 cost = 0.489619613\n",
            "Epoch: 0011 cost = 0.465846598\n",
            "Epoch: 0012 cost = 0.446371138\n",
            "Epoch: 0013 cost = 0.429083288\n",
            "Epoch: 0014 cost = 0.414352983\n",
            "Epoch: 0015 cost = 0.401432723\n",
            "Learning finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qX8FxnWeKa8M",
        "colab_type": "code",
        "outputId": "461a4149-d78e-4821-82a1-ff53a3565926",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "# Test the model using test sets\n",
        "with torch.no_grad():\n",
        "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
        "    Y_test = mnist_test.test_labels.to(device)\n",
        "\n",
        "    prediction = linear(X_test)\n",
        "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "    accuracy = correct_prediction.float().mean()\n",
        "    print('Accuracy:', accuracy.item())\n",
        "\n",
        "    # Get one and predict\n",
        "    r = random.randint(0, len(mnist_test) - 1)\n",
        "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
        "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
        "\n",
        "    print('Label: ', Y_single_data.item())\n",
        "    single_prediction = linear(X_single_data)\n",
        "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8905999660491943\n",
            "Label:  8\n",
            "Prediction:  3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:58: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:48: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G8ym8Ra8u_v",
        "colab_type": "text"
      },
      "source": [
        "### Handmade 예시"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z47z0L6m2iMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTQ6miiw7Blk",
        "colab_type": "code",
        "outputId": "61da3a37-9254-4aae-a677-99edf5b9e08a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "torch.manual_seed(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f15cd0f9e90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XnD1aE57B1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = torch.FloatTensor([[1,2,1],\n",
        "                             [1,3,2],\n",
        "                             [1,3,4],\n",
        "                             [1,5,5],\n",
        "                             [1,7,5],\n",
        "                             [1,2,5],\n",
        "                             [1,6,6],\n",
        "                             [1,7,7]\n",
        "                             ])\n",
        "y_train = torch.LongTensor([2,2,2,1,1,1,0,0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZJtdj0k3uBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test = torch.FloatTensor([[2,1,1],\n",
        "                            [3,1,2],\n",
        "                            [3,3,4]])\n",
        "y_test = torch.LongTensor([2,2,2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKvQcFM332QF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SoftmaxClassifierModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(3, 3)\n",
        "  def forward(self, x):\n",
        "    return self.linear(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMhfdEAa32ZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = SoftmaxClassifierModel()\n",
        "\n",
        "# optimizer 설정\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX9_t4sQ32R8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 훈련하도록 하는 함수\n",
        "def train(model, optimizer, x_train, y_train):\n",
        "  nb_epochs = 20\n",
        "  for epoch in range(nb_epochs):\n",
        "\n",
        "    # H(x) 계산\n",
        "    prediction = model(x_train)\n",
        "\n",
        "    # cost 계산\n",
        "    cost = F.cross_entropy(prediction, y_train)\n",
        "\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
        "        epoch, nb_epochs, cost.item()\n",
        "    ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijYYpwzU32FV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test하는 함수\n",
        "def test(model, optimizer, x_test, y_test):\n",
        "  prediction = model(x_test)\n",
        "  predicted_classes = prediction.max(1)[1]\n",
        "  correct_count = (predicted_classes == y_test).sum().item()\n",
        "  cost = F.cross_entropy(prediction, y_test)\n",
        "\n",
        "  print('Accuracy: {}% Cost: {:.6f}'.format(\n",
        "      correct_count / len(y_test) * 100, cost.item()\n",
        "  ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMM8W-UT3195",
        "colab_type": "code",
        "outputId": "f6fea363-51c0-4fb6-a587-4b79fcb72aec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "train(model, optimizer, x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/20 Cost: 2.203667\n",
            "Epoch    1/20 Cost: 1.199645\n",
            "Epoch    2/20 Cost: 1.142985\n",
            "Epoch    3/20 Cost: 1.117769\n",
            "Epoch    4/20 Cost: 1.100901\n",
            "Epoch    5/20 Cost: 1.089523\n",
            "Epoch    6/20 Cost: 1.079872\n",
            "Epoch    7/20 Cost: 1.071320\n",
            "Epoch    8/20 Cost: 1.063325\n",
            "Epoch    9/20 Cost: 1.055720\n",
            "Epoch   10/20 Cost: 1.048378\n",
            "Epoch   11/20 Cost: 1.041245\n",
            "Epoch   12/20 Cost: 1.034285\n",
            "Epoch   13/20 Cost: 1.027478\n",
            "Epoch   14/20 Cost: 1.020813\n",
            "Epoch   15/20 Cost: 1.014279\n",
            "Epoch   16/20 Cost: 1.007872\n",
            "Epoch   17/20 Cost: 1.001586\n",
            "Epoch   18/20 Cost: 0.995419\n",
            "Epoch   19/20 Cost: 0.989365\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-R7mGn-z8ewJ",
        "colab_type": "code",
        "outputId": "46fe252d-60a8-406b-ef9d-06a76a4440f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test(model, optimizer, x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.0% Cost: 1.425844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PD9qz2119NoJ",
        "colab_type": "text"
      },
      "source": [
        "Learning Rate가 크면...."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdaZgrqh8w3T",
        "colab_type": "code",
        "outputId": "34e62890-2d97-4f43-aba9-e07d2463fb36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "model = SoftmaxClassifierModel()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e5)\n",
        "\n",
        "train(model, optimizer, x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/20 Cost: 3.187324\n",
            "Epoch    1/20 Cost: 1100707.375000\n",
            "Epoch    2/20 Cost: 2482261.000000\n",
            "Epoch    3/20 Cost: 664769.875000\n",
            "Epoch    4/20 Cost: 1668198.875000\n",
            "Epoch    5/20 Cost: 748657.812500\n",
            "Epoch    6/20 Cost: 1353832.375000\n",
            "Epoch    7/20 Cost: 1790073.750000\n",
            "Epoch    8/20 Cost: 917894.875000\n",
            "Epoch    9/20 Cost: 989687.125000\n",
            "Epoch   10/20 Cost: 990845.250000\n",
            "Epoch   11/20 Cost: 1585082.500000\n",
            "Epoch   12/20 Cost: 1265073.750000\n",
            "Epoch   13/20 Cost: 1149145.000000\n",
            "Epoch   14/20 Cost: 589766.937500\n",
            "Epoch   15/20 Cost: 689678.625000\n",
            "Epoch   16/20 Cost: 983032.687500\n",
            "Epoch   17/20 Cost: 1265073.750000\n",
            "Epoch   18/20 Cost: 1686645.000000\n",
            "Epoch   19/20 Cost: 484999.593750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNNlLVf5-HUg",
        "colab_type": "text"
      },
      "source": [
        "Learning Rate가 작으면...."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KugyA8UV8xXE",
        "colab_type": "code",
        "outputId": "850f97ed-2d0c-47a5-89b9-adec392d2360",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "model = SoftmaxClassifierModel()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-10)\n",
        "\n",
        "train(model, optimizer, x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/20 Cost: 1.341574\n",
            "Epoch    1/20 Cost: 1.341574\n",
            "Epoch    2/20 Cost: 1.341574\n",
            "Epoch    3/20 Cost: 1.341574\n",
            "Epoch    4/20 Cost: 1.341574\n",
            "Epoch    5/20 Cost: 1.341574\n",
            "Epoch    6/20 Cost: 1.341574\n",
            "Epoch    7/20 Cost: 1.341574\n",
            "Epoch    8/20 Cost: 1.341574\n",
            "Epoch    9/20 Cost: 1.341574\n",
            "Epoch   10/20 Cost: 1.341574\n",
            "Epoch   11/20 Cost: 1.341574\n",
            "Epoch   12/20 Cost: 1.341574\n",
            "Epoch   13/20 Cost: 1.341574\n",
            "Epoch   14/20 Cost: 1.341574\n",
            "Epoch   15/20 Cost: 1.341574\n",
            "Epoch   16/20 Cost: 1.341574\n",
            "Epoch   17/20 Cost: 1.341574\n",
            "Epoch   18/20 Cost: 1.341574\n",
            "Epoch   19/20 Cost: 1.341574\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU26nC0WFJfd",
        "colab_type": "text"
      },
      "source": [
        "# 4. AutoEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP_VowJZFNb1",
        "colab_type": "text"
      },
      "source": [
        "## AutoEncoder란?\n",
        "- 데이터에 대한 효율적인 압축을 신경망을 통해 자동으로 학습하는 모델\n",
        "- 입력 데이터 자체가 label로 사용되는 비지도 학습\n",
        "- AutoEncoder이 목적은 압축이기에 입력값 X 자체가 목푯값이 됨\n",
        "- 그러므로 Loss는 X와 X'의 차이를 통해 계산"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpAH73YrKICp",
        "colab_type": "text"
      },
      "source": [
        "## Basic AutoEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNNZvSSTKMok",
        "colab_type": "text"
      },
      "source": [
        "![](https://upload.wikimedia.org/wikipedia/commons/2/28/Autoencoder_structure.png)\n",
        "  - X가 neural network를 통해 z(latent variable)가 됨.\n",
        "  - input과 output 외에 모든 중간 값들이 직접 보고 이해하기 어렵기에 latent variable(혹은 code)이라고 칭함."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJT4jf4kKPcf",
        "colab_type": "text"
      },
      "source": [
        "## 코드 살펴봅시다~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odN5aVK7KZvj",
        "colab_type": "text"
      },
      "source": [
        "#### 1. Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMpst1EhKbV-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Hyperparameter setting\n",
        "batch_size = 256\n",
        "learning_rate = 0.0002\n",
        "num_epoch = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH5nZR8EKgd1",
        "colab_type": "text"
      },
      "source": [
        "#### 2. Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uLuIr6KKlLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_train = dset.MNIST(\"./\", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\n",
        "mnist_test = dset.MNIST(\"./\", train=False, transform=transforms.ToTensor(), target_transform=None, download=True)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(mnist_train,batch_size=batch_size, shuffle=True,num_workers=2,drop_last=True)\n",
        "test_loader = torch.utils.data.DataLoader(mnist_test,batch_size=batch_size, shuffle=False,num_workers=2,drop_last=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw22RnVIKlmt",
        "colab_type": "text"
      },
      "source": [
        "#### 3. Model, Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO-mi03kKmFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Model \n",
        "# 인공신경망으로 이루어진 오토엔코더를 생성합니다.\n",
        "# 단순하게 하기 위해 활성화 함수는 생략했습니다.\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder,self).__init__()\n",
        "        self.encoder = nn.Linear(28*28,20)\n",
        "        self.decoder = nn.Linear(20,28*28)   \n",
        "                \n",
        "    def forward(self,x):\n",
        "        x = x.view(batch_size,-1)\n",
        "        encoded = self.encoder(x)\n",
        "        out = self.decoder(encoded).view(batch_size,1,28,28)\n",
        "        return out\n",
        "\n",
        "## Loss function & Optimizer\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "model = Autoencoder().to(device)\n",
        "loss_func = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwXP6zzpKmEW",
        "colab_type": "text"
      },
      "source": [
        "#### 4. Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXDqsWSaKmDV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_arr =[]\n",
        "for i in range(num_epoch):\n",
        "    for j,[image,label] in enumerate(train_loader):\n",
        "        x = image.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model.forward(x)\n",
        "        loss = loss_func(output,x)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    if j % 1000 == 0:\n",
        "        print(loss)\n",
        "        loss_arr.append(loss.cpu().data.numpy()[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_JCjXCxKl38",
        "colab_type": "text"
      },
      "source": [
        "#### 5. Check with trained image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvbPqDjAKl3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out_img = torch.squeeze(output.cpu().data)\n",
        "print(out_img.size())\n",
        "\n",
        "for i in range(10):\n",
        "    plt.imshow(torch.squeeze(image[i]).numpy(),cmap='gray')\n",
        "    plt.show()\n",
        "    plt.imshow(out_img[i].numpy(),cmap='gray')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7isd3ULQKlqa",
        "colab_type": "text"
      },
      "source": [
        "## Convolution AutoEncoder\n",
        "- 손실을 줄이기 위해서 deconvolution(혹은 transposed convolution)을 사용\n",
        "- Convolution 연산이 input에 filter의 가중치를 곱한 결과의 합이었다면\n",
        "- Deconvolution은 하나의 input을 받아 여기에 서로 다른 가중치를 곱해 filter의 크기만큼 input을 '퍼트리는' 역할을 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv0MK8CXKlki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Model 부분의 code\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder,self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "                        nn.Conv2d(1,16,3,padding=1),                            # batch x 16 x 28 x 28\n",
        "                        nn.ReLU(),\n",
        "                        nn.BatchNorm2d(16),\n",
        "                        nn.Conv2d(16,32,3,padding=1),                           # batch x 32 x 28 x 28\n",
        "                        nn.ReLU(),\n",
        "                        nn.BatchNorm2d(32),\n",
        "                        nn.Conv2d(32,64,3,padding=1),                           # batch x 32 x 28 x 28\n",
        "                        nn.ReLU(),\n",
        "                        nn.BatchNorm2d(64),\n",
        "                        nn.MaxPool2d(2,2)                                       # batch x 64 x 14 x 14\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "                        nn.Conv2d(64,128,3,padding=1),                          # batch x 64 x 14 x 14\n",
        "                        nn.ReLU(),\n",
        "                        nn.BatchNorm2d(128),\n",
        "                        nn.MaxPool2d(2,2),\n",
        "                        nn.Conv2d(128,256,3,padding=1),                         # batch x 64 x 7 x 7\n",
        "                        nn.ReLU()\n",
        "        )\n",
        "        \n",
        "                \n",
        "    def forward(self,x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.view(batch_size, -1)\n",
        "        return out\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder,self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "                        nn.ConvTranspose2d(256,128,3,2,1,1),                    # batch x 128 x 14 x 14\n",
        "                        nn.ReLU(),\n",
        "                        nn.BatchNorm2d(128),\n",
        "                        nn.ConvTranspose2d(128,64,3,1,1),                       # batch x 64 x 14 x 14\n",
        "                        nn.ReLU(),\n",
        "                        nn.BatchNorm2d(64)\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "                        nn.ConvTranspose2d(64,16,3,1,1),                        # batch x 16 x 14 x 14\n",
        "                        nn.ReLU(),\n",
        "                        nn.BatchNorm2d(16),\n",
        "                        nn.ConvTranspose2d(16,1,3,2,1,1),                       # batch x 1 x 28 x 28\n",
        "                        nn.ReLU()\n",
        "        )\n",
        "        \n",
        "    def forward(self,x):\n",
        "        out = x.view(batch_size,256,7,7)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "encoder = Encoder().to(device)\n",
        "decoder = Decoder().to(device)\n",
        "\n",
        "# 인코더 디코더의 파라미터를 동시에 학습시키기 위해 이를 묶는 방법\n",
        "parameters = list(encoder.parameters())+ list(decoder.parameters())\n",
        "\n",
        "loss_func = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(parameters, lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZ6HbDW9C905",
        "colab_type": "text"
      },
      "source": [
        "# 5. DNN (Deep Neural Network)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZSf-RrICfkv",
        "colab_type": "text"
      },
      "source": [
        "## DNN이란?\n",
        "![](https://static.wixstatic.com/media/a27d24_c1620508d5064be294c79bdee5310f22~mv2.png/v1/fit/w_1608,h_798,al_c,q_80/file.webp)\n",
        "- 인공 신경망에서 hidden layer가 2개 이상인 네트워크\n",
        "- 지도학습, 비지도학습, 강화학습 모두에 사용됨\n",
        "> DNN을 훈련하는 과정\n",
        " 1. Nerural Network 구조를 만든다.\n",
        " 2. 구조를 train한 다음에 성능을 평가한다.\n",
        " 3. Overfitting이 되었다면 regularization(e.g. drop-out, batch-normalization) 등을 사용한다.\n",
        " 4. 최고의 성능을 내기 위해 위 과정을 반복한다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc2C22R1nDvn",
        "colab_type": "text"
      },
      "source": [
        "## DNN 과정 살펴보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xddvenpDHok",
        "colab_type": "text"
      },
      "source": [
        "> Step.1\n",
        "- device에서 'cuda' 설정을 해준다.\n",
        "- GPU를 쓰기 위함\n",
        "\n",
        "> Step.2\n",
        "- Data 불러오기\n",
        "\n",
        "> Step.3  \n",
        "- 1) Input data를 Linear layer을 통과시킨 뒤 non-linear activation function을 통과시켜서 output을 구한다.  \n",
        "- 2) 위에서 나온 비선형 output을 다시 다음 Linear layer를 통과시킨뒤 non-linear activation function을 통과시켜 output을 구한다.  \n",
        "- 3) 위의 과정을 반복해서 최종 output을 뽑아준다.\n",
        "- 모델의 효율을 높이기 위해서 weight initilization, batch normalization, dropout 등을 사용\n",
        "\n",
        "> Step.4  \n",
        "- 1) 최종적으로 나온 output과 target 값의 차이(오차)를 구해준다.  \n",
        "- 2) 학습하면서 생긴 수많은 parameter(weight 등)가 오차에서 차지하는 비중을 계산한다.  \n",
        "- 3) 그 비중만큼 다시 parameter들을 업데이트해준다.\n",
        "\n",
        "> Step.5  \n",
        "- Step.3에서 Step.4를 반복하면서 parameter를 업데이트 해나간다.  \n",
        "\n",
        "> Step.6\n",
        "- 학습된 모델을 test data에 넣어서 성능을 test하기!!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvfPNPNXFrE-",
        "colab_type": "text"
      },
      "source": [
        "## 하나하나 뜯어봅시다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOHgaMFVFspK",
        "colab_type": "text"
      },
      "source": [
        "### Step.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mUMkA3SFwNu",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "- device에서 'cuda' 설정을 해준다.\n",
        "- GPU를 쓰기 위함\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLktu7X3Fzkk",
        "colab_type": "text"
      },
      "source": [
        "GPU는 써야 성능이 오지게 빨라지겠쥬?\n",
        "```python\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "```\n",
        "요로로콤 설정하시면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irdcEEPdGjHH",
        "colab_type": "text"
      },
      "source": [
        "### Step.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_NX8ZIVGjeB",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "1) Data 불러오기  \n",
        "2) Hyperparameter 설정해주기\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tsx-dywXG_WQ",
        "colab_type": "text"
      },
      "source": [
        "1) DataLoader를 사용해서 data를 불러주면 된다~  \n",
        "2) learning_rate, training_epochs, batch_size, drop_prob 등을 설정해주기!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jqmrW03GvSk",
        "colab_type": "text"
      },
      "source": [
        "### Step.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYmWIZt0Gjwz",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "1) Input data를 Linear layer을 통과시킨 뒤 non-linear activation function을 통과시켜서 output을 구한다.  \n",
        "2) 위에서 나온 비선형 output을 다시 다음 Linear layer를 통과시킨뒤 non-linear activation function을 통과시켜 output을 구한다.  \n",
        "3) 위의 과정을 반복해서 최종 output을 뽑아준다.  \n",
        "- 모델의 효율을 높이기 위해서 weight initialization, batch normalization, dropout 등을 사용\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pudTVE_aUCMJ",
        "colab_type": "text"
      },
      "source": [
        "1) Activation function  \n",
        "- 신경망의 출력을 결정하는 식!  \n",
        "- Sigmoid, ReLU, Leaky ReLU, Hyperbolic Tangent(tanh)  ```딥러닝 관련 용어 참조```  \n",
        "\n",
        "2) 비선형 Activation function을 사용하는 이유\n",
        "- Activation function이 선형이면 layer를 깊게 쌓더라도 결국 선형 변환이기 때문.  \n",
        "  비선형 변환을 해줘야 비선형 관계를 학습할 수 있음!\n",
        "\n",
        "3) 수식으로 보면 요로로콤 \n",
        "$$y = w_4(f(w_3(f(w_2(f(w_1*x+b_1))+b_2))+b_3))+b_4$$  \n",
        "- $w_1$: 첫 번째 input  \n",
        "- f: Activation Function\n",
        "- y: 최종 output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZF7qHzDGuTd",
        "colab_type": "text"
      },
      "source": [
        "### Step.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ING1YlXsGjtW",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "1) 최종적으로 나온 output과 target 값의 차이(오차)를 구해준다.  \n",
        "2) 학습하면서 생긴 수많은 parameter(weight 등)가 오차에서 차지하는 비중을 계산한다.  \n",
        "3) 그 비중만큼 다시 parameter들을 업데이트해준다.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG4MYurTYjqj",
        "colab_type": "text"
      },
      "source": [
        "1) Loss function  \n",
        "- Output과 target의 오차 구하기!!  \n",
        "- Loss function는 방법은 CrossEntropy, BinaryCrossEntropy```용어참조```   등을 주로 사용! \n",
        "\n",
        "2) Backpropagation```용어참조```\n",
        "- 각 parameter가 loss에서 차지하는 비중을 chain rule을 이용해서 거꾸로 gradient를 구해준다!\n",
        "\n",
        "3) Optimizer  \n",
        "- Optimizer를 활용하여 2)에서 구한 gradient들을 바탕으로 기존 parameter에 업데이트를 해준다.  \n",
        "- Optimizer로는 Gradient Descent, Stochastic Gradient Descent, Adam```용어참조```   등이 있다. \n",
        "- Adam을 가장 많이 사용한다!\n",
        "- parameter를 업데이트를 할 때는 learning_rate```용어참조```  을 고려해서 업데이트를 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOdk6QUccAwv",
        "colab_type": "text"
      },
      "source": [
        "#### Backpropgation을 풀어쓴 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMb0AXjVYjjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Backpropagation Code\n",
        "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
        "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
        "\n",
        "# weight, bias를 직접 선언\n",
        "w1 = torch.Tensor(2, 2).to(devce)\n",
        "b1 = torch.Tensor(2).to(device)\n",
        "w2 = torch.Tensor(2, 1).to(device)\n",
        "b2 = torch.Tensor(1).to(device)\n",
        "\n",
        "def sigmoid(x):\n",
        "    #  sigmoid function\n",
        "    return 1.0 / (1.0 + torch.exp(-x))\n",
        "    # return torch.div(torch.tensor(1), torch.add(torch.tensor(1.0), torch.exp(-x)))\n",
        "\n",
        "def sigmoid_prime(x):    # sigmoid를 미분한 것\n",
        "    # derivative of the sigmoid function\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "for step in range(10001):\n",
        "  # forward\n",
        "  l1 = torch.add(torch.matmul(X, w1), b1)\n",
        "  a1 = sigmoid(l1)\n",
        "  l2 = torch.add(torch.matmul(a1, w2), b2)\n",
        "  Y_pred = sigmoid(l2)\n",
        "\n",
        "  # binary cross entropy loss를 사용\n",
        "  cost = -torch.mean(Y*torch.log(Y_pred) + (1-Y)*torch.log(1 - Y_pred))\n",
        "\n",
        "  # Back prop (chain rule)    # pytorch에서 .backward() 에 해당하는 부분\n",
        "  # Loss derivative\n",
        "  d_Y_pred = (Y_pred - Y) / Y_pred * (1.0 - Y_pred) + 1e-7)    # 1e-7은 0으로 나눠주는 것을 방지하기 위함\n",
        "\n",
        "  # Layer 2\n",
        "  d_l2 = d_Y_pred * sigmoid_prime(12)\n",
        "  d_b2 = d_12\n",
        "  d_w2 = torch.matmul(torch.transpose(a1, 0, 1), d_b2)\n",
        "\n",
        "  # Layer 1\n",
        "  d_a1 = torch.matmul(d_b2, torch.transpose(w2, 0, 1))\n",
        "  d_l1 = d_a1 * sigmoid_prime(l1)\n",
        "  d_b1 = d_l1\n",
        "  d_w1 = torch.matmul(torch.transpose(X, 0, 1), d_b1)\n",
        "\n",
        "  # Weight Update    # pytorch에서 .step() 에 해당하는 부분\n",
        "  w1 = w1 - learning_rate * d_w1\n",
        "  b1 = b1 - learning_rate * torch.mean(d_b1, 0)\n",
        "  w2 = w2 - learning_rate * d_w2\n",
        "  b2 = b2 - learning_rate * torch.mean(d_b2, 0)\n",
        "\n",
        "  if step % 100 == 0:\n",
        "    print(step, cost.item())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-DBIey1G0ha",
        "colab_type": "text"
      },
      "source": [
        "### Step.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdsO22YKGjo7",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "- Step.3에서 Step.4를 반복하면서 parameter를 업데이트 해나간다.  \n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4fM7OXScFTo",
        "colab_type": "text"
      },
      "source": [
        "-.step(): backpropagation을 바탕으로 계산된 gradient를 기존 parameter에 업데이트하여 새로운 parameter를 계산"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEonMRj9i9zQ",
        "colab_type": "text"
      },
      "source": [
        "### Step.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_bYdsfTi_bo",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "- 학습된 모델을 test data에 넣어서 성능을 test하기!!\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5yboUeBjDx3",
        "colab_type": "text"
      },
      "source": [
        "test는 test지 무슨 설명이 더 필요항가~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNvpr4FadVQL",
        "colab_type": "text"
      },
      "source": [
        "## 코드 살펴봅시다~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OSPeAmOhiOi",
        "colab_type": "text"
      },
      "source": [
        "### 심플 version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kd-TIHxIe2ao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################################################################################################\n",
        "############################################ Step.1 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# for reproducibility\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.2 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
        "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.3 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# nn layers\n",
        "'''\n",
        "torch.nn.Linear(in_features, out_features, bias = True)\n",
        "- in_features: input sample의 사이즈!\n",
        "- out_features: output sample의 사이즈!\n",
        "- bias: 편차\n",
        "'''\n",
        "linear1 = torch.nn.Linear(2, 10, bias=True)\n",
        "linear2 = torch.nn.Linear(10, 10, bias=True)\n",
        "linear3 = torch.nn.Linear(10, 10, bias=True)\n",
        "linear4 = torch.nn.Linear(10, 1, bias=True)\n",
        "sigmoid = torch.nn.Sigmoid()    # 얘가 Activation function!!\n",
        "\n",
        "# torch.nn.Sequential을 통해서 model을 만들어주는 것!!!\n",
        "model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid, linear3, sigmoid, linear4, sigmoid).to(device)\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.4 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "criterion = torch.nn.BCELoss().to(device)   # 여기서는 binary cross entropy를 loss function으로 썼음!!\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1)  # Optimizer를 Stochastic Gradient Descent를 씀!!\n",
        "\n",
        "for step in range(10001):\n",
        "'''\n",
        "여기서 꼭 zero_grad()를 해줘야 됨!\n",
        "직전 for 문에서 계산되었던 gradient가 그대로 저장되어있기 때문에\n",
        "0으로 만들어줘야 제대로 학습이 됨!!\n",
        "'''\n",
        "    optimizer.zero_grad()\n",
        "    hypothesis = model(X)\n",
        "\n",
        "    # cost/loss function\n",
        "    cost = criterion(hypothesis, Y)\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.5 ###################################################################\n",
        "######################################################################################################################\n",
        "    \n",
        "    cost.backward()   # 이 부분이 Backpropagation! .backward()가 backpropagation해주는 메소드!!!\n",
        "    optimizer.step()    # 이 부분이 parameter를 업데이트하는 메소드!!!\n",
        "\n",
        "    if step % 400 == 0:\n",
        "        print(step, cost.item())\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.6 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# Accuracy computation\n",
        "# True if hypothesis>0.5 else False\n",
        "'''torch.no_grad()는 test를 진행하는데 기울기를 계산하지 않겠다는 의미!!!'''\n",
        "with torch.no_grad():\n",
        "    hypothesis = model(X)\n",
        "    predicted = (hypothesis > 0.5).float()\n",
        "    accuracy = (predicted == Y).float().mean()\n",
        "    print('\\nHypothesis: ', hypothesis.detach().cpu().numpy(), '\\nCorrect: ', predicted.detach().cpu().numpy(), '\\nAccuracy: ', accuracy.item())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMtUYXOThd3v",
        "colab_type": "text"
      },
      "source": [
        "### Weight를 initialization 해서 학습하는 code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ni5WpOtBg7jJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################################################################################################\n",
        "############################################ Step.1 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# Lab 10 MNIST and softmax\n",
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# for reproducibility\n",
        "random.seed(777)\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)\n",
        "\n",
        "'''\n",
        "Hyperparameter 설정해주기!!\n",
        "'''\n",
        "\n",
        "# parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.2 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# MNIST dataset\n",
        "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
        "                          train=True,\n",
        "                          transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
        "                         train=False,\n",
        "                         transform=transforms.ToTensor(),\n",
        "                         download=True)\n",
        "\n",
        "# dataset loader\n",
        "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          drop_last=True)\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.3 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# nn layers\n",
        "linear1 = torch.nn.Linear(784, 256, bias=True)    # input이 784개였던 input의 크기를\n",
        "linear2 = torch.nn.Linear(256, 256, bias=True)\n",
        "linear3 = torch.nn.Linear(256, 10, bias=True)   # 마지막에 10개로 줄여준다는 의미!!! (input 데이터의 class가 10개임!)\n",
        "relu = torch.nn.ReLU()\n",
        "\n",
        "# weight initialization (xavier)\n",
        "torch.nn.init.xavier_uniform_(linear1.weight)\n",
        "torch.nn.init.xavier_uniform_(linear2.weight)\n",
        "torch.nn.init.xavier_uniform_(linear3.weight)\n",
        "\n",
        "# model\n",
        "model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3).to(device)\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.4 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "total_batch = len(data_loader)\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "\n",
        "    for X, Y in data_loader:\n",
        "        # reshape input image into [batch_size by 784]\n",
        "        # label is not one-hot encoded\n",
        "        X = X.view(-1, 28 * 28).to(device)\n",
        "        Y = Y.to(device)\n",
        "'''\n",
        "여기서 꼭 zero_grad()를 해줘야 됨!\n",
        "직전 for 문에서 계산되었던 gradient가 그대로 저장되어있기 때문에\n",
        "0으로 만들어줘야 제대로 학습이 됨!!\n",
        "'''\n",
        "        optimizer.zero_grad()\n",
        "        hypothesis = model(X)\n",
        "        cost = criterion(hypothesis, Y)\n",
        "        cost.backward()\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.5 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_cost += cost / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning finished')\n",
        "\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.6 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# Test the model using test sets\n",
        "'''torch.no_grad()는 test를 진행하는데 기울기를 계산하지 않겠다는 의미!!!'''\n",
        "with torch.no_grad():\n",
        "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
        "    Y_test = mnist_test.test_labels.to(device)\n",
        "\n",
        "    prediction = model(X_test)\n",
        "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "    accuracy = correct_prediction.float().mean()\n",
        "    print('Accuracy:', accuracy.item())\n",
        "\n",
        "    # Get one and predict\n",
        "    r = random.randint(0, len(mnist_test) - 1)\n",
        "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
        "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
        "\n",
        "    print('Label: ', Y_single_data.item())\n",
        "    single_prediction = model(X_single_data)\n",
        "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bCY9Vv-jPWc",
        "colab_type": "text"
      },
      "source": [
        "### Dropout과 initialization을 사용하는 code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwEpIQqqjPCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################################################################################################\n",
        "############################################ Step.1 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# Lab 10 MNIST and softmax\n",
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# for reproducibility\n",
        "random.seed(777)\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)\n",
        "\n",
        "# parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "drop_prob = 0.3\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.2 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# MNIST dataset\n",
        "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
        "                          train=True,\n",
        "                          transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
        "                         train=False,\n",
        "                         transform=transforms.ToTensor(),\n",
        "                         download=True)\n",
        "\n",
        "# dataset loader\n",
        "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          drop_last=True)\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.3 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# nn layers\n",
        "linear1 = torch.nn.Linear(784, 512, bias=True)\n",
        "linear2 = torch.nn.Linear(512, 512, bias=True)\n",
        "linear3 = torch.nn.Linear(512, 512, bias=True)\n",
        "linear4 = torch.nn.Linear(512, 512, bias=True)\n",
        "linear5 = torch.nn.Linear(512, 10, bias=True)\n",
        "relu = torch.nn.ReLU()\n",
        "dropout = torch.nn.Dropout(p=drop_prob)\n",
        "\n",
        "# xavier initialization\n",
        "torch.nn.init.xavier_uniform_(linear1.weight)\n",
        "torch.nn.init.xavier_uniform_(linear2.weight)\n",
        "torch.nn.init.xavier_uniform_(linear3.weight)\n",
        "torch.nn.init.xavier_uniform_(linear4.weight)\n",
        "torch.nn.init.xavier_uniform_(linear5.weight)\n",
        "\n",
        "# model\n",
        "'''\n",
        "여기서 model의 layer를 어떻게 쌓아나가는지 주모오오오오옥~~~!!!\n",
        "linear -> relu -> dropout\n",
        "'''\n",
        "model = torch.nn.Sequential(linear1, relu, dropout,\n",
        "                            linear2, relu, dropout,\n",
        "                            linear3, relu, dropout,\n",
        "                            linear4, relu, dropout,\n",
        "                            linear5).to(device)\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.4 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "''' 여기서는 Adam optimizer를 사용했음!!!'''\n",
        "\n",
        "total_batch = len(data_loader)\n",
        "model.train()    # set the model to train mode (dropout=True)\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "\n",
        "    for X, Y in data_loader:\n",
        "        # reshape input image into [batch_size by 784]\n",
        "        # label is not one-hot encoded\n",
        "        X = X.view(-1, 28 * 28).to(device)\n",
        "        Y = Y.to(device)\n",
        "'''\n",
        "여기서 꼭 zero_grad()를 해줘야 됨!\n",
        "직전 for 문에서 계산되었던 gradient가 그대로 저장되어있기 때문에\n",
        "0으로 만들어줘야 제대로 학습이 됨!!\n",
        "'''\n",
        "        optimizer.zero_grad()\n",
        "        hypothesis = model(X)\n",
        "        cost = criterion(hypothesis, Y)\n",
        "        cost.backward()\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.5 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_cost += cost / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning finished')\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.6 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# Test model and check accuracy\n",
        "'''torch.no_grad()는 test를 진행하는데 기울기를 계산하지 않겠다는 의미!!!'''\n",
        "with torch.no_grad():\n",
        "    model.eval()    # set the model to evaluation mode (dropout=False)\n",
        "\n",
        "    # Test the model using test sets\n",
        "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
        "    Y_test = mnist_test.test_labels.to(device)\n",
        "\n",
        "    prediction = model(X_test)\n",
        "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "    accuracy = correct_prediction.float().mean()\n",
        "    print('Accuracy:', accuracy.item())\n",
        "\n",
        "    # Get one and predict\n",
        "    r = random.randint(0, len(mnist_test) - 1)\n",
        "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
        "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
        "\n",
        "    print('Label: ', Y_single_data.item())\n",
        "    single_prediction = model(X_single_data)\n",
        "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPrTfiO3j0gz",
        "colab_type": "text"
      },
      "source": [
        "### Batch normalization을 사용하는 code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwy0AJdrj0a-",
        "colab_type": "code",
        "outputId": "da0ec975-a4b7-42e5-ba90-480202a70e0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "######################################################################################################################\n",
        "############################################ Step.1 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# Lab 10 MNIST and softmax\n",
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# for reproducibility\n",
        "random.seed(777)\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)\n",
        "\n",
        "# parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "drop_prob = 0.3\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.2 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# MNIST dataset\n",
        "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
        "                          train=True,\n",
        "                          transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
        "                         train=False,\n",
        "                         transform=transforms.ToTensor(),\n",
        "                         download=True)\n",
        "\n",
        "# dataset loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          drop_last=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False,\n",
        "                                          drop_last=True)\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.3 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# nn layers\n",
        "linear1 = torch.nn.Linear(784, 32, bias=True)\n",
        "linear2 = torch.nn.Linear(32, 32, bias=True)\n",
        "linear3 = torch.nn.Linear(32, 10, bias=True)\n",
        "relu = torch.nn.ReLU()\n",
        "'''\n",
        "torch.nn.BatchNorm1d(num_features, eps = 1e-05, momentum = 0.1)\n",
        ":batch normalization을 하는 feature의 개수를 정해주면 간편간편쓰~!\n",
        "'''\n",
        "bn1 = torch.nn.BatchNorm1d(32)\n",
        "bn2 = torch.nn.BatchNorm1d(32)\n",
        "\n",
        "nn_linear1 = torch.nn.Linear(784, 32, bias=True)\n",
        "nn_linear2 = torch.nn.Linear(32, 32, bias=True)\n",
        "nn_linear3 = torch.nn.Linear(32, 10, bias=True)\n",
        "\n",
        "# model\n",
        "bn_model = torch.nn.Sequential(linear1, bn1, relu,\n",
        "                            linear2, bn2, relu,\n",
        "                            linear3).to(device)\n",
        "nn_model = torch.nn.Sequential(nn_linear1, relu,\n",
        "                               nn_linear2, relu,\n",
        "                               nn_linear3).to(device)\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.4 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\n",
        "bn_optimizer = torch.optim.Adam(bn_model.parameters(), lr=learning_rate)\n",
        "nn_optimizer = torch.optim.Adam(nn_model.parameters(), lr=learning_rate)\n",
        "'''여기서는 Adam optimizer를 사용했음!!!'''\n",
        "\n",
        "# Save Losses and Accuracies every epoch\n",
        "# We are going to plot them later\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "\n",
        "valid_losses = []\n",
        "valid_accs = []\n",
        "\n",
        "train_total_batch = len(train_loader)\n",
        "test_total_batch = len(test_loader)\n",
        "for epoch in range(training_epochs):\n",
        "    bn_model.train()  # set the model to train mode\n",
        "\n",
        "    for X, Y in train_loader:\n",
        "        # reshape input image into [batch_size by 784]\n",
        "        # label is not one-hot encoded\n",
        "        X = X.view(-1, 28 * 28).to(device)\n",
        "        Y = Y.to(device)\n",
        "\n",
        "# 여기서 꼭 zero_grad()를 해줘야 됨!\n",
        "# 직전 for 문에서 계산되었던 gradient가 그대로 저장되어있기 때문에\n",
        "# 0으로 만들어줘야 제대로 학습이 됨!!\n",
        "\n",
        "\n",
        "        bn_optimizer.zero_grad()\n",
        "        bn_prediction = bn_model(X)\n",
        "        bn_loss = criterion(bn_prediction, Y)\n",
        "        bn_loss.backward()\n",
        "        bn_optimizer.step()\n",
        "\n",
        "        nn_optimizer.zero_grad()\n",
        "        nn_prediction = nn_model(X)\n",
        "        nn_loss = criterion(nn_prediction, Y)\n",
        "        nn_loss.backward()\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.5 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "        nn_optimizer.step()\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.6 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# torch.no_grad()는 test를 진행하는데 기울기를 계산하지 않겠다는 의미!!!\n",
        "    with torch.no_grad():\n",
        "        bn_model.eval()     # set the model to evaluation mode\n",
        "\n",
        "        # Test the model using train sets\n",
        "        bn_loss, nn_loss, bn_acc, nn_acc = 0, 0, 0, 0\n",
        "        for i, (X, Y) in enumerate(train_loader):\n",
        "            X = X.view(-1, 28 * 28).to(device)\n",
        "            Y = Y.to(device)\n",
        "\n",
        "            bn_prediction = bn_model(X)\n",
        "            bn_correct_prediction = torch.argmax(bn_prediction, 1) == Y\n",
        "            bn_loss += criterion(bn_prediction, Y)\n",
        "            bn_acc += bn_correct_prediction.float().mean()\n",
        "\n",
        "            nn_prediction = nn_model(X)\n",
        "            nn_correct_prediction = torch.argmax(nn_prediction, 1) == Y\n",
        "            nn_loss += criterion(nn_prediction, Y)\n",
        "            nn_acc += nn_correct_prediction.float().mean()\n",
        "\n",
        "        bn_loss, nn_loss, bn_acc, nn_acc = bn_loss / train_total_batch, nn_loss / train_total_batch, bn_acc / train_total_batch, nn_acc / train_total_batch\n",
        "\n",
        "        # Save train losses/acc\n",
        "        train_losses.append([bn_loss, nn_loss])\n",
        "        train_accs.append([bn_acc, nn_acc])\n",
        "        print(\n",
        "            '[Epoch %d-TRAIN] Batchnorm Loss(Acc): bn_loss:%.5f(bn_acc:%.2f) vs No Batchnorm Loss(Acc): nn_loss:%.5f(nn_acc:%.2f)' % (\n",
        "            (epoch + 1), bn_loss.item(), bn_acc.item(), nn_loss.item(), nn_acc.item()))\n",
        "        # Test the model using test sets\n",
        "        bn_loss, nn_loss, bn_acc, nn_acc = 0, 0, 0, 0\n",
        "        for i, (X, Y) in enumerate(test_loader):\n",
        "            X = X.view(-1, 28 * 28).to(device)\n",
        "            Y = Y.to(device)\n",
        "\n",
        "            bn_prediction = bn_model(X)\n",
        "            bn_correct_prediction = torch.argmax(bn_prediction, 1) == Y\n",
        "            bn_loss += criterion(bn_prediction, Y)\n",
        "            bn_acc += bn_correct_prediction.float().mean()\n",
        "\n",
        "            nn_prediction = nn_model(X)\n",
        "            nn_correct_prediction = torch.argmax(nn_prediction, 1) == Y\n",
        "            nn_loss += criterion(nn_prediction, Y)\n",
        "            nn_acc += nn_correct_prediction.float().mean()\n",
        "\n",
        "        bn_loss, nn_loss, bn_acc, nn_acc = bn_loss / test_total_batch, nn_loss / test_total_batch, bn_acc / test_total_batch, nn_acc / test_total_batch\n",
        "\n",
        "        # Save valid losses/acc\n",
        "        valid_losses.append([bn_loss, nn_loss])\n",
        "        valid_accs.append([bn_acc, nn_acc])\n",
        "        print(\n",
        "            '[Epoch %d-VALID] Batchnorm Loss(Acc): bn_loss:%.5f(bn_acc:%.2f) vs No Batchnorm Loss(Acc): nn_loss:%.5f(nn_acc:%.2f)' % (\n",
        "                (epoch + 1), bn_loss.item(), bn_acc.item(), nn_loss.item(), nn_acc.item()))\n",
        "        print()\n",
        "\n",
        "print('Learning finished')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:01, 9439105.26it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to MNIST_data/MNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/28881 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 142344.13it/s]           \n",
            "  0%|          | 0/1648877 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 2251248.66it/s]                            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 52550.60it/s]            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n",
            "[Epoch 1-TRAIN] Batchnorm Loss(Acc): bn_loss:0.18205(bn_acc:0.95) vs No Batchnorm Loss(Acc): nn_loss:0.31072(nn_acc:0.91)\n",
            "[Epoch 1-VALID] Batchnorm Loss(Acc): bn_loss:0.19104(bn_acc:0.95) vs No Batchnorm Loss(Acc): nn_loss:0.30096(nn_acc:0.91)\n",
            "\n",
            "[Epoch 2-TRAIN] Batchnorm Loss(Acc): bn_loss:0.11432(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.22023(nn_acc:0.94)\n",
            "[Epoch 2-VALID] Batchnorm Loss(Acc): bn_loss:0.13049(bn_acc:0.96) vs No Batchnorm Loss(Acc): nn_loss:0.22086(nn_acc:0.94)\n",
            "\n",
            "[Epoch 3-TRAIN] Batchnorm Loss(Acc): bn_loss:0.09336(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.17812(nn_acc:0.95)\n",
            "[Epoch 3-VALID] Batchnorm Loss(Acc): bn_loss:0.12276(bn_acc:0.96) vs No Batchnorm Loss(Acc): nn_loss:0.18468(nn_acc:0.95)\n",
            "\n",
            "[Epoch 4-TRAIN] Batchnorm Loss(Acc): bn_loss:0.07610(bn_acc:0.98) vs No Batchnorm Loss(Acc): nn_loss:0.15115(nn_acc:0.96)\n",
            "[Epoch 4-VALID] Batchnorm Loss(Acc): bn_loss:0.11271(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.16085(nn_acc:0.95)\n",
            "\n",
            "[Epoch 5-TRAIN] Batchnorm Loss(Acc): bn_loss:0.06315(bn_acc:0.98) vs No Batchnorm Loss(Acc): nn_loss:0.13863(nn_acc:0.96)\n",
            "[Epoch 5-VALID] Batchnorm Loss(Acc): bn_loss:0.09906(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.15549(nn_acc:0.95)\n",
            "\n",
            "[Epoch 6-TRAIN] Batchnorm Loss(Acc): bn_loss:0.05637(bn_acc:0.98) vs No Batchnorm Loss(Acc): nn_loss:0.11214(nn_acc:0.97)\n",
            "[Epoch 6-VALID] Batchnorm Loss(Acc): bn_loss:0.09873(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.13308(nn_acc:0.96)\n",
            "\n",
            "[Epoch 7-TRAIN] Batchnorm Loss(Acc): bn_loss:0.05073(bn_acc:0.98) vs No Batchnorm Loss(Acc): nn_loss:0.10076(nn_acc:0.97)\n",
            "[Epoch 7-VALID] Batchnorm Loss(Acc): bn_loss:0.09781(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.12655(nn_acc:0.96)\n",
            "\n",
            "[Epoch 8-TRAIN] Batchnorm Loss(Acc): bn_loss:0.04605(bn_acc:0.99) vs No Batchnorm Loss(Acc): nn_loss:0.08977(nn_acc:0.97)\n",
            "[Epoch 8-VALID] Batchnorm Loss(Acc): bn_loss:0.09358(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.11726(nn_acc:0.96)\n",
            "\n",
            "[Epoch 9-TRAIN] Batchnorm Loss(Acc): bn_loss:0.04013(bn_acc:0.99) vs No Batchnorm Loss(Acc): nn_loss:0.08523(nn_acc:0.97)\n",
            "[Epoch 9-VALID] Batchnorm Loss(Acc): bn_loss:0.09475(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.12059(nn_acc:0.96)\n",
            "\n",
            "[Epoch 10-TRAIN] Batchnorm Loss(Acc): bn_loss:0.03882(bn_acc:0.99) vs No Batchnorm Loss(Acc): nn_loss:0.07669(nn_acc:0.98)\n",
            "[Epoch 10-VALID] Batchnorm Loss(Acc): bn_loss:0.09874(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.11307(nn_acc:0.97)\n",
            "\n",
            "[Epoch 11-TRAIN] Batchnorm Loss(Acc): bn_loss:0.03749(bn_acc:0.99) vs No Batchnorm Loss(Acc): nn_loss:0.07168(nn_acc:0.98)\n",
            "[Epoch 11-VALID] Batchnorm Loss(Acc): bn_loss:0.10003(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.11513(nn_acc:0.97)\n",
            "\n",
            "[Epoch 12-TRAIN] Batchnorm Loss(Acc): bn_loss:0.03096(bn_acc:0.99) vs No Batchnorm Loss(Acc): nn_loss:0.06543(nn_acc:0.98)\n",
            "[Epoch 12-VALID] Batchnorm Loss(Acc): bn_loss:0.09165(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.11012(nn_acc:0.97)\n",
            "\n",
            "[Epoch 13-TRAIN] Batchnorm Loss(Acc): bn_loss:0.03157(bn_acc:0.99) vs No Batchnorm Loss(Acc): nn_loss:0.06158(nn_acc:0.98)\n",
            "[Epoch 13-VALID] Batchnorm Loss(Acc): bn_loss:0.09387(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.11290(nn_acc:0.97)\n",
            "\n",
            "[Epoch 14-TRAIN] Batchnorm Loss(Acc): bn_loss:0.03102(bn_acc:0.99) vs No Batchnorm Loss(Acc): nn_loss:0.05819(nn_acc:0.98)\n",
            "[Epoch 14-VALID] Batchnorm Loss(Acc): bn_loss:0.09458(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.11470(nn_acc:0.97)\n",
            "\n",
            "[Epoch 15-TRAIN] Batchnorm Loss(Acc): bn_loss:0.02475(bn_acc:0.99) vs No Batchnorm Loss(Acc): nn_loss:0.05174(nn_acc:0.98)\n",
            "[Epoch 15-VALID] Batchnorm Loss(Acc): bn_loss:0.09618(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.11095(nn_acc:0.97)\n",
            "\n",
            "Learning finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8KkX6DaR4ve",
        "colab_type": "code",
        "outputId": "92575c3f-1668-4798-b5c4-b2f2b5b13eee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_compare(loss_list: list, ylim=None, title=None) -> None:\n",
        "    bn = [i[0] for i in loss_list]\n",
        "    nn = [i[1] for i in loss_list]\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.plot(bn, label='With BN')\n",
        "    plt.plot(nn, label='Without BN')\n",
        "    if ylim:\n",
        "        plt.ylim(ylim)\n",
        "\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid('on')\n",
        "    plt.show()\n",
        "\n",
        "plot_compare(train_losses, title='Training Loss at Epoch')\n",
        "plot_compare(train_accs, [0, 1.0], title='Training Acc at Epoch')\n",
        "plot_compare(valid_losses, title='Validation Loss at Epoch')\n",
        "plot_compare(valid_accs, [0, 1.0], title='Validation Acc at Epoch')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3AAAAJOCAYAAAD27eW+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzdd5hV1aH38e+awoCUEURGKdKsOGMQ\nsQABwd4bE2OMUVFEfTUm6jU3977GGGPy5sYSYzSxl7RrwY7YA1hAAyqRJpEmAgEEBBmBoa33j3Mg\nA1JmYGb2OWe+n+eZhzlnl/M7s/76sfZeO8QYkSRJkiRlvrykA0iSJEmSqscCJ0mSJElZwgInSZIk\nSVnCAidJkiRJWcICJ0mSJElZwgInSZIkSVnCAidJqnUhhPwQQkUIYa/a3Fd1J4RQEEKIIYROSWeR\nJG2dBU6SRLpAbfhZH0JYWeX1d2t6vhjjuhhjsxjj7Nrct6ZCCDeHEB6p7fPurOrkCiHM2WwcKkII\nd9RTRElShipIOoAkKXkxxmYbfg8hzAIGxxhf39r+IYSCGOPa+sjWwJ0YYxyZdAhJUuZwBk6StF3p\nGaPHQwj/G0JYDpwXQugVQng3hLA0hPCvEMKdIYTC9P6bXI4XQvhzevtLIYTlIYQxIYTONd03vf3E\nEMI/QwjLQgi/CyG8E0K4cAe+04EhhFHp/BNCCCdX2XZKCGFK+vPnhBCuTr/fJoQwPH3MkhDCm9s4\n/13pY78MIYwNIfTecG7gR8B307Nq7+9A9sEhhDdDCL9P/x2mhBAGVNnePoQwLJ3xkxDCRVW2FYQQ\nfhJCmJ7ONi6E0LbK6Y8PIUwLIXwRQrizptkkSXXLAidJqq4zgb8CxcDjwFrgB0BroA9wAnDpNo4/\nF/gJ0AqYDfy8pvuGENoATwDXpT93JnBYTb9ICKERMAx4EdgduBp4PISwd3qXh4GLY4zNgYOAUen3\nrwNmpI/ZA7h+Gx/zXvrYVsBQ4MkQQlGMcRjwa+Av6UtHD6lp/rTewMek/g4/B54OIeya3vY4qb9N\nW+DbwK9DCEdW+Q7lpMZrV2AwsKrKeU8CDgEOJlXUj9nBfJKkOmCBkyRV19sxxhdijOtjjCtjjGNj\njO/FGNfGGGcA9wFHbuP4oTHGcTHGNcBfgO47sO8pwPgY43Ppbb8BFu3Ad+kDNAJuiTGuSV8u+hJw\nTnr7GqBbCKF5jHFJjPGDKu+3BfaKMa6OMW51Bi7G+Kf0sWtJFbYWwN5b238rhqVn+zb8DKqy7V/A\n79L5/0qqsJ2Ynq08DPhxjHFVOvvDwPfSxw0G/jvG+El6LMfHGJdUOe//izEuizHOAkay7XGSJNUz\nC5wkqbo+q/oihLB/COHFEML8EMKXwE2kZoO2Zn6V31cAzba24zb2bVs1R4wxAnOqkX1zbYHZ6eM3\n+BRol/79TOA0YHYIYWQI4fD0+79K7/dG+hLE67b2ASGEH4UQPg4hLAO+AJqy7b/PlpwSY9y1ys/D\nVbbN2UL+tumfRTHGr7by3ToA07fxmTUZJ0lSPbPASZKqK272+l5gIrB3jLEFcAMQ6jjDv4D2G16E\nEAL/LiY1MQ/okD5+g72AuQDpmcXTgDakLrV8LP3+lzHGq2OMnYAzgP+scmniRun70a4BBpK6TLEl\nUMG//z6b/y13RPvNXu+V/l7zgNYhhKZb+m6kCnDXWvh8SVICLHCSpB3VHFgGfBVCOIBt3/9WW4YB\nPUIIp4YQCkjdg7f7do7JDyE0rvJTBIwmdQ/ftSGEwhDCUaTu/Xo8hNAkhHBuCKFF+jLN5cB6gPTn\ndk0Xv2XAug3bNtM8ff5FQCFwI6kZuA0WAJ02K5A1tWcI4cr0oiTnkCplL8cYZwLjgF+GEIpCCN2B\nQcCf08c9ANy84XuEELqHEFrtRA5JUj2ywEmSdtS1wAWkCs69pBbOqFMxxgWkFuW4HVhMqrR8CFRu\n47DzgJVVfqbGGCuBU4HTSZWsO4FzY4yfpI+5APg0fWnoxelzAOwH/I3UbNo7wG9jjG9t4TOHA68D\nnwCzgC9JzR5u8Dipe/CWhBD+vo3sL4VNnwP3ZJVto4EDgSWkCuLAGOMX6W3fBvYhdTnkUFL3vI1M\nb7sFeBZ4I53rPqDxNjJIkjJI2PTyeUmSskcIIZ/UJYPlWylSOSmEMBg4L8bYP+kskqT65QycJCmr\nhBBOCCHsmr4U8iekVobc1iyWJEk5wwInSco23yT1LLbPgeOBM9OXREqSlPO8hFKSJEmSsoQzcJIk\nSZKUJQqSDrC51q1bx06dOiUd42u++uormjZtuv0dlTEcs+zjmGUfxyy7OF7ZxzHLPo5ZdsnU8Xr/\n/fcXxRi3+JicjCtwnTp1Yty4cUnH+JqRI0fSv3//pGOoBhyz7OOYZR/HLLs4XtnHMcs+jll2ydTx\nCiF8urVtXkIpSZIkSVnCAidJkiRJWcICJ0mSJElZIuPugZMkSZKUnDVr1jBnzhxWrVqVdJQ6V1xc\nzJQpUxL7/MaNG9O+fXsKCwurfYwFTpIkSdJGc+bMoXnz5nTq1IkQQtJx6tTy5ctp3rx5Ip8dY2Tx\n4sXMmTOHzp07V/s4L6GUJEmStNGqVavYbbfdcr68JS2EwG677VbjmU4LnCRJkqRNWN7qx478nS1w\nkiRJkpQlLHCSJEmSMsbVV1/NHXfcsfH18ccfz+DBgze+vvbaa7n99tuZN28e5eXlAIwfP57hw4dv\n3OfGG2/k1ltv3e5nlZaWUlZWRvfu3SkrK+O5557buC2EwLXXXrvx9a233sqNN964M1+tVljgJEmS\nJGWMPn36MHr0aADWr1/PokWLmDRp0sbto0ePpnfv3rRt25ahQ4cCXy9wNTFixAjGjx/P0KFDueqq\nqza+X1RUxNNPP82iRYt24tvUPgucJEmSpIzRu3dvxowZA8CkSZMoLS2lefPmfPHFF1RWVjJlyhR6\n9OjBrFmzKC0tZfXq1dxwww08/vjjdO/enccffxyAyZMn079/f7p06cKdd9653c/98ssvadmy5cbX\nBQUFDBkyhN/85jd180V3kI8RkCRJkrRFP3thEpPnfVmr5+zWtgU/PfXArW5v27YtBQUFzJ49m9Gj\nR9OrVy/mzp3LmDFjKC4upqysjEaNGm3cv1GjRtx0002MGzeOu+66C0hdQvnxxx8zYsQIli9fzn77\n7cfll1++xeetDRgwgBgjM2bM4Iknnthk2xVXXMFBBx3Ej370o1r69jvPAidJkiQpo/Tu3ZvRo0cz\nevRorrnmGubOncvo0aMpLi6mT58+1TrHySefTFFREUVFRbRp04YFCxbQvn37r+03YsQIWrduzfTp\n0zn66KPp378/zZo1A6BFixacf/753HnnnTRp0qRWv+OOssBJkiRJ2qJtzZTVpQ33wU2YMIHS0lI6\ndOjAbbfdRosWLRg0aFC1zlFUVLTx9/z8fNauXbvN/bt27UpJSQmTJ0/msMMO2/j+D3/4Q3r06FHt\nz61r3gMnSZIkKaP07t2bYcOG0apVK/Lz82nVqhVLly5lzJgx9O7d+2v7N2/enOXLl+/UZy5cuJCZ\nM2fSsWPHTd5v1aoVZ599Ng8++OBOnb+2WOAkSZIkZZSysjIWLVrEEUccscl7xcXFtG7d+mv7Dxgw\ngMmTJ2+yiEl1DRgwgO7duzNgwAB+9atfUVJS8rV9rr322oxZjdJLKCVJkiRllPz8fL78ctPFUx55\n5JFNXnfq1ImJEycCqVmysWPHbvV8G/bb0vvNmzff4raKioqNv5eUlLBixYrqRK9zzsBJkiRJUpaw\nwEmSJElSlrDASZIkSVKWsMBJkiRJUpawwFVXjEknkCRJktTAWeCqY9Y7HPHuxbBoWtJJJEmSJDVg\nFrjqaNWZosolMHFo0kkkSZKknHb11Vdzxx13bHx9/PHHM3jw4I2vr732Wm6//XbmzZtHeXk5AOPH\nj2f48OEb97nxxhu59dZbayXPI488wrx587a47cILL6Rz5850796d/fffn5/97Gcbt/Xv35+ePXtu\nfD1u3Dj69++/03kscNXRoi3Lig+ECUO9lFKSJEmqQ3369GH06NEArF+/nkWLFjFp0qSN20ePHk3v\n3r1p27YtQ4emJlg2L3C1aVsFDuCWW25h/PjxjB8/nkcffZSZM2du3LZw4UJeeumlWs1jgaumBSV9\nYfEnMP+jpKNIkiRJOat3796MGTMGgEmTJlFaWkrz5s354osvqKysZMqUKfTo0YNZs2ZRWlrK6tWr\nueGGG3j88cfp3r07jz/+OACTJ0+mf//+dOnShTvvvHPj+W+//XZKS0spLS3l7rvvBth4rg1uvfVW\nbrzxRoYOHcq4ceP47ne/S/fu3Vm5cuVWc69atQqApk2bbnzvuuuu4xe/+EXt/XGAglo9Ww77fPfe\n7Dft/tQs3J7fSDqOJEmSVPde+jHMn1C759yjDE781VY3t23bloKCAmbPns3o0aPp1asXc+fOZcyY\nMRQXF1NWVkajRo027t+oUSNuuukmxo0bx1133QWkLqH8+OOPGTFiBMuXL2e//fbj8ssv56OPPuLh\nhx/mvffeI8bIoYceyvHHH0/Lli23mKW8vJy77rqLW2+9dZPLIau67rrruPnmm5k2bRpXXXUVbdq0\n2bitV69ePPPMM4wYMYLmzZvvyF/ra5yBq6a1hS2g61Ew8WlYvz7pOJIkSVLO6t27N6NHj95Y4Hr1\n6rXxdZ8+fap1jpNPPpmioiJat25NmzZtWLBgAW+//TZnnnkmTZs2pVmzZpx66qm89dZbO5V1wyWU\n8+fP54033th4+ecG119/PTfffPNOfUZVzsDVRGk5fDIEPnsPOvZKOo0kSZJUt7YxU1aXNtwHN2HC\nBEpLS+nQoQO33XYbLVq0YNCgQdU6R1FR0cbf8/PzWbt27Vb3LSgoYH2VSZoNl0PWRLNmzejfvz9v\nv/02vXv33vj+UUcdxfXXX8+7775b43NuiTNwNbH/SVDQ2NUoJUmSpDrUu3dvhg0bRqtWrcjPz6dV\nq1YsXbqUMWPGbFKONmjevDnLly/f7nn79u3Ls88+y4oVK/jqq68YNmwYffv2paSkhIULF7J48WIq\nKysZNmxYjc+9du1a3nvvPbp27fq1bddffz2//vWvt3uO6rDA1URRc9j3BJj0LKzbeoOXJEmStOPK\nyspYtGgRRxxxxCbvFRcX07p166/tP2DAACZPnrzJIiZb0qNHDy688EIOO+wwDj/8cM4//3wOPvhg\nCgsLueGGGzjssMM49thj2X///Tcec+GFF3LZZZdtdRGT6667ju7du3PQQQdRVlbGWWed9bV9Tjrp\nJHbfffea/hm2yEsoa6qsHCY/CzNHwd5HJ51GkiRJyjn5+fl8+eWXm7z3yCOPbPK6U6dOTJw4EYBW\nrVoxduzYrZ5vw34A11xzDddccw3AJjNrV111FVddddXXjh04cCADBw7c4nk3z1TVyJEjN3n9/vvv\nb3XfmnAGrqb2PhaKWsDEp5JOIkmSJKmBscDVVGFjOOBUmPICrKn5zY2SJEmStKMscDuidCBUfgnT\nXks6iSRJklTrYoxJR2gQduTvbIHbEZ2PhKa7px7qLUmSJOWQxo0bs3jxYktcHYsxsnjxYho3blyj\n41zEZEfkF0C3M+DDP0Hl8tTqlJIkSVIOaN++PXPmzOHzzz9POkqdW7VqVY0LVG1q3Lgx7du3r9Ex\nFrgdVVYOY++Hj4fDN76ddBpJkiSpVhQWFtK5c+ekY9SLkSNHcvDBBycdo0aqdQllCOGEEMLUEMK0\nEMKPt7D9shDChBDC+BDC2yGEblW2/Vf6uKkhhONrM3yi2h8GxR18qLckSZKkerPdAhdCyAfuBk4E\nugHfqVrQ0v4aYyyLMXYHfg3cnj62G3AOcCBwAvD79PmyX14elJ4F0/8GK5YknUaSJElSA1CdGbjD\ngGkxxhkxxtXAY8DpVXeIMVZ9yl5TYMMdj6cDj8UYK2OMM4Fp6fPlhtJyWL829WBvSZIkSapj1bkH\nrh3wWZXXc4DDN98phHAFcA3QCDiqyrHvbnZsuy0cOwQYAlBSUvK1p5ZngoqKiq/nipFDd2nPmrce\nZHxFl0Ryaeu2OGbKaI5Z9nHMsovjlX0cs+zjmGWXbByvWlvEJMZ4N3B3COFc4Hrgghocex9wH0DP\nnj1j//79aytWrRk5ciRbzBW+ByN/Rf8e+0KLtvWeS1u31TFTxnLMso9jll0cr+zjmGUfxyy7ZON4\nVecSyrlAhyqv26ff25rHgDN28NjsU1oORJj4dNJJJEmSJOW46hS4scA+IYTOIYRGpBYleb7qDiGE\nfaq8PBn4JP3788A5IYSiEEJnYB/g7zsfO4O03hv2/IarUUqSJEmqc9stcDHGtcCVwCvAFOCJGOOk\nEMJNIYTT0rtdGUKYFEIYT+o+uAvSx04CngAmAy8DV8QY19XB90hWaTnM+xAWT086iSRJkqQcVq17\n4GKMw4Hhm713Q5Xff7CNY38B/GJHA2aF0rPgtZ/AxKfgyB8lnUaSJElSjqrWg7y1HcXtYa/eMGEo\nxLj9/SVJkiRpB1jgakvZQFg0FRZMTDqJJEmSpBxlgast3c6AkJ+ahZMkSZKkOmCBqy1NW0PXAanH\nCXgZpSRJkqQ6YIGrTaXlsGw2fJZbT0qQJEmSlBkscLVp/5OhoLHPhJMkSZJUJyxwtalxC9jnOJj0\nDKxbm3QaSZIkSTnGAlfbysrhq89h1ptJJ5EkSZKUYyxwtW2f46BRc5jwVNJJJEmSJOUYC1xtK2wC\nB5wCU16AtZVJp5EkSZKUQyxwdaG0HCqXwSevJZ1EkiRJUg6xwNWFLkfCLru5GqUkSZKkWmWBqwv5\nhdDtDJj6MlRWJJ1GkiRJUo6wwNWVsnJYuxKmDk86iSRJkqQcYYGrKx2OgBbtYIKXUUqSJEmqHRa4\nupKXB6VnwfQ3YMWSpNNIkiRJygEWuLpUWg7r18Lk55JOIkmSJCkHWODq0p7fgN32hok+1FuSJEnS\nzrPA1aUQUrNws96GL+clnUaSJElSlrPA1bWyciDCpGeSTiJJkiQpy1ng6lrrfWCPg1yNUpIkSdJO\ns8DVh7JymPcBLJ6edBJJkiRJWcwCVx8OPCv178Snk80hSZIkKatZ4OrDrh1gr14wcSjEmHQaSZIk\nSVnKAldfSgfC5x/DgklJJ5EkSZKUpSxw9eXAMyHkp2bhJEmSJGkHWODqS9PW0KV/6qHeXkYpSZIk\naQdY4OpTWTksnQ1zxiadRJIkSVIWssDVp/1PgfwinwknSZIkaYdY4OpT4xaw73Ew6RlYtzbpNJIk\nSZKyjAWuvpWWw1cLYdZbSSeRJEmSlGUscPVt3+OhUXNXo5QkSZJUYxa4+lbYBPY/Gaa8AGsrk04j\nSZIkKYtY4JJQVg6rlsG0N5JOIkmSJCmLWOCS0KU/NGnlZZSSJEmSasQCl4T8QjjwDJj6Eqz+Kuk0\nkiRJkrKEBS4ppeWwZkWqxEmSJElSNVjgkrJXL2jRzod6S5IkSao2C1xS8vLgwDNh2uuwYknSaSRJ\nkiRlAQtcksrKYf2a1CMFJEmSJGk7LHBJ2rM7tOrqapSSJEmSqsUCl6QQUrNwM9+C5fOTTiNJkiQp\nw1ngklZaDkSY9EzSSSRJkiRlOAtc0nbfF/YoczVKSZIkSdtlgcsEpeUwdxwsmZl0EkmSJEkZzAKX\nCUoHpv6d+FSyOSRJkiRlNAtcJti1A3Q4wgInSZIkaZsscJmirBwWToYFk5NOIkmSJClDWeAyRbcz\nIOT5TDhJkiRJW2WByxTNdofOR6Yuo4wx6TSSJEmSMpAFLpOUlcMXs2Du+0knkSRJkpSBLHCZZP9T\nIL+Rz4STJEmStEUWuEzSZFfY5ziY9DSsX5d0GkmSJEkZxgKXaUoHQsUCmPV20kkkSZIkZRgLXKbZ\n9wRo1MzVKCVJkiR9jQUu0zTaBfY7CSY/D2tXJ51GkiRJUgaxwGWisnJYtRSmv5F0EkmSJEkZxAKX\niboMgCYtXY1SkiRJ0iYscJmooBF0Ox2mDofVXyWdRpIkSVKGsMBlqtJyWLMCpr6UdBJJkiRJGcIC\nl6k69obme8LEp5JOIkmSJClDWOAyVV4+HHgWfPIarPwi6TSSJEmSMoAFLpOVDYT1a2DKC0knkSRJ\nkpQBLHCZrG0PaNnZ1SglSZIkARa4zBZC6plws96C5QuSTiNJkiQpYRa4TFdaDnE9THom6SSSJEmS\nEmaBy3Rt9oeSUpjoZZSSJElSQ2eBywalA2HOWPhiVtJJJEmSJCXIApcNSgem/vWZcJIkSVKDZoHL\nBi07QvvDYIIFTpIkSWrILHDZoqwcFk6ChVOSTiJJkiQpIRa4bHHgmRDyfCacJEmS1IBZ4LJFszbQ\nuV9qNcoYk04jSZIkKQEWuGxSWp5aiXLuB0knkSRJkpQAC1w2OeBUyG/kM+EkSZKkBsoCl02a7Ap7\nHwsTn4b165JOI0mSJKmeWeCyTdlAqJgPn76TdBJJkiRJ9cwCl232PREKm7oapSRJktQAVavAhRBO\nCCFMDSFMCyH8eAvbrwkhTA4hfBRCeCOE0LHKtnUhhPHpn+drM3yD1GgX2P8kmPwcrF2ddBpJkiRJ\n9Wi7BS6EkA/cDZwIdAO+E0LottluHwI9Y4wHAUOBX1fZtjLG2D39c1ot5W7YSsth1VKY/rekk0iS\nJEmqR9WZgTsMmBZjnBFjXA08BpxedYcY44gY44r0y3eB9rUbU5voehQ03hUmPpV0EkmSJEn1KMTt\nPBQ6hFAOnBBjHJx+/T3g8BjjlVvZ/y5gfozx5vTrtcB4YC3wqxjjs1s4ZggwBKCkpOSQxx57bMe/\nUR2pqKigWbNmScfYaN+pd1Oy4E3e6fNH1ucXJR0nI2XamGn7HLPs45hlF8cr+zhm2ccxyy6ZOl4D\nBgx4P8bYc0vbCmrzg0II5wE9gSOrvN0xxjg3hNAF+FsIYUKMcXrV42KM9wH3AfTs2TP279+/NmPV\nipEjR5JRuTrmwaOv0q/kKyg9Puk0GSnjxkzb5ZhlH8csuzhe2ccxyz6OWXbJxvGqziWUc4EOVV63\nT7+3iRDCMcD/BU6LMVZueD/GODf97wxgJHDwTuTVBh37QLM9vIxSkiRJakCqU+DGAvuEEDqHEBoB\n5wCbrCYZQjgYuJdUeVtY5f2WIYSi9O+tgT7A5NoK36Dl5UPpWfDJq7ByadJpJEmSJNWD7Ra4GONa\n4ErgFWAK8ESMcVII4aYQwoZVJW8BmgFPbva4gAOAcSGEfwAjSN0DZ4GrLaXlsG41fDws6SSSJEmS\n6kG17oGLMQ4Hhm/23g1Vfj9mK8eNBsp2JqC2oV0PaNk59VDvg89LOo0kSZKkOlatB3krQ4UApQNh\n5iioWLj9/SVJkiRlNQtctisrh7geJn3t6QySJEmScowFLtu1OQDaHAgThyadRJIkSVIds8DlgrKB\n8Nl7sHR20kkkSZIk1SELXC4oHZj612fCSZIkSTnNApcLWnaC9ofCBAucJEmSlMsscLmitBwWTIDP\npyadRJIkSVIdscDligPPhJCXeiacJEmSpJxkgcsVzUugU9/UapQxJp1GkiRJUh2wwOWSsnJYMgPm\nfZh0EkmSJEl1wAKXSw44FfIKXY1SkiRJylEWuFzSpCXscyxMfBrWr086jSRJkqRaZoHLNaUDYfk8\nmD066SSSJEmSapkFLtfsdyIU7uJqlJIkSVIOssDlmkZNYb+TYPJzsG5N0mkkSZIk1SILXC4qHQgr\nl8D0EUknkSRJklSLLHC5aO+joXFx6plwkiRJknKGBS4XFRTBAafBxy/C6hVJp5EkSZJUSyxwuaqs\nHFZXwCevJJ1EkiRJUi2xwOWqTn2hWYmrUUqSJEk5xAKXq/Ly4cAz4ZPXYNWypNNIkiRJqgUWuFxW\nWg7rKmHKsKSTSJIkSaoFFrhc1r4n7NrR1SglSZKkHGGBy2UhpJ4JN2MUVHyedBpJkiRJO8kCl+vK\nyiGug8nPJp1EkiRJ0k6ywOW6kgNh9wNcjVKSJEnKARa4hqBsIHz2Liz9LOkkkiRJknaCBa4hKB2Y\n+nfiU8nmkCRJkrRTLHANQasu0O4QV6OUJEmSspwFrqEoLYf5E+DzfyadRJIkSdIOssA1FAeeCQRn\n4SRJkqQsZoFrKFrsCZ2+mVqNMsak00iSJEnaARa4hqSsHJZMh3+NTzqJJEmSpB1ggWtIDjgN8gp9\nJpwkSZKUpSxwDckurWDvo2HSM7B+fdJpJEmSJNWQBa6hKS2HL+fC7DFJJ5EkSZJUQxa4hma/E6Gg\niatRSpIkSVnIAtfQFDVLlbhJz8K6NUmnkSRJklQDFriGqKwcVi6BGSOTTiJJkiSpBixwDdHex0Dj\nYlejlCRJkrKMBa4hKiiCA06Fj4fBmpVJp5EkSZJUTRa4hqq0HFZXwD9fSTqJJEmSpGqywDVUnftB\n0zYw8amkk0iSJEmqJgtcQ5WXDweemZqBW/Vl0mkkSZIkVYMFriErK4d1lal74SRJkiRlPAtcQ9b+\nUGi9L4z4JaxYknQaSZIkSdthgWvIQoAz74Xl8+G5KyDGpBNJkiRJ2gYLXEPXrgcc93OYOhze/X3S\naSRJkiRtgwVOcPhlsP8p8NpPYc77SaeRJEmStBUWOKUupTz9Lmi+Jwy9EFYuTTqRJEmSpC2wwCml\nSUv41sPw5Tzvh5MkSZIylAVO/9a+Jxzzs9RjBd67N+k0kiRJkjZjgdOmel0B+54Ir14Pcz9IOo0k\nSZKkKixw2lQIcMbvoVkJDB0Eq5YlnUiSJElSmgVOX7dLq9T9cMvmwPPf9344SZIkKUNY4LRlHQ6D\no2+Ayc/B2AeSTiNJkiQJC5y2pdf3YZ/j4JX/hnnjk04jSZIkNXgWOG1dXh6ccQ/s0hqevBBWfZl0\nIkmSJKlBs8Bp25ruBuUPwdLZ8MIPvB9OkiRJSpAFTtvXsRccdT1MehrefzjpNJIkSVKDZYFT9fT5\nIex9DLz0Y5g/Iek0kiRJUoNkgVP15OXBmfemHjHwxAVQuTzpRJIkSVKDY4FT9TVtDQMfhC9mwrCr\nvR9OkiRJqmcWONVMpz7Q/79hwpPwwR+TTiNJkiQ1KBY41Vzfa6DLAHjpR7BgUtJpJEmSpAbDAqea\ny8uHs+6DxsWp58NVViSdSFkzpy4AACAASURBVJIkSWoQLHDaMc3awMAHYPE0ePFa74eTJEmS6oEF\nTjuucz848j/ho8dg/F+STiNJkiTlPAucdk6/61JF7sX/gIVTkk4jSZIk5TQLnHZOXj6c9QAUNU/d\nD7f6q6QTSZIkSTnLAqed17wEBt4Pn0+F4T9KOo0kSZKUsyxwqh1d+qcupxz/Zxj/v0mnkSRJknKS\nBU61p/+PoeM34cVrUrNxkiRJkmqVBU61Jy8/9WiBwl3S98OtSDqRJEmSlFMscKpdLfZMPeR74RR4\n+T+TTiNJkiTlFAucat/eR0Pfa+CDP8JHTySdRpIkScoZFjjVjf7/DXv1hhd+CIs+STqNJEmSlBOq\nVeBCCCeEEKaGEKaFEH68he3XhBAmhxA+CiG8EULoWGXbBSGET9I/F9RmeGWw/ILU/XAFRan74das\nTDqRJEmSlPW2W+BCCPnA3cCJQDfgOyGEbpvt9iHQM8Z4EDAU+HX62FbAT4HDgcOAn4YQWtZefGW0\n4nap++EWTISX/yvpNJIkSVLWq84M3GHAtBjjjBjjauAx4PSqO8QYR8QYNyw5+C7QPv378cBrMcYl\nMcYvgNeAE2onurLCPsdCnx/C+w/DxKeSTiNJkiRltYJq7NMO+KzK6zmkZtS25mLgpW0c227zA0II\nQ4AhACUlJYwcObIasepXRUVFRubKBiG/L91bvEzTZ67g/dmVrNylbb18rmOWfRyz7OOYZRfHK/s4\nZtnHMcsu2The1Slw1RZCOA/oCRxZk+NijPcB9wH07Nkz9u/fvzZj1YqRI0eSibmyxiH7wz3f5PDZ\nv4eLX4fCxnX+kY5Z9nHMso9jll0cr+zjmGUfxyy7ZON4VecSyrlAhyqv26ff20QI4Rjg/wKnxRgr\na3KsGoDi9nDGPTB/Arz6f5NOI0mSJGWl6hS4scA+IYTOIYRGwDnA81V3CCEcDNxLqrwtrLLpFeC4\nEELL9OIlx6XfU0O03wnQ+/sw9gGY9GzSaSRJkqSss90CF2NcC1xJqnhNAZ6IMU4KIdwUQjgtvdst\nQDPgyRDC+BDC8+ljlwA/J1UCxwI3pd9TQ3X0T6H9ofD892HJjKTTSJIkSVmlWvfAxRiHA8M3e++G\nKr8fs41jHwIe2tGAyjH5hVD+ENzTF54cBBe/mnpWnCRJkqTtqtaDvKVatetecMbv4V/j4dWfJJ1G\nkiRJyhoWOCVj/5PhiP8Df78XJj+//f0lSZIkWeCUoGN+Bm17wHNXwhezkk4jSZIkZTwLnJJT0Ai+\n9XDq9ycHwdrVyeaRJEmSMpwFTslq2QnOuBvmfQCv35h0GkmSJCmjWeCUvANOhcMuhXfvho9fTDqN\nJEmSlLEscMoMx/0c9uwOz14OS2cnnUaSJEnKSBY4ZYaCotT9cDHC0Itg3ZqkE0mSJEkZxwKnzNGq\nC5z2O5gzFt74WdJpJEmSpIxjgVNmOfAMOHQwjP4dTH056TSSJElSRrHAKfMc9wvYowyevQyWzUk6\njSRJkpQxLHDKPIWN4VuPwrq13g8nSZIkVWGBU2barSuc9lv47D34281Jp5EkSZIyggVOmat0IBwy\nCN65Az55Lek0kiRJUuIscMpsJ/w/KCmFp4fAsrlJp5EkSZISZYFTZitsAt96BNZWwlMXp+6LkyRJ\nkhooC5wyX+t94NQ7YPYYGPnLpNNIkiRJibHAKTscdDb0OB/euh2mvZF0GkmSJCkRFjhljxP+B9oc\nkLof7st/JZ1GkiRJqncWOGWPRruk7odbswKeGuz9cJIkSWpwLHDKLrvvByffDp++DaP+J+k0kiRJ\nUr2ywCn7dP8OdD8P3rwFpo9IOo0kSZJUbyxwyk4n/To1G/f0EFi+IOk0kiRJUr2wwCk7NWqauh+u\ncjk8PRjWr0s6kSRJklTnLHDV8K9lK3ns49V8VemiGRmlzQFw8q0w883U5ZSSJElSjrPAVcO8pat4\nedYaHh/7WdJRtLnu34WDzoGRv0oVOUmSJCmHWeCq4ZCOLdm3ZR4Pvj2TNevWJx1HVYUAJ98GrfdJ\nPVqgYmHSiSRJkqQ6Y4GrppM6FzJ36UqGfTQv6SjaXFGz1P1wq5alFjVZb8mWJElSbrLAVdNBu+ez\nb0kz7h01gxhj0nG0uZID4cRfw4wR8PZtSaeRJEmS6oQFrpryQuDSfl35eP5yRk79POk42pIe50PZ\nt2DEL2HW20mnkSRJkmqdBa4GTuvelrbFjfnDqOlJR9GWhACn/AZadYGnBlO4elnSiSRJkqRaZYGr\ngcL8PC7u24W/z1zCB7O/SDqOtqSoeep+uBVL6Db5Vli9IulEkiRJUq2xwNXQOYd2oLhJIfeMdBYu\nY+1RBqfewa5LJ8Cjp8DyBUknkiRJkmqFBa6GmhYVcH6vjrw2ZQHTFlYkHUdb0/1cJpb+FyycAg8c\nk/pXkiRJynIWuB1wQe9ONMrP4743nYXLZItbHw6DhsO6SnjwOJj+t6QjSZIkSTvFArcDWjcr4uye\nHXjmw7ks+HJV0nG0LW0PhsFvQHEH+HM5vP9I0okkSZKkHWaB20GX9O3CuvWRh96emXQUbc+uHeCi\nl6HrAHjhB/DaDT7sW5IkSVnJAreD9tptF04+qC1/eW82y1auSTqOtqdxC/jO49DzYnjnt/DkBa5Q\nKUmSpKxjgdsJl/brQkXlWv7y3qdJR1F15BfAybfB8b+EKS+kVqisWJh0KkmSJKnaLHA7obRdMX33\nac1Db89i1Zp1ScdRdYQAva6Ab/85tTLl/Ue7QqUkSZKyhgVuJ11+ZFcWVVTy9Adzk46imjjgFLjw\nxSorVI5IOpEkSZK0XRa4ndSr626UtSvmvjens259TDqOaqJdj3+vUPmXcnj/0aQTSZIkSdtkgdtJ\nIQQuO7Irsxav4NVJ85OOo5rasEJl5yPhhavgtZ+6QqUkSZIylgWuFpxQugeddtuFe0ZNJ0Zn4bJO\n4xZw7hPQ8yJ45w4YeiGsWZl0KkmSJOlrLHC1ID8vcEm/LvxjzjLGzFicdBztiPwCOPl2OO4XMPl5\neMQVKiVJkpR5LHC1ZGCP9rRuVsQ9o2YkHUU7KgTofWVqhcoFk+CBo2Hhx0mnkiRJkjaywNWSxoX5\nDOrTiTf/+TmT5i1LOo52xgGnwKDhsLYSHjzWFSolSZKUMSxwtei8IzrStFE+9zoLl/02rlDZ3hUq\nJUmSlDEscLWouEkh5x6+F8M+msdnS1YkHUc7a9cOcNErrlApSZKkjGGBq2UXf7ML+XmB+99yFi4n\nbFih8pBBrlApSZKkxFngatkexY05o3s7nhj3GYsrKpOOo9qQXwCn/AaOu9kVKiVJkpQoC1wduPTI\nLqxas55Hx3yadBTVlhCg9/fh239yhUpJkiQlxgJXB/Zu05xju5XwxzGzWLF6bdJxVJsOOBUGvZhe\nofI4V6iUJElSvbLA1ZHLjuzK0hVreOzvnyUdRbWt3SEw+HUobpdaofKDPyadSJIkSQ2EBa6OHNKx\nJYd1asWDb89kzTpXLsw5u+4FF70MnfvB89+H1290hUpJkiTVOQtcHbqsfxfmLl3JC/+Yl3QU1YXG\nxf9eofLt38DQQa5QKUmSpDplgatD/fdtw74lzbh31AxijEnHUV3IL6yyQuVz8OipUPF50qkkSZKU\noyxwdSgvL3Bpv65MXbCcEVNddj5nVV2hcv5EeOAoV6iUJElSnbDA1bHTurelbXFj7hnlg71z3oYV\nKtesSq1QOWNk0okkSZKUYyxwdawwP4+L+3bh7zOX8MHsL5KOo7rW7hC45I3UCpV/HugKlZIkSapV\nFrh6cM6hHShuUsg9I6cnHUX1wRUqJUmSVEcscPWgaVEBF/TqyGtTFjBtYUXScVQfXKFSkiRJdcAC\nV08u6N2JRvl53Pems3ANxoYVKo/9uStUSpIkqVZY4OrJbs2KOLtnB575cC7zl61KOo7qSwjQ5yo4\n+4/pFSqPhs+nJp1KkiRJWcoCV48u6duFdesjD70zM+koqm/dTkuvULkSHjjWFSolSZK0Qyxw9Wiv\n3Xbh5IPa8tf3ZrNs5Zqk46i+bVihskXb9AqVf0o6kSRJkrKMBa6eXdqvCxWVa/nLe58mHUVJ2HUv\nuPgV6NQXnr8SXv+ZK1RKkiSp2ixw9ay0XTF992nNQ2/PYtWadUnHURIaF8N3n4RDLoS3b4enLnKF\nSkmSJFWLBS4Blx/ZlUUVlTz9wdykoygp+YVwyh2pFSonPesKlZIkSaoWC1wCenXdjYPaF3Pfm9NZ\ntz4mHUdJcYVKSZIk1ZAFLgEhBC7t15VZi1fwyqT5ScdR0rqdBhdWXaFyVNKJJEmSlKEscAk5oXQP\nOu22C/eMmk6MzsI1eO2rrlB5litUSpIkaYsscAnJzwtc0q8LH81Zxpjpi5OOo0zgCpWSJEnaDgtc\nggb2aE/rZkXc8+aMpKMoU2xYobLHBa5QKUmSpK+xwCWocWE+g/p04s1/fs6kecuSjqNMkV8Ip/4W\njr0JJj0Dj54GXy1KOpUkSZIygAUuYecd0ZFmRQXcO8pZOFURAvT5QXqFyo/g/qNcoVKSJEkWuKQV\nNynk3MP3YthH8/hsyYqk4yjTdDs9vULlCnjQFSolSZIaOgtcBrioT2fy8wL3v+UsnLagfU8Y/AY0\n3zO1QuWHf046kSRJkhJSrQIXQjghhDA1hDAthPDjLWzvF0L4IISwNoRQvtm2dSGE8emf52sreC7Z\no7gxZ3RvxxPjPmNxRWXScZSJWnaEi16BTt+E566AB45JFbnVXyWdTJIkSfVouwUuhJAP3A2cCHQD\nvhNC6LbZbrOBC4G/buEUK2OM3dM/p+1k3px16ZFdWLVmPY+OnpV0FGWqJrvCd4fCCb+CVV+mitxt\n+8OL18L8CUmnkyRJUj2ozgzcYcC0GOOMGONq4DHg9Ko7xBhnxRg/Anxo1Q7au01zju1WwqNjPuWr\nyrVJx1Gmyi+EIy6HK96DQS/DfielHvp9zzdTC5188EeorEg6pSRJkupIiDFue4fUJZEnxBgHp19/\nDzg8xnjlFvZ9BBgWYxxa5b21wHhgLfCrGOOzWzhuCDAEoKSk5JDHHntsh79QXamoqKBZs2Z1+hnT\nvljHze+t4tz9G3Fcp8I6/ayGoD7GLBMUrFlOyYKRtJ33Ck1XfMba/CYsKDmSf+15PBXNuyQdr0Ya\nypjlEscsuzhe2ccxyz6OWXbJ1PEaMGDA+zHGnlvaVlAPn98xxjg3hNAF+FsIYUKMcXrVHWKM9wH3\nAfTs2TP279+/HmLVzMiRI6nrXP2BVxeMYdT8lfzse/0ozHeNmZ1RH2OWOU6FeCt89ncK3n+EdpOe\npt28l6HtwXDIhVA6EIqaJx1yuxrWmOUGxyy7OF7ZxzHLPo5ZdsnG8apOQ5gLdKjyun36vWqJMc5N\n/zsDGAkcXIN8Dc5l/bswd+lKXvjHvKSjKNuEAHsdDmf+Aa79GE68Bdauhhd+kLpX7oUfwLwPk04p\nSZKknVCdAjcW2CeE0DmE0Ag4B6jWapIhhJYhhKL0762BPsDkHQ3bEAzYrw37lTTn3lEz2N7lrdJW\nNWkJhw+By9+Bi1+HbmfAPx6H+/rDvf1g3EOphVAkSZKUVbZb4GKMa4ErgVeAKcATMcZJIYSbQgin\nAYQQDg0hzAG+BdwbQpiUPvwAYFwI4R/ACFL3wFngtiGEwJB+XZi6YDkjpi5MOo6yXQjQ4VA44274\nj6lw0q2wfh0Muzo1K/f892Hu++B/FkiSJGWFat0DF2McDgzf7L0bqvw+ltSllZsfNxoo28mMDc5p\n3dty26tTuWfkDI7avyTpOMoVjYvhsEvg0MEw9wN4/2GYMDS1cmVJGRxyARx0dmo/SZIkZSRXychA\nhfl5XNy3C3+ftYT3P/0i6TjKNSFA+0Pg9Lvg2qlwym9S7w3/j9Ss3LNXwGdjnZWTJEnKQBa4DHXO\noR0oblLIPaOmb39naUc1bgE9L4LL3oIhI1MzcJOfhQePgT/0gffug5VLk04pSZKkNAtchmpaVMAF\nvTry2uQFTFvog5lVD9oeDKf+NrWC5am/hYJG8NJ1qVm5Zy6H2e85KydJkpQwC1wGu6B3JxoX5nHf\nm87CqR4VNU89O27ISBgyCrp/B6a8AA8dB7/vBe/eAyu9tFeSJCkJFrgMtluzIs7u2YFnPpzL/GWr\nko6jhqht99Q9ctd+DKf9DhrtAi//Z2pW7ulL4dMxzspJkiTVIwtchrukbxfWrY889M7MpKOoIStq\nBj3Oh0v+Bpe+BQefB1OHw8MnwN2Hw5jfw4olSaeUJEnKeRa4DNeh1S6cfFBb/vrebJatXJN0HAn2\nPAhOvi01K3f63amFUF75r9Ss3FODYdbbzspJkiTVEQtcFri0XxcqKtfy53c/TTqK9G+NmqZm4ga/\nDpe9k3qO3D9fhUdOhrsOhdG/g68WJ51SkiQpp1jgskBpu2L67tOah9+Zxao165KOI33dHqVw0i2p\nWbkz/gC7tIJXr4fb94ehF8HMN52VkyRJqgUWuCxx+ZFdWVRRyVMfzEk6irR1jXaB7ufCxa/C/3kX\nel4M016HR0+F3x0C7/wWKj5POqUkSVLWssBliV5dd+Og9sXc/+YM1q13JkNZoM0BcOKv4NqpcOZ9\n0KwEXrsBbj8AnrwQZoyE9euTTilJkpRVLHBZIoTAZUd2ZdbiFbwyaX7ScaTqK2wC3/g2XPQSXPF3\nOGxIqrz98XT4XQ94+zdQsTDplJIkSVnBApdFjj9wDzrttgv3jJpO9H4iZaPd94MTfgnXfAxnPQAt\n2sHrN6Zm5Z44n2bLZySdUJIkKaNZ4LJIfl7gkn5d+GjOMsZMd3U/ZbHCxnDQt2DQi3DFWDj8Mpj5\nFgd/+J8w8amk00mSJGUsC1yWGdijPa2bFfGHUdOTjiLVjt33heN/AVeOZXnzvVOrVo78H1etlCRJ\n2gILXJZpXJjPoD6deOuTRUycuyzpOFLtadqaf3zjJvjGd2DkL+HpS2DNqqRTSZIkZRQLXBY674iO\nNCsq4N43vV9IuSXmFaaeI3f0T2HCk6nHD7jAiSRJ0kYWuCxU3KSQcw/fixc/msdnS1YkHUeqXSFA\n32vg7D/B/Alw/1GwYFLSqSRJkjKCBS5LXdSnM/l5gfvfchZOOarbaalHD6xfCw8eB/98JelEkiRJ\nibPAZak9ihtz5sHteGLcZyyuqEw6jlQ32h4Ml/wNdusK/3sOjPm9i5tIkqQGzQKXxYb068KqNet5\ndPSspKNIdadFWxj0Eux3ErzyXzDsali3JulUkiRJibDAZbG92zTn2G4lPDrmU76qXJt0HKnuNGqa\nuifum9fA+w/DnwfCyi+STiVJklTvLHBZ7rIju7Js5RoeG/tZ0lGkupWXB8f8NLVK5aej4YFjYLHP\nQ5QkSQ2LBS7LHdKxJYd1asWDb81gzbr1SceR6l73c+GC52HFktQKlTPfSjqRJElSvbHA5YDL+ndh\n3rJVPD9+XtJRpPrRsTdc8gY0K4E/nQEf/DHpRJIkSfXCApcDBuzXhv1KmnPvm9OJrtCnhqJVFxj8\nGnTuB89/H169HtavSzqVJElSnbLA5YAQApce2YV/LqhgxNSFSceR6k/jYjj3STj0Ehj9O3j8PKis\nSDqVJElSnbHA5YhTv9GWtsWNuWekD/ZWA5NfACffCifeAv98GR46AZbNSTqVJElSnbDA5YjC/Dwu\n7tuFv89awvufury6GqDDh6Rm45Z+mlrcZM77SSeSJEmqdRa4HHLOoR0oblLIPaNcWl0N1D7HwMWv\nQUFjeOQkmPh00okkSZJqlQUuhzQtKuCCXh15bfICpi1cnnQcKRlt9odL/gZ7doehg2DULeDiPpIk\nKUdY4HLMBb070bgwj3tHeS+cGrCmrVPPijvoHBhxMzw9BNasSjqVJEnSTrPA5ZjdmhVxds8OPDt+\nLv9atjLpOFJyCorgzHvgqJ/AhCfg0VOh4vOkU0mSJO0UC1wOuqRvF9ZHePidWUlHkZIVAvT7D/jW\nozB/QmpxkwWTk04lSZK0wyxwOahDq104uWxP/vrebJatXJN0HCl5B54Bg4bDutXw4HHwz1eTTiRJ\nkrRDLHA5aki/LlRUruXP736adBQpM7TrkVrcpFVn+N9vw7t/cHETSZKUdSxwOaq0XTF992nNw+/M\nYtWadUnHkTJDcTu46GXY7yR4+cfw4jWwzllqSZKUPSxwOezyI7uyqKKSpz6Yk3QUKXM0agpn/wm+\neTWMewj+PBBWfpF0KkmSpGqxwOWwXl1346D2xdz/5gzWrfdSMWmjvDw45kY4/ffw6Wh44FhYPD3p\nVJIkSdtlgcthIQQuO7Irsxav4OWJ85OOI2Weg78L5z8HKxbDA0fDrLeTTiRJkrRNFrgcd/yBe9C5\ndVPuGTWd6IIN0td16gOXvAFNd4c/ngEf/CnpRJIkSVtlgctx+XmBS/p2YcLcZYyZvjjpOFJmatUF\nLn4NOn0Tnr8SXv0JrHfxH0mSlHkscA3AWT3a0bpZEX8Y5T0+0lY12RW+OxQOHQyj74THvweVFUmn\nkiRJ2oQFrgFoXJjPoD6deOuTRUycuyzpOFLmyi+Ak2+DE38N/3wJHj4Bls1NOpUkSdJGFrgG4rwj\nOtKsqIB735yRdBQp8x1+KZz7BCyZBfcfBXPfTzqRJEkSYIFrMIqbFHLu4Xvx4kfzmL14RdJxpMy3\nz7Fw8atQ0AgePgkmPZN0IkmSJAtcQ3JRn87k5wXuf8tZOKlaSrrB4L/Bnt+AJy+EUbeAq7lKkqQE\nWeAakD2KG3Pmwe14YtxnLKqoTDqOlB2a7Q7nPw8HfRtG3AxPD4E1q5JOJUmSGigLXAMzpF9XVq9b\nzx9Hz0o6ipQ9ChvDmffCUf+/vfuOjqu61z7+3TMaaVRHvXdZlnvBxjbGwTbF9JZQA4SEEJIQchNC\neFNvckNyc1PoEEJoCcG00HuxsQ3YgLFxly33Jrk3yU39vH+c0cxIlnGTdDTS81lrL43OOSP/xmdJ\no0e7/RoW/wf+fRHs2+50VSIiItILKcD1Mn3S4zirfwZPfrqe/XWNTpcjEj6MgdNuh8ufhM2L4LHT\nYetSp6sSERGRXkYBrhf67vgSqg828NycjU6XIhJ+Bl4C33obGuvh8UmwcorTFYmIiEgvogDXC40o\nSGJUYTKPf7yGhqZmp8sRCT85J8F3pkFyITxzBXz2sBY3ERERkS6hANdLfW9CMZuqa3l9wSanSxEJ\nT74c+Na7UHYevPszeOsn0NTgdFUiIiLSwynA9VITy9Ipy4jnHx+tprlZPQcixyUqDq54Ck79Mcx9\nAp6+DA7ucboqERER6cEU4HopYwzfHV/Miq37eH/pVqfLEQlfLhec9Tu4+CFYNwsePwt2rna6KhER\nEemhFOB6sQuHZlOUGssPn53H/R+s1Hw4kRMx/Br4xmuwfzs8dgasm+l0RSIiItIDKcD1Yh63i5e+\nP5ZzBmVx95QVXPK3WSzbXON0WSLhq/BUuPEDiEmFf18C8yc7XZGIiIj0MApwvVxybCQPXD2ch689\nia01tVz04Ez1xomciJQSuHGKHeZe+wFM+Q006/tJREREOoYCnABwzqAs3r91fKveuKWb1Bsnclyi\nk+CaF2HkDTDrPntxk9XToLnJ6cpEREQkzCnASUB7vXH3TVVvnMhxcXvg/Lvh3L9C5Vx46lK4ewC8\n9yvYstjp6kRERCRMKcDJIc4ZlMWUW8dz3uAs7pm6gosfVG+cyHExBkbfBD9dAZf/y94AfPbD8PA4\neGgszLwXqqucrlJERETCiAKctCspNpL7rx7Ow9eOYNte9caJnBCPFwZeClc/C7etgPPuhMgYmPpb\nuGcgPHkhzH8aavWHEhEREflyCnDypc4ZlMmUW8dz/hD1xol0iNgUGPUduHEq/HAejP8Z7NkIr90M\nd/aFF2+AFe9DU4PTlYqIiEg3pAAnR5QUG8l9Vw3nH9eNYNveOi56cCb3Tl2h3jiRE5VSAhN/Af81\nH749BYZ93V7s5JnL4a5+8M7PoGoeWJbTlYqIiEg3EeF0ARI+zh6YyajCZP7njXLunbqS98u38tfL\nhzAw2+d0aSLhzRjIG2W3c/4Eq6bAwudg7hP2nLmUUhh6JQy+ApIKnK5WREREHKQeODkmbXvjLn5w\nFvdOXUF9o3rjRDpERCT0Ox+ufMpe/OTC+yA2Dab9Ae4bAk+cC3P/CQd3O12piIiIOEABTo7L2QMz\nmXLraZw/JIt7p67k4r/NonxTtdNlifQs0Ukw4ptwwzvwo0Vw+q9h/3Z488f2fLnnr4Nlb0JjvdOV\nioiISBdRgJPjFtobt93fG3fPFPXGiXSKpAI47Xa4ZQ58ZzqM/DZs+BSevwbu6gtv/gQ2zNZ8ORER\nkR5Oc+DkhLXMjfvdG+Xc98FK3l+6lTs1N06kcxhj7yeXcxJM+j2sng6LnocFz8DcxyGpEIZcabeU\nEqerFRERkQ6mHjjpEEmxkdx71XAeUW+cSNdxe6DvJLjscXu+3CV/h8QC+PAv8MBJ8OgZ8PmjsH+n\n05WKiIhIB1GAkw41aWAmU39yGhcMyeK+DzQ3TqTLeBPsbQiufx1uLYez7oCGg/D2T+0hls9cBeWv\nQEOt05WKiIjICVCAkw6XGBPsjduxT71xIl3OlwOn/ghu/gS+NwvGfB82L4AXvgl3lsJrt8C6mdCs\n70kREZFwozlw0mkmDcxkVFEyv3tjqebGiTglcxBk/gHO/B2s/cieL7fkZZj/FPjyYPDl9ny59H5O\nVyoiIiJHQT1w0qkSYyK558phPPqNkYHeuLvVGyfS9VxuKJkIlz4Mt6+Erz4GaWUw6154aDT84zT4\n9CHYu9XpSkVERORLKMBJlzhrQAZTbj2NC4dmc/8HK7nowZksqdLcOBFHRMbCkMvh2pfgJxVw9v8B\nBt77BdzdHyZ/DRa9APX7na5URERE2lCAky4T2hu3c389l/xNvXEijovPgFNuhu9+CDfPtufObV8O\nL99obxb+yvdg9TRoM8WUdQAAIABJREFUbnK6UhEREUFz4MQBZw3I4OTCJH73xlLu/2Al75dv4c7L\nhzIoR3PjRByV3g/O/C2c/t+w4RN7vlz5a7DwWYjLhMGXwdCrIHOw05WKiIj0WkfVA2eMOccYs9wY\ns8oY8/N2zp9mjJlnjGk0xlzW5tz1xpiV/nZ9RxUu4a3d3rj3l6s3TqQ7cLmgcBxc9IC9v9zlT9ob\nh89+GB4eBw+NhZn3QnWV05WKiIj0OkfsgTPGuIG/AWcBlcAcY8zrlmUtDblsA/BN4KdtnpsM/BYY\nCVjAF/7n7u6Y8iXctfTG3fHGUu6ftsq/UqV640S6DY8XBl5it/07ofxlu2du6m9h6v8wNHEgxH8H\n+l8EMclOVysiItLjHU0P3ChglWVZayzLqgeeAy4OvcCyrHWWZS0C2nafnA1MsSxrlz+0TQHO6YC6\npQdJjInk7iuH8Zi/N+5i9caJdE+xKTDqO3DjVPjhPBj/M6LqdsEbP7Lnyz1zFSx+UYufiIiIdCJj\nWdaXX2APiTzHsqwb/Z9fB4y2LOuWdq79F/CmZVkv+j//KeC1LOsP/s//GzhoWdadbZ53E3ATQEZG\nxojnnnvuRF9Xh9u3bx9xcXFOl9Hj7au3eKaink82NZIbZ7hxcBSFPvfxfS3ds7CjexZ+9u3dSybb\nyNj6EenbPiaqfidNrih2pI5mW/pX2JU8HMvlcbpM8dP3WPjRPQs/umfhpbver4kTJ35hWdbI9s51\ni0VMLMt6BHgEYOTIkdaECROcLagdM2bMoDvW1RNdMAmmLt3KL19ZzO9n1/GDCSXccnopkRHHtmiq\n7ln40T0LPzNmzGDkhAuBb0NzM2z4BPfiF8lY+ioZ2z4CbyIMuNjeMLxgrL0fnThG32PhR/cs/Oie\nhZdwvF9HE+CqgLyQz3P9x45GFTChzXNnHOVzpRc7c0AGJxcm87s3yzU3TiRctCx+UjgOzv0LrJlu\nD6lc/CLMexLis2DgV+3VLLOHgzFOVywiIhJ2jqZLYw5QaowpMsZEAlcBrx/l138PmGSMSTLGJAGT\n/MdEjsgX4+HuK+y5cbv8c+Pu0tw4kfAQEQl9z4avPQq3r4TLnrBD2+ePwKMT4YGTYPofYfsKpysV\nEREJK0cMcJZlNQK3YAevZcB/LMsqN8bcYYy5CMAYc7IxphK4HPiHMabc/9xdwO+xQ+Ac4A7/MZGj\nduaADKbcOp6Lh2XzwLRVXPTgTBZXVjtdlogcrchYGPQ1uPpZO8xdeD/4cuHDv8DfToaHvwKz7oPq\nSqcrFRER6faOag6cZVlvA2+3OfabkMdzsIdHtvfcJ4AnTqBGkUBv3PmDs/jFy4u55KFZ3DyhhFtO\n70NUhObUiISN6CQYcb3dajZD+Suw5EWY8hu75Y+FwV+DAZfaq16KiIhIK8e2KoSIw87o36Y37oFZ\n6o0TCVcJWXDKzfCdafa2BBN/DQd2wlu3wV19YfJlsPB5qNvrdKUiIiLdhgKchJ2W3rjHrx/JnoP1\nXPLQLO58bzl1jU1OlyYixyulBMbfDj+YDd+bCafcAtsr4JWb4K+l8MI3Ydmb0FjndKUiIiKO6hbb\nCIgcjzP6Z/B+QTJ3vLmUB6evYop/pcrBuVqpUiRsGQOZg+12xm+h8nNY/II91LL8FYjywYALYdBl\nUHSatiUQEZFeRz1wEtZ8MR7uumIoT3xTvXEiPY7LBflj4Py74LblcM1L0O88KH8VnroE7u4P7/wM\nKueCZTldrYiISJdQD5z0CKf3y+D9Hyfz+7eCvXFXFTe12oRQRMKY2wOlZ9rtgntgxXv24idz/wmz\nH4akQrtXbvBlkN7f6WpFREQ6jXrgpMfwxXi48/Jgb9wdn9Zy238WsmnPQadLE5GO5ImGgZfAlZPt\nbQkufgiSimDm3fDQGHhoLHx8N+xe73SlIiIiHU4BTnqc0/tl8P6t4zmnyMMbizYx8c4Z/PndCmpq\nG5wuTUQ6mtcHw6+Bb7xqD7M896/2vnMf/A7uGwKPT4LZj8C+bU5XKiIi0iEU4KRH8kV7uLIskmm3\njee8wVn8fcZqxv9lOv+ctZb6xmanyxORzhCXDqNvghunwI8Wwhm/gbp98M7tcFcZPHUpzH8aarX1\niIiIhC8FOOnRcpNiuOfKYbz5w3H0z0rgd28s5ax7PuStRZuxtOiBSM+VVAhfuQ1u/gS+/ymMuxV2\nroLXbra3JXj+WnsxlAYNsRYRkfCiRUykVxiU4+PpG0czY8V2/vR2BT94Zh7D8hL51fn9Obkw2eny\nRKQzZQyAjN/A6f9tr1i55EVY8jIsewMi46H/BfbiJ0UTwK23RRER6d70TiW9hjGGiWXpnFaaxktf\nVHLXlOVc/vCnnDUgg5+f24+StDinSxSRzmQM5J1st0n/C+s+hsUv2kFu4bMQkwoDL7XDXO4oexsD\nERGRbkYBTnodt8twxcl5XDg0m8dnruHhD9cw6Z6PuHpUHj86oy9p8VFOlyginc0dASUT7Xb+XbBq\nqr1h+PynYM6j4ImF5GJIKYbkEkgpCX6MTbPDoIiIiAMU4KTXio50c8vppVw1Kp/7P1jJM7M38Mq8\nKm46rYTvnFZETKS+PUR6BY/XHkbZ/wKo2wsVb8Om+bBrNWxZAhVvQXNj8PrI+PaDXXIJxCQr3ImI\nSKfSb6jS66XGRXHHxYP45thC/vLucu6ZuoKnZ6/nJ2f15bIRuUS4NYxKpNeIioehV9qtRVMD7NkA\nu9bAztV2sNu5GjbNg6WvghWysq3X136wSymG6KSufz0iItLjKMCJ+BWnxfHwdSOYu24Xf3x7GT9/\neTFPzFrLz8/tx8SydIz+qi7SO7k9dhBLKYHSs1qfa6yHPetbB7tdq2HDbHt+HSGr3UYntwl2xcHP\nvQld+pJERCR8KcCJtDGyMJmXvj+W98q38Od3l3PDv+YypjiZX57XnyG5iU6XJyLdSUQkpJbara2G\nWti9rnWw27naXjxl0XOtr41Naz/YJRdDlBZYEhGRIAU4kXYYYzhnUBZn9M/g2c83cO/UlVz04Cwu\nGprN7WeXkZcc43SJItLdebyQ3s9ubdUfgN1r2/TcrYFVH8C+p1tfG5d5aLBr+dwT3TWvRUREug0F\nOJEv4XG7+MYphVw6PId/fLiGx2au4d0lW7h+bAE/mNiHxJhIp0sUkXAUGQMZA+3WVt0+O8yFBrud\nq2HFu7B/e+trE3IODXYpfeyNzCO0oq6ISE+kACdyFOK9Hn56dhnXjMnnnikreGzmWp6fs5FbTu/D\nN04pxOtxO12iiPQUUXGQNcRubdVWhyymErKoytLX4eCu4HXGBb7c1r11ySV4D+4By9JKmSIiYUwB\nTuQYZPmi+ctlQ7lhXBH/93YFf3y7gic/Wc/tZ5dx0dBsXC79UiQincjrg+zhdmvr4G7YuebQOXeL\nX7CDHzAGoOJ/g3vgFY23tz4QEZGwoQAnchz6ZSbw5A2jmLlyB//3zjJ+/PwCHpu5hl+e25+xfVKd\nLk9EeqPoJMgdYbdQlgUHdsGu1az46EX6uqug/BWY9yRg7DBYMhFKTofcUfbCLCIi0m0pwImcgHGl\nqbxRMo7XFlZx53sr+Ppjs5lYlsYvzutP34x4p8sTEbGHS8amQGwKm3IO0HfCBGhqhKovYM10WD0N\nZt4LH98FnlgoHBcMdKl9NdxSRKSbUYATOUEul+HS4bmcOyiLJz9Zx4PTV3HOvR9x+Yg8fjKpLxkJ\nXqdLFBFpzR0B+aPtNuHn9hDLtR8HA93K9+zr4rPtIFcyEYonQKxGGIiIOE0BTqSDeD1uvju+hCtG\n5vHg9FX8+9N1vLawiu98pZjvji8hLkrfbiLSTXl90P8CuwHsXh8McxVvwoLJ9vHMIcFAlzfG3ipB\nRES6lH6jFOlgSbGR/PcFA7j+lEL++v5yHpi2imdmb+DHZ5Zy1ah8PG6X0yWKiHy5pAIY8U27NTfB\npgWwZhqsng6fPgiz7oWIaCgYGwx06QM03FJEpAsowIl0kvyUGB64ejjfHlfEH99exn+/Vs4/Z63j\n/53Tj7MHZmD0i46IhAOXO7g4ymm32/vUrZ9l986tng7v/8q+Li4Div2rWxZPhPgMZ+sWEemhFOBE\nOtmwvESev2kMHyzbxp/ereB7k79gZEESvzivPyMKkpwuT0Tk2ETFQd+z7QZQXQlrZtiBbtUUWPSc\nfTx9YHC7gvyx9ublIiJywhTgRLqAMYYzB2QwoSyN/8yt5J6pK/ja3z/hvMGZ3H52P4pSY50uUUTk\n+PhyYfi1dmtuhi2LgvPnPn/EHnLpjoL8McHhlhmDwaXh5CIix0MBTqQLRbhdfH10PhcPy+bRj9fw\nyEdreL98K9eOKeCHp/chJS7K6RJFRI6fywXZw+w27laoPwDrPwkGuqm/tVtMqr2qZctwS1+O05WL\niIQNBTgRB8RGRfDjM/vy9dH53Dt1JU99tp6XvqjkexNKuOHUIqIj3U6XKCJy4iJjoPRMuwHs3RIc\nbrl6Oix50T6eWhbsnSs41R6mKSIi7VKAE3FQeryXP146mBtOLeRP7yznr+8t56lP13PbpL589aRc\n3C4tdCIiPUh8Jgy9ym6WBVvLg71zX/wTZv8dXB7IGx2cP5c1zF5IRUREAAU4kW6hT3o8j10/ktlr\ndvLHdyq4/cVFPD5zLb84rz/j+6Y5XZ6ISMczBjIH2W3sD6GhFjZ8Ggx0035vt+gkKBof7KFLzHe6\nchERRynAiXQjo4tTePXmsby1eDN/eXc51z/xOV8pTeXn5/ZjYLbP6fJERDqPxxvsdTvrDti33R5u\nuWa6Pdxy6av2dSl9/NsVnA6F48Cb4GjZIiJdTQFOpJsxxnDBkGzOGpDB059t4P5pK7nggZlcOiyH\n284uIycx2ukSRUQ6X1waDLncbpYF25cHe+cWPA1zHgXjhtyT/UMth0JqX0gq1JBLEenRFOBEuqmo\nCDc3jCviayNyeWjGKv45ax1vLt7Mt04t5NvjikiP9zpdoohI1zAG0vvZbcz3obEONn4eDHQz/gRY\n9rXuKLuXLq2vHehS+0JamX3Moz+AiUj4U4AT6eZ80R5+cW5/vnFKIXe9v5xHPlrD4x+v5exBmVw7\nuoAxxckYo8VORKQXiYiCoq/Y7YzfwME9sGOF3Uu3YzlsXwGb5kP5qwSCHcaeP5dWFgx1qWV20ItO\ncvLViIgcEwU4kTCRkxjN3VcM45aJfXhm9gZe+KKStxZtpk96HNeOzuerI3JJ8HqcLlNEpOtFJ0Le\nKLuFaqiFnauCoW6Hv635EJrqgtfFpgXDXGoZpJbaAS8hx+79ExHpRhTgRMJMcVocv75gAD89u4w3\nFm5i8uwN/M8bS/nzu8u5ZHg214wuYFCOFjwREcHjDa50Gaq5CfasDwl1/oC35CWorQ5eFxlnh7nQ\nUJdaBslF4NYfzETEGQpwImHK63Fz+cg8Lh+Zx+LKaiZ/tp5X5lfx7OcbGZaXyHVjCjh/SBZejybz\ni4i04nJDcrHdys4JHrcs2L+99VDMHcth3cew6LmQ50fYz207FDOlVJuQi0inU4AT6QEG5/r482VD\n+OV5/XlpXiWTZ6/nthcW8vu3lnL5iFyuGV1AYWqs02WKiHRvxkBcut2KvtL6XN1e/zw7f6jbsdIO\nesvfAaspeF1CbnAoZmAhlTKITdVwTBHpEApwIj2IL8bDDeOK+NaphXy6eieTZ6/nn7PW8ejHa/lK\naSrXjSng9H7pRLhdTpcqIhJeouIhZ4TdQjXWw641rYdi7lgO856EhgPB66KTDg11aX3Blw8u/UwW\nkaOnACfSAxljGNsnlbF9UtlaU8tzn2/k2c83cNNTX5Dl83L1qHyuOjmP9ARtRSAickIiIoNbHIRq\nboaaqtahbsdKqHgbDvw75PnRkNqndahLLYOUEnu1TRGRNhTgRHq4jAQvPzqzlB9MLOGDim1M/mw9\nd09Zwf0frOTsgZlcMyafU4pTtBWBiEhHcrkgMc9ufc5sfe7AruA8u5ahmJVzYMnLBLY9MC5IKmSg\nKx1ccyBrOGQPs4diikivpgAn0ktEuF2cPTCTswdmsnbHfp7+bL29FcFieyuCa0bn89WTcvFFa2U1\nEZFOFZMMBafYLVT9Af+2B8E97WLXzoFpfwhek5ADWcPsMNfyMS69a+sXEUcpwIn0QkWpsYdsRfC7\nN5byl3eXc/GwbK4do60IRES6XGQMZA2xm9/nM2YwYcxw2LwINi+ATQvsj8vfCj4vPrt1oMsaBvEZ\nDrwAEekKCnAivVh7WxG8uqCK5+bYWxFcO6aAC7QVgYiIs7w+e1XM0JUxa2tgy+I2oe4dAkMw4zIP\nDXUJWY6ULyIdSwFORICQrQjO789LX9hbEfz0hYX8QVsRiIh0P94EKDzVbi3q9tqhriXQbVoAK94j\nGOoyDh1+GZ+l7Q1EwowCnIi04osO2YpgzU4mf9Z6K4JrxxRwhrYiEBHpfqLioWCs3VrU7YOtS1qH\nulVTwGq2z8emQ9bQ1qEuIUehTqQbU4ATkXYZYxhbksrYklS21dTy3JyNPDN7A9/1b0Vw1cn5XD1K\nWxGIiHRrUXGQP8ZuLer3w5YlIcMvF8LqacENyWNSDx1+6ctVqBPpJhTgROSI0hO8/NcZpdw8IbgV\nwT1TV/DAtJVMGpjBtWMKtBWBiEi4iIyF/NF2a1F/ALaWt55Tt/qekFCXYvfUhYa6xHyFOhEHKMCJ\nyFFruxXBM7PX85+5lby9eAslabFcM7qAr43QVgQiImEnMgbyTrZbi4aDdqjbNN8OdJsXwif3Q3Oj\nfT46+dDhl4kFCnUinUwBTkSOS1FqLL86fwC3TSrjzUWbmfzZeu54cyl/ea+Ci4fmcO2YAgbnaisC\nEZGw5YmG3JF2a9FQC9vKW8+p++RBaG6wz3sTDw11SUUKdSIdSAFORE6I1+PmshG5XDYilyVV9lYE\nry3YxPNzNzI0L5FrR+dz4dBsbUUgItITeLyQM8JuLRrr2gy/XAifPhQS6nyHDr9MLlaoEzlOCnAi\n0mEG5fj409eG8Ivz+vPyvEomf7ae219cxB/eWmZvRTCmgCJtRSAi0rNEREHOSXZr0VgP25a2nlM3\n+2FoqrfPR8bZc+gS88GXB4l5/sf59uPYNAU8kcNQgBORDueL9vCtU4v45thCPluzi8mfredfn6zj\nsZn2VgTXjC7gzP7aikBEpMeKiLR727KHQUtnXWM9bF9mB7ptS2HPRtizATZ8CrXVbZ7vDQY7X16b\nsJcP8Zng0sgO6Z0U4ESk0xhjOKUkhVNKUgJbETz7+Qa+N/kLMhO8XD0qn6tG5ZGhrQhERHq+iEj/\nUMqhh56rrbYDXfVGf7BbH3y8eREc2NH6eleEvV9du714efa5iMiueV0iXUwBTkS6ROhWBNMqtvGU\nfyuC+6etZNKADK4bU4BlWU6XKSIiTvD6INMHmYPaP19/AKor7R676g3B3rvqjbB6OuzdDIS+hxiI\nz/IHvNBevLzgME1PdFe8MpEOpwAnIl0qwu1i0sBMJg3MZN2O/Tzz+Qb+M3cj7yzZQrwHRq77nOH5\nSQzLS2RoXqK2JBAREXubg7S+dmtPYz3UVIb04m0IPt44G8pfCW5/0CI2rf35dy29eN6Ezn9dIsdB\nAU5EHFOYGssvz+vPT87qyztLNvPyzHIqdx9kxorttHTGFafFMjwviWH5iQzPS6QsMx6P5s6JiEio\niEh7Zcvk4vbPNzfZvXSBYLch+HhrOSx/F5rqWj/H62sd7Nr24sUka6EVcYQCnIg4zutxc+nwXJKq\nVzFhwnhqahtYtLGaBRt3s2DjHj5csY2X5lX6r3UxKNvH8PxEhvmDXbbPi9GbqIiIHI7LDb5cuxW0\nc765GfZvD+m92xCcg7drDaz9EOr3tX6OJ7adYJdHQvV22FMMcZmahyedQgFORLqdBK+HcaWpjCtN\nBcCyLCp3H2T+xj0s2LCHBRt38+Sn63n047UApMVHMSwv0R/qEhmSm0hclH68iYjIUXK5ID7DbqEb\nl7ewLDi4u3WwCzzeAJVzoHYPACcBzP+5/bzYdEjIbt3i23weqe115NjoNxwR6faMMeQlx5CXHMNF\nQ7MBqG9sZtnmGhZs3BNoU5ZuBcBloDQ9Phjq8hMpTY/H7VIvnYiIHAdj7CGTMcn21gjtqdsLezay\naOa7DClMsYds1lRBzSbYvR7WfxIIea14fYeGuoRseyXN+Cz7cXSShmtKgAKciISlyAgXQ/0LnVzv\nP7Z7fz0LKlt66fbwbvkWnp+7EYDYSDeDc32BBVKG5yWSru0LRESko0TFQ8YAdqVsgxET2r+m/kBI\nsPN/3LvZDnk1VfZ8vH1bab2iJva+eG1DXUIOJIQ8jk3T3ni9hAKciPQYSbGRTCxLZ2JZOmAPvVy7\nY3+rXrpHP1pDY7P9xpjt8wYC3bD8RAZl+4iO1JufiIh0ksgYSCmx2+E0NcDeLYcGvZpN9rGNn9nH\nmhtaP8+4/eEu6/BhLz4LIqI69zVKp1OAE5EeyxhDcVocxWlxfPWkXABqG5oo31TNfH8v3fwNe3hr\n8WYA3C5Dv8z44AIpeYkUp8bi0tBLERHpKm6PfzuDvMNf09wMB3aG9OD5A15L2Nu2DFZOhYb9hz43\nNs0f7HLaCXs59udRcZ33+uSEKcCJSK/i9bgZUZDMiILkwLHte+v8PXT2qpevzt/E5M82AJDgjWCo\nf8jlMH+wS47VqmIiIuIglwvi0uzGYebkWRbU1bQ/VLNms70x+sbZcHDXoc+NSjh00ZXkIsgeDql9\nNVTTYQpwItLrpcVHcdaADM4akAFAU7PF6u37WLBhj73y5cY9PDh9Ff6Rl+QnxwRWvByWl8iA7ASi\nIvRmJiIi3Ygx9gIpXh+k9zv8dQ0H/eEuJOCFDt/cVgH7toDVbF/viYWsoZBzkh3osofb++9pkZUu\nowAnItKG22XomxFP34x4rjjZHsKyv66RxVXVdk/dhj18tmYnry3YBECk20X/7ASGh2xlkJ8co73p\nRESk+/NEf/km6ABNjbBrNVTNg03zYdM8mPMYNNba572+YJjL9gc7X65CXSdRgBMROQqxURGMKU5h\nTHFK4Njm6oOBFS/nb9zD83M28q9P1gGQHBsZ6KEbkutjQFYCafFRCnUiIhJ+3BGQVma3YVfbx5oa\n7Ll2LYFu03z45AFobrTPx6a1DnQ5J0FcunOvoQdRgBMROU5ZvmiyBkdz7uAsABqbmlm+dW+gl27+\nxj1Mq9gWuD45NpJ+mfH0y0ygX1Y8/TMTKM2Iw+vR8EsREQkzbg9kDbHbCP+GPg219lYILYGuah6s\nmhocfpmQE+ypyzkJsobZe+vJMVGAExHpIBFuFwOzfQzM9nHN6AIAamobWFJVzfIte6nYvJeKLTU8\n8/l6ahvsNzOXgaLUWDvUZcbTL8v+mJsUrd46EREJLx4v5I6wW4u6fbBlUTDQbZoPFW8GzycVBQNd\n9kl2IIyK7/raw4gCnIhIJ0rwehhbksrYktTAsaZmiw27DlCxuYZlW/ZSsbmGxVXVge0MAOKjIijL\njKdflt1j1z/LnpMX7/U48TJERESOT1QcFIy1W4uDu2HzwmCgq5wD5S/7Txp7qGbonLrMQfZcPQEU\n4EREupzbZShKjaUoNTYw/BJgX12j3VO3pSbQY/fagk1Mrt0QuCY3KToQ6FqGYhamxOLWXnUiIhIu\nopOgeILdWuzb7p9P559Tt+oDWPisfc4VAen9W8+pyxhoD+PshRTgRES6ibioCEYUJDGiIClwzLIs\nNlXXUrG5hoote1nm/zh9+Taa/PsaREW4KMuMpyzDHoLZ3z8UU/vViYhI2IhLg76T7Ab2PnY1m1ov\nkrL0dZj3b/u8O8rumQtdJKWX7FGnACci0o0ZY8hJjCYnMZoz+mcEjtc2NLFq2z4q/EMwK7bsZVrF\nNl74ojJwTXp8VEigs3vsStLiiIxwOfFSREREjp4x4MuxW/8L7GOWBbvXhSySMt/upZvzqH2+ZY+6\nwJy6nrlHnQKciEgY8nrcDMrxMSjH1+r49r11VGypoWLzXpb5h2L+c9ZO6pvsRVMiXIaStLhAoGtZ\nDTMjQVsciIhIN2cMJBfZbdDX7GPNzbBzZetFUuY+Dp/9zT7v9dmrXQY2Hj8p7PeoU4ATEelB0uKj\nSItP4yulaYFjDU3NrNuxP7BgSsWWvcxZuyuwETlAYownuMWBfwhm34w4YiL1NiEiIt2YyxXco27o\nVfaxpgbYXtF64/F296gbTspOD9SNtBdbCRNH9c5sjDkHuA9wA49ZlvWnNuejgH8DI4CdwJWWZa0z\nxhQCy4Dl/ks/syzrex1TuoiIHA2P20VpRjylGfFcNDQ7cLz6QAPLt9qLpizzb3Hwn7kbOVDfBNh/\nnCxMiT1k77rcpGhcWjRFRES6K7cHMgfb7XB71G2aD6umMthqhvEX2QEwTBwxwBlj3MDfgLOASmCO\nMeZ1y7KWhlz2bWC3ZVl9jDFXAX8GrvSfW21Z1rAOrltERE6QL8bDqKJkRhUFN1FtbrbYuPtAINDZ\ne9ft5d3yLVj2minERLrtLQ4yE6CmgeaKrRSlxpGXFE2EW/PrRESkGzrMHnXz33mS4SmlztV1HI6m\nB24UsMqyrDUAxpjngIuB0AB3MfA//scvAg8aTaYQEQk7LpehICWWgpRYzhmUGTh+oL6RFVv3tVoN\n850lm9lzoIFnK+YC9vy6/JQYiv1bJBSlxlGUGktxWizp8ZpjJyIi3UxUHNWJA+1hmGHEWC1/Uj3c\nBcZcBpxjWdaN/s+vA0ZblnVLyDVL/NdU+j9fDYwG4oByYAVQA/zasqyP2/k3bgJuAsjIyBjx3HPP\ndcBL61j79u0jLi58xsaK7lk40j0LL5ZlsXXPfvYSzZb9zWw9YLFlf3PgcUNz8FqvGzJiXWTGGDJj\nXfbjWENmjIsYj4JdV9H3WPjRPQs/umfhpbver4kTJ35hWdbI9s519uz0zUC+ZVk7jTEjgFeNMQMt\ny6oJvciyrEc8J3QVAAASMUlEQVSARwBGjhxpTZgwoZPLOnYzZsygO9Ylh6d7Fn50z8LPjBkzuKqd\ne9bcbLG5ppa12/ezZsc+1mzfz9oddpuz9QDNIX87TI2LtHvqUuMoSov1P44lPyWGqIiev59PV9L3\nWPjRPQs/umfhJRzv19EEuCogL+TzXP+x9q6pNMZEAD5gp2V379UBWJb1hb9nri8w90QLFxGR7svl\nCu5fN640tdW5usYmNu460CrUrdmxnw8qtrFjbl3waxjISYqmKDWOYv9QzCL/8MxsnxZSERGR3ulo\nAtwcoNQYU4Qd1K4Cvt7mmteB64FPgcuAaZZlWcaYNGCXZVlNxphioBRY02HVi4hI2ImKcNMnPZ4+\n6fGHnKupbWBdS6gLCXgvrNvFfv/qmPbXcFGY4g90Ib12xWlxJMV4NN9ORER6rCMGOMuyGo0xtwDv\nYW8j8IRlWeXGmDuAuZZlvQ48DjxljFkF7MIOeQCnAXcYYxqAZuB7lmXt6owXIiIi4S/B62FIbiJD\nchNbHbcsi+1761izI6TXbvt+Vm7bywcVW2loCo7J9EV7AoEuNOAVpcZqXzsREQl7R/VOZlnW28Db\nbY79JuRxLXB5O897CXjpBGsUEZFezhhDeoKX9AQvY4pTWp1rbGqmas9BO9z559yt3bGfz9bs5OX5\nrUf8ZyZ4Ww3FtB/HkZsUjUdbIIiISBjQnyJFRCSsRbhdga0PJrbZh/VgfRPrdrbutVu7Yx9vLba3\nQAh8DZchPzkmEOpK0+MZkJ1AaUacFlIREZFuRQFORER6rOhIN/2zEuiflXDIud3760OGZO4LBLxZ\nq3dQ698DIcJl6JMex4DsBAZkJQQ+JsZEdvVLERERARTgRESkl0qKjWREbCQjCpJaHW9uttiw6wBL\nN9dQvqmapZtqmLVqBy/PCw7HzEmMPiTU5SZFa/EUERHpdApwIiIiIVwuQ2FqLIWpsZw3OCtwfMe+\nOpZuqmHp5prAxw+WbQ3saZfgjfCHOV8g1JVmxGlunYiIdCgFOBERkaOQGhfFaX3TOK1vWuDYwfom\nKra0DnXPfr6Bgw32lgeRbhelGXGteur6ZyeQ4PU49TJERCTMKcCJiIgcp+hIN8PzkxieHxyG2dRs\nsXbH/lahbvrybbzwRWXgmvzkmFahbmBOApkJXg3BFBGRI1KAExER6UBu/8InfdLjuGhoNhDcx668\nJdT5g9275VsCz0uK8bSaVzcw20dxaiwRGoIpIiIhFOBEREQ6Weg+dhPL0gPH99U1snyLHejK/aHu\nyU/XU99or4IZGeGiX2a83UuXbQe7fpkJxEbp7VtEpLfSO4CIiIhD4qIiGFGQzIiC5MCxxqZm1uzY\nH1gBs6Wn7rk5GwEwBgpTYoNDMLMTGJiVQFp8lIZgioj0AgpwIiIi3UiE20XfjHj6ZsRz6XD7mGVZ\nbKmppbwquGDK4qpq3lq8OfC81LhI+mfZQy9bhmIWpcbidinUiYj0JApwIiIi3ZwxhixfNFm+aM4c\nkBE4XlPbwLI2Wxs8PnMNDU323gZej4t+mS1z6hLYt6uJ1KpqYiLdREe6ifFEEB3pxuM26r0TEQkT\nCnAiIiJhKsHrYXRxCqOLUwLH6hubWbVtX0ioq+bNhZt4ZvYG+4LPZx7yddwuQ4zHjTfSbYc7jz/g\nRbqJ9oe8GP+xQx4Hro8g2uMOhMPQx5FulwKiiEgHUYATERHpQSIjXIG5cYywj1mWReXug7w27VNK\n+w/kYH0TBxuaOFDfRG1DEwfqG0MeN7V6vGv/wcA1Lc9r6eE7Wi4DMZEReP2hLibS3e7jliB4aIgM\nDYURhwTJqAgFRBHpPRTgREREejhjDHnJMQxKdTNhYOYJf72GpmYONjRx0B/27GDXGPI45Fyrx42H\nHN9zoCEkODZS29BMfVPzMdXjMrQKf20DYnRkRLs9iDEtYfGQnseQ4x63tnIQkW5FAU5ERESOicft\nwuN2keD1dMrXb2xq5kBDE7UhYS80HIb2BrY+3sRBf29iy+e7AwExGDAbm4+tBzHS7cLrcR0a+NoJ\nhoHj7QwljQnpXQw9r95DETkWCnAiIiLSrUS4XSR0YkCsb2y2ewLb9Ai2DYAt50KPhwbL6oMNbKk+\nGAiGLc87Vq1DXst8QjswWvvr2Bq7gaF5ifRJi1NvoIgowImIiEjvEhnhIjLChY+OD4iWZVHb0Bzs\n8Ws4dJhpq8BX39jqmpaQeLC+ka01tazZ1sj0lxYDdtAblJPAkNxEhuT6GJqbSEFKjHrwRHoZBTgR\nERGRDmKMCQypTDny5Uc0bfp0CgadzKLKPSzcWM2iyj1M/mw9dY32PMHEGA+Dc+wwNyTXx9C8RDIS\nvB3wL4tId6UAJyIiItJNuYyhJC2OkrQ4Lh2eC9iLyKzYupdFldUs3LiHhZXV/P3D1TT55/ZlJEQx\nJDeRYXl2qBuSk4gvpnOGo4pI11OAExEREQkjHreLgdk+Bmb7uHpUPgAH65tYurk60Eu3qLKaKUu3\nBp5TmBITGHo5LC+Rgdk+oiPdTr0EETkBCnAiIiIiYS460s2IgmRGFCQHjlUfbGBxZTULK/ewqHIP\nc9bt4vWFmwB78/bS9Dh76GWePQSzLDMejxZJEen2FOBEREREeiBftIdxpamMK00NHNu2t5ZFG+1Q\nt7CymveWbuH5uRsBe3GXgdkJgfl0Q3ITKU6NxeXSIiki3YkCnIiIiEgvkR7v5cwBXs4ckAHYq2Zu\n3HUw0Eu3sLKa/8zdyL8+WQdAfFQEg3J8DMnzMSw3kSF5iWT7vFr5UsRBCnAiIiIivZQxhvyUGPJT\nYrhwaDYATc0Wq7btC4S6RZXVPDFzLQ1N9iIpqXGRrbYyGJLrIyUuysmXIdKrKMCJiIiISIDbZSjL\njKcsM54rRuYBUNfYRMXmvSyq3MMC/0Ip05dvw7IzHblJ0a2GXg7O9REXpV8zRTqDvrNERERE5EtF\nRbgZmpfI0LxErjvFPravrpElVdWBoZeLKvfw1uLNABgDJWlxgVUvh+Qm0j8rnqgI51e+tCyLhiaL\nxuZmGposmpotGpuaaWj56D/X2GTR0NRMY7P/Y8hzWj8OPrep2WL1ugbWf7KOCLchwmWIcLn8j124\nXf5j7tDjhgi3K+R4yLUt17XzHA1j7b0U4ERERETkmMVFRTCmOIUxxcEty3fuq2NRVTWL/L10H63Y\nwcvzqgDwuA39MhMYkuujLDMeIBCAQkNSgz88hQYj+/jhA9YhxwPnD722Zb+8TlVR3un/hNtlgoEw\nNAS2CYRulwuPO/TaYAgMPedxtxMwXQa32+BxuUiJi2Rwjo8B2QnERCpCOEn/+yIiIiLSIVLiophY\nls7EsnTA7u3aXF0b2HB8UeUeXl+wib11je0+32Ugwu3C4w8hnpBeJ08glLQctx97PS4ioiIOudYO\nJYd5fjtfp71rPW7XIT1focc87tZfz+0yzJw5k9GnjKXJH0qbmi1/yAyGycaQHruGZoum0OPNwVDb\n2GSfC/QUhp5rDr3m0K8d/Dftc00hNdQ2Wu3U0UxTezX4H7fkXpe/d3Vwjo9BOT4G5/oYkJVArIbM\ndhn9T4uIiIhIpzDGkJ0YTXZiNOcOzgKgudlix/463CYYnNwuu5enJ2xZEOsxpPbARV221tSyuLKa\nxVXVLKmqZuaqHbw83+5dNW1DXY6PgdkKdZ1F/6siIiIi0mVcLkN6vNfpMuQYZSR4yQjZggJgW00t\ni6vsULe4sppZq3bwikJdp9P/oIiIiIiIHLP0BC9nJHg5o3/7oW5JVTWfrG4d6opTY1uHuhytWHqs\n9L8lIiIiIiIdot1Qt7eWJVXVLK6sYXFVNZ+t2cWrCzYBdqgrSo1liELdUdP/jIiIiIiIdJr0eC+n\n9/Nyer9jC3WD/YFukH/4ZbzX49RL6FYU4EREREREpEu1F+q2762zQ52/fb52F6+FhrqUWAbl+BiS\n27tDnQKciIiIiIg4Li0+ion90pnYLz1wbMe+Ons+nX8FzLnrdvH6wk2B88WpsYGhl4NyfAzK6fmh\nTgFORERERES6pdQ2ewvCkUNdUSDUJfhDnY+EHhTqFOBERERERCRstBfqdraEOv/wy3nrd/NGDw11\nCnAiIiIiIhLWUuKimFCWzoQ2oW7JphqWVFWzqHLPIaGuMCWGdE8dJUMOkJcc40TZx0UBTkRERERE\nepyUuCjG901jfN+0wLFd++uDPXWV1cxZvZXoSLeDVR47BTgREREREekVkmMjW4W6GTNmkBoX5XBV\nx8bldAEiIiIiIiJydBTgREREREREwoQCnIiIiIiISJhQgBMREREREQkTCnAiIiIiIiJhQgFORERE\nREQkTCjAiYiIiIiIhAkFOBERERERkTChACciIiIiIhImFOBERERERETChAKciIiIiIhImFCAExER\nERERCRMKcCIiIiIiImFCAU5ERERERCRMKMCJiIiIiIiECQU4ERERERGRMKEAJyIiIiIiEiYU4ERE\nRERERMKEApyIiIiIiEiYUIATEREREREJEwpwIiIiIiIiYUIBTkREREREJEwowImIiIiIiIQJY1mW\n0zW0YozZDqx3uo52pAI7nC5CjonuWfjRPQs/umfhRfcr/OiehR/ds/DSXe9XgWVZae2d6HYBrrsy\nxsy1LGuk03XI0dM9Cz+6Z+FH9yy86H6FH92z8KN7Fl7C8X5pCKWIiIiIiEiYUIATEREREREJEwpw\nR+8RpwuQY6Z7Fn50z8KP7ll40f0KP7pn4Uf3LLyE3f3SHDgREREREZEwoR44ERERERGRMKEAJyIi\nIiIiEiYU4I6CMeYcY8xyY8wqY8zPna5HvpwxJs8YM90Ys9QYU26M+ZHTNcmRGWPcxpj5xpg3na5F\njswYk2iMedEYU2GMWWaMOcXpmuTLGWNu9f9MXGKMedYY43W6JmnNGPOEMWabMWZJyLFkY8wUY8xK\n/8ckJ2uUoMPcr7/6fy4uMsa8YoxJdLJGaa29exZy7jZjjGWMSXWitmOhAHcExhg38DfgXGAAcLUx\nZoCzVckRNAK3WZY1ABgD/ED3LCz8CFjmdBFy1O4D3rUsqx8wFN27bs0YkwP8FzDSsqxBgBu4ytmq\npB3/As5pc+znwAeWZZUCH/g/l+7hXxx6v6YAgyzLGgKsAH7R1UXJl/oXh94zjDF5wCRgQ1cXdDwU\n4I5sFLDKsqw1lmXVA88BFztck3wJy7I2W5Y1z/94L/YvljnOViVfxhiTC5wPPOZ0LXJkxhgfcBrw\nOIBlWfWWZe1xtio5ChFAtDEmAogBNjlcj7RhWdZHwK42hy8GnvQ/fhK4pEuLksNq735ZlvW+ZVmN\n/k8/A3K7vDA5rMN8jwHcA/w/ICxWd1SAO7IcYGPI55UoDIQNY0whMByY7WwlcgT3Yv/gbHa6EDkq\nRcB24J/+Ya+PGWNinS5KDs+yrCrgTuy/Lm8Gqi3Let/ZquQoZViWtdn/eAuQ4WQxckxuAN5xugj5\ncsaYi4Eqy7IWOl3L0VKAkx7LGBMHvAT82LKsGqfrkfYZYy4AtlmW9YXTtchRiwBOAv5uWdZwYD8a\n1tWt+edNXYwdvrOBWGPMtc5WJcfKsvd+Cosegt7OGPMr7CkdTztdixyeMSYG+CXwG6drORYKcEdW\nBeSFfJ7rPybdmDHGgx3enrYs62Wn65EvdSpwkTFmHfYQ5dONMZOdLUmOoBKotCyrpWf7RexAJ93X\nmcBay7K2W5bVALwMjHW4Jjk6W40xWQD+j9scrkeOwBjzTeAC4BpLGy53dyXYf9ha6P89JBeYZ4zJ\ndLSqI1CAO7I5QKkxpsgYE4k96ft1h2uSL2GMMdhzc5ZZlnW30/XIl7Ms6xeWZeVallWI/f01zbIs\n9Qx0Y5ZlbQE2GmPK/IfOAJY6WJIc2QZgjDEmxv8z8gy08Ey4eB243v/4euA1B2uRIzDGnIM9JeAi\ny7IOOF2PfDnLshZblpVuWVah//eQSuAk//tct6UAdwT+iai3AO9hv9n9x7KscmerkiM4FbgOuydn\ngb+d53RRIj3MD4GnjTGLgGHAHx2uR76Ev7f0RWAesBj7/f8RR4uSQxhjngU+BcqMMZXGmG8DfwLO\nMsasxO5J/ZOTNUrQYe7Xg0A8MMX/+8fDjhYprRzmnoUdo55dERERERGR8KAeOBERERERkTChACci\nIiIiIhImFOBERERERETChAKciIiIiIhImFCAExERERERCRMKcCIiIiIiImFCAU5ERERERCRM/H9F\nvLDw1u4PTgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2oAAAJOCAYAAADGYfSfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIZJoRYREREREdEw/wJzk6ujwpqgDgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3AAAAJOCAYAAAD27eW+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnIiIiIiISJj4/wGycygzWg28NgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2oAAAJOCAYAAADGYfSfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8mP6r2VcgzcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1080x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qPInjKlSSRc",
        "colab_type": "text"
      },
      "source": [
        "### Mnist_fashion 분류하는 code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJqyoFbBSXJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 100\n",
        "num_epochs = 5\n",
        "learning_rate = 0.001\n",
        "\n",
        "# dataset loader\n",
        "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          drop_last=True)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "root = './data'\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=(0.5,), std=(0.5,))])\n",
        "train_data = dset.FashionMNIST(root=root, train=True, transform=transform, download=True)\n",
        "test_data = dset.FashionMNIST(root=root, train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
        "                                           batch_size = batch_size,\n",
        "                                           shuffle = True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
        "                                           batch_size = batch_size,\n",
        "                                           shuffle = True)\n",
        "\n",
        "\n",
        "labels_map = {0 : 'T-Shirt', 1 : 'Trouser', 2 : 'Pullover', 3 : 'Dress', 4 : 'Coat', 5 : 'Sandal', 6 : 'Shirt',\n",
        "              7 : 'Sneaker', 8 : 'Bag', 9 : 'Ankle Boot'}\n",
        "columns = 5\n",
        "rows = 5\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "\n",
        "for i in range(1, columns*rows+1):\n",
        "    data_idx = np.random.randint(len(train_data))\n",
        "    img = train_data[data_idx][0][0,:,:].numpy() # numpy()를 통해 torch Tensor를 numpy array로 변환\n",
        "    label = labels_map[train_data[data_idx][1]] # item()을 통해 torch Tensor를 숫자로 변환\n",
        "    \n",
        "    fig.add_subplot(rows, columns, i)\n",
        "    plt.title(label)\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "linear_layer = nn.Linear(10, 3)\n",
        "relu = nn.ReLU()\n",
        "\n",
        "# 네트워크 설계\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(DNN, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            torch.nn.Linear(784, 256, bias=True),\n",
        "            torch.nn.BatchNorm1d(256),\n",
        "            torch.nn.ReLU()\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            torch.nn.Linear(256, 64, bias=True)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1) # flatten\n",
        "        x_out = self.layer1(x)\n",
        "        x_out = self.layer2(x_out)\n",
        "        return x_out\n",
        "\n",
        "# Weight initialization\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Linear): # 모델의 모든 MLP 레이어에 대해서\n",
        "        nn.init.xavier_normal_(m.weight) # Weight를 xavier_normal로 초기화\n",
        "        print(m.weight)\n",
        "\n",
        "# 모델 생성\n",
        "torch.manual_seed(7777) # 일관된 weight initialization을 위한 random seed 설정\n",
        "model = DNN().to(device)\n",
        "model.apply(weights_init) # 모델에 weight_init 함수를 적용하여 weight를 초기화\n",
        "\n",
        "# Loss function, Optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (imgs, labels) in enumerate(train_loader):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        ## 코드 시작 ##\n",
        "        outputs = model(imgs)  # 위의 설명 1. 을 참고하여 None을 채우세요.\n",
        "        loss = criterion(outputs, labels)     # 위의 설명 2. 를 참고하여 None을 채우세요.\n",
        "        \n",
        "        optimizer.zero_grad()            # Clear gradients: 위의 설명 3. 을 참고하여 None을 채우세요.\n",
        "        loss.backward()            # Gradients 계산: 위의 설명 4. 를 참고하여 None을 채우세요.\n",
        "        optimizer.step()            # Parameters 업데이트: 위의 설명 5. 를 참고하여 None을 채우세요.\n",
        "        ## 코드 종료 ##\n",
        "        \n",
        "        _, argmax = torch.max(outputs, 1)\n",
        "        accuracy = (labels == argmax).float().mean()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n",
        "                epoch+1, num_epochs, i+1, len(train_loader), loss.item(), accuracy.item() * 100))\n",
        "\n",
        "# Test       \n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, (imgs, labels) in enumerate(test_loader):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        outputs = model(imgs)\n",
        "        _, argmax = torch.max(outputs, 1) # max()를 통해 최종 출력이 가장 높은 class 선택\n",
        "        total += imgs.size(0)\n",
        "        correct += (labels == argmax).sum().item()\n",
        "    \n",
        "    print('Test accuracy for {} images: {:.2f}%'.format(total, correct / total * 100))\n",
        "\n",
        "\n",
        "# Check~\n",
        "columns = 5\n",
        "rows = 5\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "\n",
        "model.eval()\n",
        "for i in range(1, columns*rows+1):\n",
        "    data_idx = np.random.randint(len(test_data))\n",
        "    input_img = test_data[data_idx][0].unsqueeze(dim=0).to(device) \n",
        "    '''\n",
        "    unsqueeze()를 통해 입력 이미지의 shape을 (1, 28, 28)에서 (1, 1, 28, 28)로 변환. \n",
        "    모델에 들어가는 입력 이미지의 shape은 (batch_size, channel, width, height) 되어야 함에 주의하세요!\n",
        "    '''\n",
        "    output = model(input_img)\n",
        "    _, argmax = torch.max(output, 1)\n",
        "    pred = labels_map[argmax.item()]\n",
        "    label = labels_map[test_data[data_idx][1]]\n",
        "    \n",
        "    fig.add_subplot(rows, columns, i)\n",
        "    if pred == label:\n",
        "        plt.title(pred + '(O)')\n",
        "    else:\n",
        "        plt.title(pred + '(X)' + ' / ' + label)\n",
        "    plot_img = test_data[data_idx][0][0,:,:]\n",
        "    plt.imshow(plot_img, cmap='gray')\n",
        "    plt.axis('off')\n",
        "model.train()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfvhxoLWmVPV",
        "colab_type": "text"
      },
      "source": [
        "# 6. CNN (Convolutional Neural Network)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVYbnpE1nV2L",
        "colab_type": "text"
      },
      "source": [
        "## CNN이란?\n",
        "![](https://taewanmerepo.github.io/2018/01/cnn/head.png)\n",
        "> 중요 특징\n",
        "- CNN은 이미지의 feature를 추출하는 모델이다!\n",
        "- input에 대해 몇 번의 convolution 연산을 activation function과 함께 적용한 이후 pooling으로 전체 크기를 줄여주는 과정을 반복 -> 필터가 이미지를 지나가면서 이미지의 부분 부분이 필터와 얼마나 일치하는지 계산 -> 이를 통해서 이미지의 feature를 추출함\n",
        "- CNN은 convolution 연산을 사용해 하나의 함수가 다른 함수와 얼마나 일치하는지 판단, 필터가 이미지를 지나가면서 이미지의 부분 부분이 필터와 얼마나 일치하는지 계산\n",
        "\n",
        "> 기타 특징\n",
        "- Feature extraction에서 channel을 늘려서 학습한 뒤 classification에서 channel을 줄여서 output으로 바꿔줌!\n",
        "- CNN은 parameter를 공유하여 전체 parameter 수를 줄여 overfitting을 줄여줌\n",
        "- 하나의 이미지에 같은 필터를 연달아 적용하여 가중치가 공유. 그래서 기존 neural network보다 학습의 대상이 되는 변수가 적다!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9HBT10VDLsV",
        "colab_type": "text"
      },
      "source": [
        "## CNN 과정 살펴보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwPuIaPzDMQn",
        "colab_type": "text"
      },
      "source": [
        "> Step.1\n",
        "- device에서 'cuda' 설정을 해준다.\n",
        "- GPU를 쓰기 위함\n",
        "\n",
        "> Step.2\n",
        "- 1) Data 불러오기\n",
        "- 2) Hyperparameter 설정해주기\n",
        "\n",
        "> Step.3  \n",
        "- 1) Layer 만들어주기: Convolution 연산, Activation Function, Pooling  \n",
        "- 2) Funnly connected layer로 모아주기  \n",
        "- 3) 모델 설정해주고 Loss function, Optimizer 설정해주기\n",
        "\n",
        "> Step.4  \n",
        "- 모델 Train하기  \n",
        "\n",
        "> Step.5  \n",
        "- 학습된 모델을 test data에 넣어서 성능을 test하기!!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mN_CYG-3HTOD"
      },
      "source": [
        "## 하나하나 뜯어봅시다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Gm6CisLiHTOF"
      },
      "source": [
        "### Step.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4azL2mQSHTOG"
      },
      "source": [
        "---\n",
        "- device에서 'cuda' 설정을 해준다.\n",
        "- GPU를 쓰기 위함\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BcYAGOVMHTOH"
      },
      "source": [
        "GPU는 써야 성능이 오지게 빨라지겠쥬?\n",
        "```python\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "```\n",
        "요로로콤 설정하시면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PA2pDhTBHTOH"
      },
      "source": [
        "### Step.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7Hz1Wvw2HTOI"
      },
      "source": [
        "---\n",
        "1) Data 불러오기  \n",
        "2) Hyperparameter 설정해주기\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YX-bQFaWHTOJ"
      },
      "source": [
        "1) DataLoader를 사용해서 data를 불러주면 된다~  \n",
        "2) Learning rate, Training_epochs, batch_size 등을 설정해주면 됨!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BKVSlh3lHTOK"
      },
      "source": [
        "### Step.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W4qV9dA_HTOL"
      },
      "source": [
        "---\n",
        "1) Layer 만들어주기: Convolution 연산, Activation Function, Pooling  \n",
        "2) Fully connected layer로 모아주기  \n",
        "3) 모델 설정해주고 Loss function, Optimizer 설정해주기\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X5jhCX5OHTOM"
      },
      "source": [
        "1) Layer 만들기\n",
        "- Layer는 크게 Conv2d(Convolution 연산), Activation Function, Pooling을 사용한다.\n",
        "- Conv2d 메소드에 대한 설명은 위에 ```메소즈참조```를 참조\n",
        "- Convolution 연산을 통해서 filter와 이미지가 얼마나 일치하는지 보고 -> 비선형 변환 -> Pooling으로 그림의 크기를 줄여주는 과정의 반복!\n",
        "- 즉! 크기는 줄이고! Channel은 늘리고!!\n",
        "  - Channel을 늘려주는 이유: 같은 채널을 여러 겹으로 쌓는 방식! 이를 통해서 채널 수를 늘려서 여러 개로 만든 다음에 이것을 다시 수합해서 얻은 결과로 학습하기 위함\n",
        "- input과 1st hidden layer에 필터가 지정한 개수만큼 학습되고, 이를 통해 다양한 특징들을 뽑아내 다음 층으로 전달!!! 필터를 통해 뽑은 이런 특성들이 중첩됨에 따라 모델은 더 복잡하고 다양한 형태를 구분하는 거임!!!\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "2) Fully connected layer\n",
        "- Fully connected layer(FC)는 ```용어참조``` 확인\n",
        "\n",
        "![](https://taewanmerepo.github.io/2018/01/cnn/head.png)\n",
        "- Layer를 만들어 통과시키면서 channel을 늘리는 건 위 그림에서 Feature extraction 부분에 해당!\n",
        "- Feature extraction에서 channel을 늘리고 classification에서 늘린 channel을 줄여서 output으로 바꿔주는 것!\n",
        "- Channel을 줄일 때 fully connected layer를 사용\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "3) Loss function, Optimizer\n",
        "- Loss function은 보통 BCE, CE를 사용하고 Optimizer는 Adam을 사용한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9MlBrxX-HTON"
      },
      "source": [
        "### Step.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lYMhJj0MHTOO"
      },
      "source": [
        "---\n",
        "- 모델 Train하기 \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yzJIFuO8HTOP"
      },
      "source": [
        "- 모델을 train하는 과정은 DNN과 유사\n",
        "- Loss function을 구하고, Backpropgation을 진행한 뒤에, Optimizer를 통해서 parameter를 업데이트해주기!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IJOYGnWxHTOT"
      },
      "source": [
        "### Step.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iDT-6wqmHTOU"
      },
      "source": [
        "---\n",
        "- 학습된 모델을 test data에 넣어서 성능을 test하기!! \n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fvqY3r_NHTOV"
      },
      "source": [
        "test는 test지 무슨 설명이 더 필요항가~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUHDAlyP9vN9",
        "colab_type": "text"
      },
      "source": [
        "## 코드 살펴봅시다~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bCODadtTuj-",
        "colab_type": "text"
      },
      "source": [
        "### Output size 구하는 code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edYkDjPySopl",
        "colab_type": "code",
        "outputId": "a06f10f5-40e1-4065-b7f3-f4bc1d521762",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## Output size 구해보기\n",
        "# Conv2d, MaxPool2d 예시\n",
        "# CNN implementation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "inputs = torch.Tensor(1,1,28,28)\n",
        "conv1 = nn.Conv2d(1,5,5)    # input channel은 1개, Output channel은 5개, filter_size는 5\n",
        "pool = nn.MaxPool2d(2)\n",
        "out = conv1(inputs)\n",
        "out2 = pool(out)\n",
        "out.size(), out2.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5, 24, 24]), torch.Size([1, 5, 12, 12]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPC5_RUmTyRF",
        "colab_type": "text"
      },
      "source": [
        "### Mnist에 CNN 적용하기 (layer 2개)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7_UjkYL9y3a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################################################################################################\n",
        "############################################ Step.1 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.init\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(777) # random value를 고정해주는 코드\n",
        "if device =='cuda':\n",
        "  torch.cuda.manual_seed_all(777)\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.2 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# hyperparameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "\n",
        "# MNIST dataset\n",
        "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
        "                          train = True,\n",
        "                          transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root = 'MNIST_data/',\n",
        "                        train = False,\n",
        "                        transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "\n",
        "# data_loader\n",
        "data_loader = torch.utils.data.DataLoader(dataset=mnist_train, \n",
        "batch_size = batch_size,\n",
        "shuffle = True,\n",
        "drop_last = True)\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.3 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "        nn.Conv2d(1,32,kernel_size=3, stride=1, padding=1),   # 합성곱 연산하고\n",
        "        nn.ReLU(),    # Activation function을 통과하고\n",
        "        nn.MaxPool2d(2)   # Pooling 중에서 Maxpooling을 진행!\n",
        "    )\n",
        "    self.layer2 = nn.Sequential(\n",
        "        nn.Conv2d(32,64, kernel_size=3, stride = 1, padding = 1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2)\n",
        "    )\n",
        "    self.fc = nn.Linear(7*7*64, 10, bias=True)\n",
        "    torch.nn.init.xavier_uniform(self.fc.weight)    # Xavier로 weight initialization을 진행\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.layer1(x)\n",
        "    out = self.layer2(out)\n",
        "\n",
        "    out = out.view(out.size(0),-1)  # 다음 fully connected layer에 넣어기 위해 1줄로 flatten해서 shape을 맞춰주는 것이다.\n",
        "    out = self.fc(out)\n",
        "    return out\n",
        "\n",
        "model = CNN().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)    # Cross entropy로 loss function을 계산\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)    # Adam Optimizer 사용!\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.4 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# training\n",
        "total_batch = len(data_loader)\n",
        "\n",
        "for epoch in range(training_epochs):\n",
        "  avg_cost = 0\n",
        "\n",
        "  for X, Y in data_loader:\n",
        "    X = X.to(device)\n",
        "    Y = Y.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    hypothesis = model(X)\n",
        "    \n",
        "    cost = criterion(hypothesis, Y)\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    avg_cost += cost / total_batch\n",
        "\n",
        "  print('[Epoch:{}] cost = {}'.format(epoch+1, avg_cost))\n",
        "\n",
        "print('Learning Finished!')\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.5 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "''' Test할 때는 no_grad() 하는 거 잊지 말기!!!'''\n",
        "\n",
        "with torch.no_grad():\n",
        "  X_test = mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float().to(device)\n",
        "  Y_test = mnist_test.test_labels.to(device)\n",
        "\n",
        "  prediction = model(X_test)\n",
        "  correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "  accuracy = correct_prediction.float().mean()\n",
        "  print('Accuracy: ', accuracy.item())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlT0Mwg3T57Y",
        "colab_type": "text"
      },
      "source": [
        "### Mnist에 CNN 적용하기 (layer 3개)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6yQIxv5y8LAi",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch.nn.init"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b2P-E8dB8LAt",
        "colab": {}
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(777) # random value를 고정해주는 코드\n",
        "if device =='cuda':\n",
        "  torch.cuda.manual_seed_all(777)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kO4AFifQ8LA1",
        "colab": {}
      },
      "source": [
        "# parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "\n",
        "# MNIST dataset\n",
        "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
        "                          train = True,\n",
        "                          transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root = 'MNIST_data/',\n",
        "                        train = False,\n",
        "                        transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "\n",
        "# data_loader\n",
        "data_loader = torch.utils.data.DataLoader(dataset=mnist_train, \n",
        "batch_size = batch_size,\n",
        "shuffle = True,\n",
        "drop_last = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yMfmHmiQ8LA6",
        "colab": {}
      },
      "source": [
        "class CNN(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "        nn.Conv2d(1,32,kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2)\n",
        "    )\n",
        "    self.layer2 = nn.Sequential(\n",
        "        nn.Conv2d(32,64, kernel_size=3, stride = 1, padding = 1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2)\n",
        "    )\n",
        "\n",
        "    self.layer3 = nn.Sequential(\n",
        "        nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2)\n",
        "    )\n",
        "\n",
        "    self.fc1 = nn.Linear(3*3*128, 625)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(625, 10, bias=True)\n",
        "    torch.nn.init.xavier_uniform(self.fc1.weight)\n",
        "    torch.nn.init.xavier_uniform(self.fc2.weight)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.layer1(x)\n",
        "    out = self.layer2(out)\n",
        "    out = self.layer3(out)\n",
        "\n",
        "    out = out.view(out.size(0),-1)\n",
        "    out = self.fc1(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.fc2(out)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1be9e1d9-b109-481d-d567-3c746388df8b",
        "id": "GggU7x-e8LBB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "model = CNN().to(device)\n",
        "\n",
        "value = (torch.Tensor(1,1,28,28)).to(device)\n",
        "print( (model(value)).shape )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 10])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yepftuzl8LBK",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "65c480e0-cd6a-4efc-e375-eb2bbd82e6b1",
        "id": "YKJStjYa8LBX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        }
      },
      "source": [
        "# training\n",
        "total_batch = len(data_loader)\n",
        "\n",
        "for epoch in range(training_epochs):\n",
        "  avg_cost = 0\n",
        "\n",
        "  for X, Y in data_loader:\n",
        "    X = X.to(device)\n",
        "    Y = Y.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    hypothesis = model(X)\n",
        "    \n",
        "    cost = criterion(hypothesis, Y)\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    avg_cost += cost / total_batch\n",
        "\n",
        "  print('[Epoch:{}] cost = {}'.format(epoch+1, avg_cost))\n",
        "\n",
        "print('Learning Finished!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch:1] cost = 0.1541777104139328\n",
            "[Epoch:2] cost = 0.04089852422475815\n",
            "[Epoch:3] cost = 0.02892174944281578\n",
            "[Epoch:4] cost = 0.022953888401389122\n",
            "[Epoch:5] cost = 0.01613018475472927\n",
            "[Epoch:6] cost = 0.014172021299600601\n",
            "[Epoch:7] cost = 0.012643410824239254\n",
            "[Epoch:8] cost = 0.010536188259720802\n",
            "[Epoch:9] cost = 0.008092082105576992\n",
            "[Epoch:10] cost = 0.009055017493665218\n",
            "[Epoch:11] cost = 0.007136085536330938\n",
            "[Epoch:12] cost = 0.009609756991267204\n",
            "[Epoch:13] cost = 0.0034160716459155083\n",
            "[Epoch:14] cost = 0.0070384531281888485\n",
            "[Epoch:15] cost = 0.004809462931007147\n",
            "Learning Finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "63410bb9-d543-4c8d-fb93-5d8155557897",
        "id": "e9f1DL7r8LBn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "with torch.no_grad():\n",
        "  X_test = mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float().to(device)\n",
        "  Y_test = mnist_test.test_labels.to(device)\n",
        "\n",
        "  prediction = model(X_test)\n",
        "  correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "  accuracy = correct_prediction.float().mean()\n",
        "  print('Accuracy: ', accuracy.item())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.9892999529838562\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:58: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:48: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkJmXY3N9NZv",
        "colab_type": "text"
      },
      "source": [
        "깊게 쌓는 게 마냥 좋지만은 않다. 얼마나 효율적으로 쌓는지가 중요하다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_iS_0n2CMJ5",
        "colab_type": "text"
      },
      "source": [
        "### Mnist와 Visdom 사용하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5f836ef1-d603-4dee-ef2e-e83d38814a82",
        "id": "y81bl4T4ULuf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "!pip install visdom"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting visdom\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/75/e078f5a2e1df7e0d3044749089fc2823e62d029cc027ed8ae5d71fafcbdc/visdom-0.1.8.9.tar.gz (676kB)\n",
            "\u001b[K     |████████████████████████████████| 686kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.6/dist-packages (from visdom) (1.17.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from visdom) (1.3.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from visdom) (2.21.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.6/dist-packages (from visdom) (4.5.3)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from visdom) (17.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from visdom) (1.12.0)\n",
            "Collecting jsonpatch\n",
            "  Downloading https://files.pythonhosted.org/packages/86/7e/035d19a73306278673039f0805b863be8798057cc1b4008b9c8c7d1d32a3/jsonpatch-1.24-py2.py3-none-any.whl\n",
            "Collecting torchfile\n",
            "  Downloading https://files.pythonhosted.org/packages/91/af/5b305f86f2d218091af657ddb53f984ecbd9518ca9fe8ef4103a007252c9/torchfile-0.1.0.tar.gz\n",
            "Collecting websocket-client\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/19/44753eab1fdb50770ac69605527e8859468f3c0fd7dc5a76dd9c4dbd7906/websocket_client-0.56.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 40.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from visdom) (4.3.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->visdom) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->visdom) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->visdom) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->visdom) (3.0.4)\n",
            "Collecting jsonpointer>=1.9\n",
            "  Downloading https://files.pythonhosted.org/packages/18/b0/a80d29577c08eea401659254dfaed87f1af45272899e1812d7e01b679bc5/jsonpointer-2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->visdom) (0.46)\n",
            "Building wheels for collected packages: visdom, torchfile\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.1.8.9-cp36-none-any.whl size=655252 sha256=82e8ac16c98b0f4f073b9c507b404f8fe532382490edd261c1947bbf87c51474\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/19/a7/6d589ed967f4dfefd33bc166d081257bd4ed0cb618dccfd62a\n",
            "  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchfile: filename=torchfile-0.1.0-cp36-none-any.whl size=5711 sha256=e64e6c3a34539bda831e5d4af69a03eaf64af1aedb2f5802bafd74b1792718cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/c3/d6/9a1cc8f3a99a0fc1124cae20153f36af59a6e683daca0a0814\n",
            "Successfully built visdom torchfile\n",
            "Installing collected packages: jsonpointer, jsonpatch, torchfile, websocket-client, visdom\n",
            "Successfully installed jsonpatch-1.24 jsonpointer-2.0 torchfile-0.1.0 visdom-0.1.8.9 websocket-client-0.56.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCtRjHPMCOPY",
        "colab_type": "code",
        "outputId": "717f77d5-f8c3-4173-c195-9c57fe5b9b8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "import visdom\n",
        "\n",
        "python -m visdom.server\n",
        "vis = visdom.Visdom()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting up a new session...\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\", line 159, in _new_conn\n",
            "    (self._dns_host, self.port), self.timeout, **extra_kw)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/util/connection.py\", line 80, in create_connection\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/util/connection.py\", line 70, in create_connection\n",
            "    sock.connect(sa)\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 600, in urlopen\n",
            "    chunked=chunked)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 354, in _make_request\n",
            "    conn.request(method, url, **httplib_request_kw)\n",
            "  File \"/usr/lib/python3.6/http/client.py\", line 1254, in request\n",
            "    self._send_request(method, url, body, headers, encode_chunked)\n",
            "  File \"/usr/lib/python3.6/http/client.py\", line 1300, in _send_request\n",
            "    self.endheaders(body, encode_chunked=encode_chunked)\n",
            "  File \"/usr/lib/python3.6/http/client.py\", line 1249, in endheaders\n",
            "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
            "  File \"/usr/lib/python3.6/http/client.py\", line 1036, in _send_output\n",
            "    self.send(msg)\n",
            "  File \"/usr/lib/python3.6/http/client.py\", line 974, in send\n",
            "    self.connect()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\", line 181, in connect\n",
            "    conn = self._new_conn()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\", line 168, in _new_conn\n",
            "    self, \"Failed to establish a new connection: %s\" % e)\n",
            "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f3c0ae90710>: Failed to establish a new connection: [Errno 111] Connection refused\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/requests/adapters.py\", line 449, in send\n",
            "    timeout=timeout\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 638, in urlopen\n",
            "    _stacktrace=sys.exc_info()[2])\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/util/retry.py\", line 399, in increment\n",
            "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
            "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=8097): Max retries exceeded with url: /env/main (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f3c0ae90710>: Failed to establish a new connection: [Errno 111] Connection refused',))\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/visdom/__init__.py\", line 711, in _send\n",
            "    data=json.dumps(msg),\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/visdom/__init__.py\", line 677, in _handle_post\n",
            "    r = self.session.post(url, data=data)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/requests/sessions.py\", line 581, in post\n",
            "    return self.request('POST', url, data=data, json=json, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/requests/sessions.py\", line 533, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/requests/sessions.py\", line 646, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/requests/adapters.py\", line 516, in send\n",
            "    raise ConnectionError(e, request=request)\n",
            "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8097): Max retries exceeded with url: /env/main (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f3c0ae90710>: Failed to establish a new connection: [Errno 111] Connection refused',))\n",
            "[Errno 99] Cannot assign requested address\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception in user code:\n",
            "------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Errno 99] Cannot assign requested address\n",
            "[Errno 99] Cannot assign requested address\n",
            "Visdom python client failed to establish socket to get messages from the server. This feature is optional and can be disabled by initializing Visdom with `use_incoming_socket=False`, which will prevent waiting for this request to timeout.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8I8wyRNCvNd",
        "colab_type": "text"
      },
      "source": [
        "Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nYZkjWICpne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vis.text(\"Hello, world!\",env=\"main\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX_p6f3GCwgV",
        "colab_type": "text"
      },
      "source": [
        "Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X04bA1O8CxRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a=torch.randn(3,200,200)\n",
        "vis.image(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t3KNs93DOJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vis.images(torch.Tensor(3,3,28,28))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-R8zdLRDRLG",
        "colab_type": "text"
      },
      "source": [
        "Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVIjgyBVDSIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 시간이 좀 걸립니다.\n",
        "MNIST = dsets.MNIST(root=\"./MNIST_data\",train = True,transform=torchvision.transforms.ToTensor(), download=True)\n",
        "cifar10 = dsets.CIFAR10(root=\"./cifar10\",train = True, transform=torchvision.transforms.ToTensor(),download=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM3UAXvqDT5Q",
        "colab_type": "text"
      },
      "source": [
        "CIFAR 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49K9S3nuDUCJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = cifar10.__getitem__(0)\n",
        "print(data[0].shape)\n",
        "vis.images(data[0],env=\"main\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjQF_ULjDUJ4",
        "colab_type": "text"
      },
      "source": [
        "MNIST\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rAVdmahDUPn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = MNIST.__getitem__(0)\n",
        "print(data[0].shape)\n",
        "vis.images(data[0],env=\"main\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq-mfQ2ADUVq",
        "colab_type": "text"
      },
      "source": [
        "check Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrM9iZuODdwi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_loader = torch.utils.data.DataLoader(dataset = MNIST,\n",
        "                                          batch_size = 32,\n",
        "                                          shuffle = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjV_UALdDd6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "for num, value in enumerate(data_loader):\n",
        "    value = value[0]\n",
        "    print(value.shape)\n",
        "    vis.images(value)\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO7o30CYDeCh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vis.close(env=\"main\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHZjc05SDgjM",
        "colab_type": "text"
      },
      "source": [
        "Line Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jksptLbtDgqX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_data = torch.randn(5)\n",
        "plt = vis.line (Y=Y_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArKbTjKlDgyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_data = torch.Tensor([1,2,3,4,5])\n",
        "plt = vis.line(Y=Y_data, X=X_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x405wInWDlHP",
        "colab_type": "text"
      },
      "source": [
        "Line update"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYQ6lCFiDlX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_append = torch.randn(1)\n",
        "X_append = torch.Tensor([6])\n",
        "\n",
        "vis.line(Y=Y_append, X=X_append, win=plt, update='append')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P71Dv_3oDsos",
        "colab_type": "text"
      },
      "source": [
        "multiple Line on single windows¶"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxNqyLmpDlfc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num = torch.Tmultiple Line on single windows¶ensor(list(range(0,10)))\n",
        "num = num.view(-1,1)\n",
        "num = torch.cat((num,num),dim=1)\n",
        "\n",
        "plt = vis.line(Y=torch.randn(10,2), X = num)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFzwSvhNDu0L",
        "colab_type": "text"
      },
      "source": [
        "Line info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_a15yL2Du7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt = vis.line(Y=Y_data, X=X_data, opts = dict(title='Test', showlegend=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCQR9Z0bDvCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt = vis.line(Y=Y_data, X=X_data, opts = dict(title='Test', legend = ['1번'],showlegend=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECaFW2vbDyn3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt = vis.line(Y=torch.randn(10,2), X = num, opts=dict(title='Test', legend=['1번','2번'],showlegend=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0xIsBGiDywP",
        "colab_type": "text"
      },
      "source": [
        "make function for update line¶\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH998zhsDy4v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_tracker(loss_plot, loss_value, num):\n",
        "    '''num, loss_value, are Tensor'''\n",
        "    vis.line(X=num,\n",
        "             Y=loss_value,\n",
        "             win = loss_plot,\n",
        "             update='append'\n",
        "             )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxSOuC0NDyhY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt = vis.line(Y=torch.Tensor(1).zero_())\n",
        "\n",
        "for i in range(500):\n",
        "    loss = torch.randn(1) + i\n",
        "    loss_tracker(plt, loss, torch.Tensor([i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7_V6eojD4O_",
        "colab_type": "text"
      },
      "source": [
        "close the window\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FAjIluTD4Vh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vis.close(env=\"main\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2gajRY7FUUSv"
      },
      "source": [
        "### 내 사진으로 CNN 돌리기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tr9YeIZvUUSw",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from matplotlib.pyplot import imshow\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wsPeklkeUUSy",
        "colab": {}
      },
      "source": [
        "trans = transforms.Compose([\n",
        "    transforms.Resize((64,128))    \n",
        "])\n",
        "train_data = torchvision.datasets.ImageFolder(root='custom_data/origin_data', transform=trans)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6WQ3QPh5UUS0"
      },
      "source": [
        "사진 한 번 좀 보고!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0uq2zNyPUUS0",
        "colab": {}
      },
      "source": [
        "for num, value in enumerate(train_data):\n",
        "    data, label = value\n",
        "    print(num, data, label)\n",
        "    \n",
        "    imshow(data)\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8hY_rm9rUUS2"
      },
      "source": [
        "새로 저장하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9AaESZnYUUS3",
        "colab": {}
      },
      "source": [
        "for num, value in enumerate(train_data):\n",
        "    data, label = value\n",
        "    print(num, data, label)\n",
        "    \n",
        "    if(label==0):\n",
        "        data.save(\"custom_data/train data/mouse/%d_%d.jpeg\"%(num, label))\n",
        "    else:\n",
        "        data.save(\"custom_data/train data/watch/%d_%d.jpeg\"%(num, label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FwmoUb5WUUS4"
      },
      "source": [
        "학습하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3SQPMyjaUUS5",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uwERKeWyUUS6",
        "colab": {}
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(777)\n",
        "if device =='cuda':\n",
        "    torch.cuda.manual_seed_all(777)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UPWkE2LRUUS-",
        "colab": {}
      },
      "source": [
        "trans = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_data = torchvision.datasets.ImageFolder(root='./custom_data/train data', transform=trans)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i35ZfajHUUS_",
        "colab": {}
      },
      "source": [
        "data_loader = DataLoader(dataset = train_data, batch_size = 8, shuffle = True, num_workers=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tAIV-40UUUTB",
        "colab": {}
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3,6,5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(6,16,5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Linear(16*13*29, 120),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(120,2)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.view(out.shape[0], -1)\n",
        "        out = self.layer3(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dJ7DGeG0UUTC",
        "colab": {}
      },
      "source": [
        "#testing \n",
        "net = CNN().to(device)\n",
        "test_input = (torch.Tensor(3,3,64,128)).to(device)\n",
        "test_out = net(test_input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RiVnGTa4UUTE",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(net.parameters(), lr=0.00005)    # lr을 높이면 잘 학습된다.\n",
        "loss_func = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7TM-opr-UUTF",
        "colab": {}
      },
      "source": [
        "total_batch = len(data_loader)\n",
        "\n",
        "epochs = 7\n",
        "for epoch in range(epochs):\n",
        "    avg_cost = 0.0\n",
        "    for num, data in enumerate(data_loader):\n",
        "        imgs, labels = data\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = net(imgs)\n",
        "        loss = loss_func(out, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        avg_cost += loss / total_batch\n",
        "        \n",
        "    print('[Epoch:{}] cost = {}'.format(epoch+1, avg_cost))\n",
        "print('Learning Finished!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p_Hs3yDtUUTI"
      },
      "source": [
        "학습한 모델 저장하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "frWmBHwjUUTJ",
        "colab": {}
      },
      "source": [
        "torch.save(net.state_dict(), \"./model/model.pth\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3gHBupZAUUTL"
      },
      "source": [
        "저장된 모델 부르기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rwXtjTRiUUTN",
        "colab": {}
      },
      "source": [
        "new_net = CNN().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "69jAo2NCUUTT",
        "colab": {}
      },
      "source": [
        "new_net.load_state_dict(torch.load('./model/model.pth'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "afBKOMijUUTW",
        "colab": {}
      },
      "source": [
        "print(net.layer1[0])\n",
        "print(new_net.layer1[0])\n",
        "\n",
        "print(net.layer1[0].weight[0][0][0])\n",
        "print(new_net.layer1[0].weight[0][0][0])\n",
        "\n",
        "net.layer1[0].weight[0] == new_net.layer1[0].weight[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7OdSsZIAUUTZ"
      },
      "source": [
        "학습시킨 모델 test하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h1dyDCVzUUTZ",
        "colab": {}
      },
      "source": [
        "trans=torchvision.transforms.Compose([\n",
        "    transforms.Resize((64,128)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "test_data = torchvision.datasets.ImageFolder(root='custom_data/test_data', transform=trans)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uuHrz3AlUUTb",
        "colab": {}
      },
      "source": [
        "test_set = DataLoader(dataset = test_data, batch_size = len(test_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oSUE0Gz4UUTe",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "    for num, data in enumerate(test_set):\n",
        "        imgs, label = data\n",
        "        imgs = imgs.to(device)\n",
        "        label = label.to(device)\n",
        "        \n",
        "        prediction = new_net(imgs)\n",
        "        \n",
        "        correct_prediction = torch.argmax(prediction, 1) == label\n",
        "        \n",
        "        accuracy = correct_prediction.float().mean()\n",
        "        print('Accuracy:', accuracy.item())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGaQBIA8e4K-",
        "colab_type": "text"
      },
      "source": [
        "## VGG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHAy93K9wsQj",
        "colab_type": "text"
      },
      "source": [
        "### VGGNET은\n",
        "- Oxford Visual Geometry Group에서 만든 모델의 깊이와 구조에 변화를 주며 만든 Network\n",
        "- Convolution layer에서 3*3 convolution 필터와 stride=1, padding=1만을 이용함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAXRuHf1e9rY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.utils.model_zoo as model_zoo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM3UQCEVe5ra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "__all__ = [\n",
        "    'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n",
        "    'vgg19_bn', 'vgg19',\n",
        "]\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'vgg11': 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth',\n",
        "    'vgg13': 'https://download.pytorch.org/models/vgg13-c768596a.pth',\n",
        "    'vgg16': 'https://download.pytorch.org/models/vgg16-397923af.pth',\n",
        "    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',\n",
        "    'vgg11_bn': 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth',\n",
        "    'vgg13_bn': 'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth',\n",
        "    'vgg16_bn': 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth',\n",
        "    'vgg19_bn': 'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth',\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjNU8w0Ve5zt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self, features, num_classes=1000, init_weights=True):\n",
        "        super(VGG, self).__init__()\n",
        "        \n",
        "        self.features = features #convolution layer\n",
        "        \n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "        \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )#Fully Connected layer\n",
        "        \n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x) #Convolution \n",
        "        x = self.avgpool(x) # average pooling\n",
        "        x = x.view(x.size(0), -1) #\n",
        "        x = self.classifier(x) #FC layer\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjy8i6Fhe54g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_layers(cfg, batch_norm=False):\n",
        "    layers = [ ]\n",
        "    in_channels = 3\n",
        "    \n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "            if batch_norm:\n",
        "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "            else:\n",
        "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "                     \n",
        "    return nn.Sequential(*layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u16llGzae5-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cfg = {\n",
        "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], #8 + 3 =11 == vgg11\n",
        "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], # 10 + 3 = vgg 13\n",
        "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'], #13 + 3 = vgg 16\n",
        "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'], # 16 +3 =vgg 19\n",
        "    'custom' : [64,64,64,'M',128,128,128,'M',256,256,256,'M']\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbTGNLmpe6EG",
        "colab_type": "code",
        "outputId": "564c6855-9822-4940-b89c-106ef263d4c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "conv = make_layers(cfg['custom'], batch_norm=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 99] Cannot assign requested address\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6djXZsJYe6J0",
        "colab_type": "code",
        "outputId": "bccc1fec-1761-40cc-aca3-9a20c2823f5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "CNN = VGG(make_layers(cfg['custom']), num_classes=10, init_weights=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 99] Cannot assign requested address\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6U_yPM8e6Po",
        "colab_type": "code",
        "outputId": "1bb048b4-f114-479f-a76c-a1dd8e08ff7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        }
      },
      "source": [
        "CNN"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (10): ReLU(inplace=True)\n",
              "    (11): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (12): ReLU(inplace=True)\n",
              "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): ReLU(inplace=True)\n",
              "    (18): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (19): ReLU(inplace=True)\n",
              "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PrBCXstnH0G",
        "colab_type": "text"
      },
      "source": [
        "## RESNET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7YEngygw97x",
        "colab_type": "text"
      },
      "source": [
        "### ResNet(Residual Network)란\n",
        "- shortcut connections 방법을 통한 residual learning\n",
        "- layer가 깊어질수록 gradient vanishing/exploding 문제가 발생하는 것을 residual learning으로 해결\n",
        "- 핵심; 특정 위치에서 입력이 들어왔을 때 convolution 연산을 통과한 결과와 입력으로 들어온 결과 두 가지를 더해서 다음 layer에 전달하는 것!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0UMHqmInNZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "\n",
        "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
        "           'resnet152']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kk2T-WlnNhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0dpJP-mnNmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x) # 3x3 stride = 2\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out) # 3x3 stride = 1\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOM2PkxwnNzX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = conv1x1(inplanes, planes) #conv1x1(64,64)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes, stride)#conv3x3(64,64)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = conv1x1(planes, planes * self.expansion) #conv1x1(64,256)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x) # 1x1 stride = 1\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out) # 3x3 stride = stride \n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out) # 1x1 stride = 1\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77CtOOZjnXm7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResNet(nn.Module):\n",
        "    # model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs) #resnet 50 \n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False):\n",
        "        super(ResNet, self).__init__()\n",
        "        \n",
        "        self.inplanes = 64\n",
        "               \n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        \n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        \n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        \n",
        "        self.layer1 = self._make_layer(block, 64, layers[0]'''3''')\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1]'''4''', stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2]'''6''', stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3]'''3''', stride=2)\n",
        "        \n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "    \n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        \n",
        "        downsample = None\n",
        "        \n",
        "        if stride != 1 or self.inplanes != planes * block.expansion: \n",
        "            \n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride), #conv1x1(256, 512, 2)\n",
        "                nn.BatchNorm2d(planes * block.expansion), #batchnrom2d(512)\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        \n",
        "        self.inplanes = planes * block.expansion #self.inplanes = 128 * 4\n",
        "        \n",
        "        for _ in range(1, blocks): \n",
        "            layers.append(block(self.inplanes, planes)) # * 3\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "    \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ke5FnGSInXzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet18(pretrained=False, **kwargs):\n",
        "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs) #=> 2*(2+2+2+2) +1(conv1) +1(fc)  = 16 +2 =resnet 18\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LOnX3KnnXxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet50(pretrained=False, **kwargs):\n",
        "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs) #=> 3*(3+4+6+3) +(conv1) +1(fc) = 48 +2 = 50\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIdO744NnNt2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet152(pretrained=False, **kwargs):\n",
        "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs) # 3*(3+8+36+3) +2 = 150+2 = resnet152    \n",
        "    return mode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3DffO7ZnNqZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision.models.resnet as resnet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIupHzImoEYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res = resnet.resnet50()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCRIrrJcoEdS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KcKv7SepaVit"
      },
      "source": [
        "# 7. NLP (Natural Language Processing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj2hlHdCgBED",
        "colab_type": "text"
      },
      "source": [
        "### NLP 관련해서 읽을 자료들\n",
        "- [DL, NLP, Representation](https://dgkim5360.tistory.com/entry/deep-learning-nlp-and-representations-kr)\n",
        "- [쉽게 씌여진 word2vec](https://dreamgonfly.github.io/machine/learning,/natural/language/processing/2017/08/16/word2vec_explained.html)\n",
        "- [word2vect 관련 이론 정리](https://shuuki4.wordpress.com/2016/01/27/word2vec-%EA%B4%80%EB%A0%A8-%EC%9D%B4%EB%A1%A0-%EC%A0%95%EB%A6%AC/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPmMl0PFahTU",
        "colab_type": "text"
      },
      "source": [
        "## 0. KoNLPy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VwvBT4oLaVi2"
      },
      "source": [
        "- [KoNLPy 홈페이지](https://konlpy-ko.readthedocs.io/ko/v0.5.1/#)\n",
        "- [KoNLPy 설치방법](https://konlpy-ko.readthedocs.io/ko/v0.5.1/install/)\n",
        "- [한국어 품사 태그 비교표](https://docs.google.com/spreadsheets/d1OGAjUvalBuX-oZvZ_-9tEfYD2gQe7hTGsgUpiiBSXI8/edit#gid=0)\n",
        "- KoNLP는 형태소를 분석하고 품사를 태깅해준다! 형태소를 비롯해 다양한 언어적 속성의 구조를 파악하고 형태소의 뜻과 문맥을 고려하여 그것에 마크업을 해준다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nWLjXpU8aVi4"
      },
      "source": [
        "### 빈용 메소드 정리\n",
        "\n",
        "### KoNLPy의 내장된 분석기 종류\n",
        "1. Hannanum\n",
        "2. Kkma\n",
        "3. Okt (Twitter)\n",
        "4. Komoran\n",
        "5. Mecab # 쓰려면 따로 다운 받아야 함\n",
        "- 분석기마다 로딩시간과 실행시간이 다르다.\n",
        "- 동일한 문장이라도 분석기에 따라 품사 태깅하는 결과가 다르다.\n",
        "- KoNLPy의 분석기 중에서 사용할 것을 불러와서 인스턴스로 만들어준다. (인스턴스: 클래스의 정의를 통해 만들어진 객체)\n",
        "\n",
        "\n",
        "## 분석기를 선택하기!\n",
        "- **konlpy .tag .Kkma ( )**\n",
        "- **konlpy .tag .Okt ( )**\n",
        "- **konlpy .tag .Hannanum ( )**\n",
        "- **konlpy .tag .Komoran ( )**\n",
        "\n",
        "\n",
        "- **.morphs ( phrase )**: phrase의 형태소를 분석하기\n",
        "- **.pos ( phrase , norm = False , stem = False )**: phrase의 단어를 품사에 태깅하기\n",
        "  - norm: True로 설정하면 오류나 실수를 수정한다. e.g 사릉해 -> 사랑해\n",
        "  - stem: True로 설정하면 어근화시켜서 단어의 기본형으로 변환한다. e.g 되나요 -> 되다\n",
        "- **.nouns( phrase )**: phrase의 형태소 중에서 명사만 추출\n",
        "- **.phrases ( phrase )**: phrase에서 phrase를 추출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kdSR8xI5aVi7",
        "colab": {}
      },
      "source": [
        "import konlpy\n",
        "import collections\n",
        "from collections import Counter\n",
        "# 헌법 문서를 불러옴\n",
        "from konlpy.corpus import kolaw\n",
        "from konlpy.corpus import kobill"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BImFstcWaVjD",
        "colab": {}
      },
      "source": [
        "# konlpy 예제\n",
        "## 예제1\n",
        "# 우선 분석기를 인스턴스로 만들어주기\n",
        "kkma=konlpy.tag.Kkma()\n",
        "okt=konlpy.tag.Okt()\n",
        "han=konlpy.tag.Hannanum()\n",
        "komo = konlpy.tag.Komoran()\n",
        "\n",
        "# 해봅시다\n",
        "kolaw.fileids()\n",
        "\n",
        "korealaw = kolaw.open('constitution.txt').read()\n",
        "korealaw\n",
        "\n",
        "# 품사 태깅\n",
        "pos_korealaw = okt.pos(korealaw)\n",
        "pos_korealaw\n",
        "\n",
        "#품사가 명사인 단어들만 추리기\n",
        "nouns=[]\n",
        "\n",
        "for each_tuple in pos_korealaw:\n",
        "        if each_tuple[1]==\"Noun\":\n",
        "            nouns.append(each_tuple[0])\n",
        "\n",
        "#명사 빈도수 확인하기\n",
        "import collections\n",
        "from collections import Counter\n",
        "Counter(nouns).most_common(100)        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZEoizR0ApBl",
        "colab_type": "text"
      },
      "source": [
        "## 0. Customized-KoNLPy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jVUVe8OBDOE",
        "colab_type": "text"
      },
      "source": [
        "- KoNLPy에서 사용자가 dictionary를 추가할 수 있도록 만든 NLP 패키지!\n",
        "- [출처](https://inspiringpeople.github.io/data%20analysis/ckonlpy/)\n",
        "- 단어 선택/필터/치환/결합 목록을 파일에 저장하고 로딩해서 실행하기\n",
        "  - stopwords : 해당 단어 필터\n",
        "  - passwords : 해당 단어만 선택\n",
        "  - passtags : 해당 품사만 선택\n",
        "  - replace : 해당 단어 set 치환\n",
        "  - ngram : 해당 복합 단어 set을 한 단어로 결합"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz0M6Q6GBShN",
        "colab_type": "text"
      },
      "source": [
        "### 빈용 메소드 모음\n",
        "- **ckonlpy .tag .Twitter ( )**: Okt(twitter) 형태소 분석기 꺼내기!\n",
        "- **ckonlpy .tag .Twitter .add_dictionary()** : 직접 단어를 등록하는 방법!\n",
        "- **ckonlpy .tag .Twitter .pos(텍스트, norm=True, stem=True)** : 형태소 분석하기\n",
        "- **ckonlpy .tag .Postprocessor ( )** : 전처리 해주기\n",
        "- **ckonlpy .utils .load_wordset** : txt 파일 불러오기\n",
        "- **ckonlpy .utils .load_replace_wordpair** : 파일 로딩해서 전처리 할 때 사용\n",
        "- **ckonlpy .utils .load_ngram** : 파일 로딩해서 전처리 할 때 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GTlV0Z4ArD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ckonlpy\n",
        "from ckonlpy.tag import Twitter\n",
        "twitter.pos(sen, norm=True, stem=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTNEDdy2D2-2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 파일을 로딩해서 전처리\n",
        "from ckonlpy.utils import load_wordset\n",
        "passwords = load_wordset('./passwords.txt')\n",
        "stopwords = load_wordset('./stopwords.txt')\n",
        "\n",
        "from ckonlpy.utils import load_replace_wordpair\n",
        "replace = load_replace_wordpair('./replacewords.txt')\n",
        "\n",
        "from ckonlpy.utils import load_ngram\n",
        "ngrams = load_ngram('./ngrams.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaaNZpU8ArIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Postprocessor 기본 설정\n",
        "from ckonlpy.tag import Postprocessor\n",
        "\n",
        "postprocessor = Postprocessor(\n",
        "    base_tagger = twitter, # base tagger\n",
        "    stopwords = stopwords, # 해당 단어 필터\n",
        "    passwords = passwords, # 해당 단어만 선택\n",
        "    passtags = passtags, # 해당 품사만 선택\n",
        "    replace = replace, # 해당 단어 set 치환\n",
        "    ngrams = ngrams # 해당 복합 단어 set을 한 단어로 결합\n",
        ")\n",
        "\n",
        "print(passwords)\n",
        "print(stopwrods)\n",
        "print(replace)\n",
        "print(ngrams)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_pINzvYArMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Postprocessor 예시\n",
        "stopwords = {'는'}\n",
        "\n",
        "passwords = {'아이오아이', ('정말', 'Noun')}\n",
        "\n",
        "passtags = {'Noun'}\n",
        "\n",
        "replace = {\n",
        "    '아이오아이': '아이돌',\n",
        "    ('이뻐', 'Adjective'): '예쁘다'\n",
        "}\n",
        "\n",
        "ngrams = [(('미스', '함무라비'), 'Noun'), (('바람', '의', '나라'), 'Game')]\n",
        "\n",
        "\n",
        "\n",
        "postprocessor = Postprocessor(\n",
        "    base_tagger = twitter, # base tagger\n",
        "    #stopwords = stopwords, # 해당 단어 필터\n",
        "    #passwords = passwords, # 해당 단어만 선택\n",
        "    #passtags = passtags, # 해당 품사만 선택\n",
        "    #replace = replace, # 해당 단어 set 치환\n",
        "    ngrams = ngrams # 해당 복합 단어 set을 한 단어로 결합\n",
        ")\n",
        "\n",
        "sent = '바람의 나라는 게임이름 입니다'\n",
        "print('before : %s\\n' % twitter.pos(sent))\n",
        "print('after  : %s' % postprocessor.pos(sent))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "A8pURKEJPNKh"
      },
      "source": [
        "# 8. RNN (Recurrent Neural Network)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bFmKaMvTPNKk"
      },
      "source": [
        "### RNN이란?\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
        "> 중요특징\n",
        "- 순서가 있는 데이터에서 의미를 찾아내기 위해 고안된 모델\n",
        "- RNN은 음성인식, 기계번역, 목소리로 성별 분류 등에 사용함\n",
        "\n",
        "> 기타특징\n",
        "- RNN은 모든 cell이 parameter를 공유한다.\n",
        "- 같은 순환 내에서 각 Hidden Layer의 함수 및 parameter들은 같은 값을 공유하며, 이들과의 연산으로 나온 각 step 별 Hidden state를 계산해줌\n",
        "- RNN은 Sequence가 길어질수록 parameter들이 업데이트 되며 Vanishing Gradient / Exploding Gradient 문제 발생 \n",
        "\n",
        "> Input data의 형태\n",
        "  - batch_first=True: (Batch Size, Sequence Length, Input Dimension)\n",
        "  - batch_first=False: (Sequence Length, Batch Size, Input Dimension)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L_TaEX40PNKm"
      },
      "source": [
        "## RNN 과정 살펴보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "U1_kqxfUPNKo"
      },
      "source": [
        "> Step.1\n",
        "- device에서 'cuda' 설정을 해준다.\n",
        "- GPU를 쓰기 위함\n",
        "\n",
        "> Step.2\n",
        "- Data를 setting 해준다.\n",
        "\n",
        "> Step.3  \n",
        "- 1) 모델을 설정해준다.\n",
        "- 2) Loss function, Optimizer를 설정해준다.\n",
        "\n",
        "> Step.4  \n",
        "- Train 해주자아아아ㅏㅏㅏ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Owxe5wyaPNKq"
      },
      "source": [
        "## 하나하나 뜯어봅시다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eRIlvUjePNKs"
      },
      "source": [
        "### Step.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "txY-6mAgPNKv"
      },
      "source": [
        "---\n",
        "- device에서 'cuda' 설정을 해준다.\n",
        "- GPU를 쓰기 위함\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sm_Rz9i5PNKy"
      },
      "source": [
        "GPU는 써야 성능이 오지게 빨라지겠쥬?\n",
        "```python\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "```\n",
        "요로로콤 설정하시면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vKgukolaPNK0"
      },
      "source": [
        "### Step.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dSkRbp7GPNK2"
      },
      "source": [
        "---\n",
        "- Data를 setting 해준다.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yFtqP7lXPNK7"
      },
      "source": [
        "- 데이터를 dictionary 형태로 바꿔준다!!\n",
        "- input data와 hidden data를 설정해준다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-9NMU3GOPNK-"
      },
      "source": [
        "### Step.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ohOA9AB0PNLB"
      },
      "source": [
        "---\n",
        "- 1) 모델을 설정해준다.\n",
        "- 2) Loss function, Optimizer를 설정해준다.\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nX9mZ0p_PNLD"
      },
      "source": [
        "1) RNN model  \n",
        " - input dimension과 hidden dimension 그리고 layer를 설정해준다.\n",
        " - fully connected layer도 설정해준다.\n",
        "\n",
        "2) Loss function, Optimizer\n",
        " - Loss function으로는 보통 Cross entropy, Optimizer로는 Adam을 사용한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MIGdnIpnPNLF"
      },
      "source": [
        "### Step.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QKVHEY68PNLH"
      },
      "source": [
        "---\n",
        "- Train 해주자아아아ㅏㅏㅏ.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v5qTO-irPNLJ"
      },
      "source": [
        "- input data를 넣어서 RNN network를 통과시킨다.\n",
        "- loss를 구하고 backpropagation으로 gradient를 구한 뒤 optimizer로 parameter를 업데이트해준다!\n",
        "- 바로바로 예측 결과를 출력해 얼마나 모델이 나아지는지 살펴본다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Md_SPwBgPNLq"
      },
      "source": [
        "## 코드 살펴봅시다~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8i5iJALPF4t1",
        "colab_type": "text"
      },
      "source": [
        "### Basic RNN (Hello)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upoVjVQG9Vdq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpkJdPvL9VTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_size = 4\n",
        "hidden_size = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNIRWHYoF9fK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "h = [1, 0, 0, 0]\n",
        "e = [0, 1, 0, 0]\n",
        "l = [0, 0, 1, 0]\n",
        "o = [0, 0, 0, 1]\n",
        "input_data_np = np.array([[h, e, l, l, o], [e, o, l, l, l], [l, l, e, e, l]], dtype=np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SI_N1HyRGAjU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_data = torch.Tensor(input_data_np)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XI1jHP4GAsK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn = torch.nn.RNN(input_size, hidden_size)\n",
        "\n",
        "outputs, _status = rnn(input_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EVntpV-GA5h",
        "colab_type": "code",
        "outputId": "ced5eb66-d387-4ba6-a8ec-29e5626bb87f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "source": [
        "print(outputs) #(batch size, sequence, outputsize)\n",
        "print(outputs.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-0.6577,  0.3839],\n",
            "         [-0.6197,  0.2534],\n",
            "         [-0.1782,  0.4564],\n",
            "         [-0.1782,  0.4564],\n",
            "         [-0.2100,  0.3579]],\n",
            "\n",
            "        [[-0.5103,  0.4424],\n",
            "         [-0.0777,  0.5676],\n",
            "         [-0.0817,  0.3251],\n",
            "         [-0.0817,  0.3251],\n",
            "         [-0.0919,  0.3960]],\n",
            "\n",
            "        [[-0.0325,  0.5174],\n",
            "         [-0.0801,  0.1984],\n",
            "         [-0.5794,  0.1169],\n",
            "         [-0.5794,  0.1169],\n",
            "         [-0.1043,  0.3050]]], grad_fn=<StackBackward>)\n",
            "torch.Size([3, 5, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pEZ84Kz8zfi",
        "colab_type": "text"
      },
      "source": [
        "### Basic RNN (hihello)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPox5OUdG15z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ7h6PFDG2q5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char_set = ['h', 'i', 'e', 'l', 'o']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or9XE7dTG2ay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_size = len(char_set)\n",
        "hidden_size = len(char_set)\n",
        "learning_rate = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyLFI8gAG1xQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = [[0, 1, 0, 2, 3, 3]]\n",
        "x_one_hot = [[[1, 0, 0, 0, 0],\n",
        "              [0, 1, 0, 0, 0],\n",
        "              [1, 0, 0, 0, 0],\n",
        "              [0, 0, 1, 0, 0],\n",
        "              [0, 0, 0, 1, 0],\n",
        "              [0, 0, 0, 1, 0]]]\n",
        "y_data = [[1, 0, 2, 3, 3, 4]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1AyXDAjG1pr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J451fbJFH9zH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True)  # batch_first guarantees the order of output = (B, S, F)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Na0vCI3H-Eo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(rnn.parameters(), learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-SpZdU9H-DL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# start training\n",
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs, _status = rnn(X)\n",
        "    loss = criterion(outputs.view(-1, input_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    result = outputs.data.numpy().argmax(axis=2)\n",
        "    result_str = ''.join([char_set[c] for c in np.squeeze(result)])\n",
        "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DFl_5y8H98K",
        "colab_type": "text"
      },
      "source": [
        "### Basic RNN (if you want you)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPL9nyXMIbx_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample = \" if you want you\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbPiqnkEIcOH",
        "colab_type": "code",
        "outputId": "176d0ee0-1297-47cf-e98a-19b21708a4a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "char_set = list(set(sample))\n",
        "char_dic = {c: i for i, c in enumerate(char_set)}\n",
        "print(char_dic)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'f': 0, 'y': 1, 'o': 2, 'n': 3, 'i': 4, ' ': 5, 'u': 6, 'w': 7, 't': 8, 'a': 9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaV-HCkMIcFa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dic_size = len(char_dic)\n",
        "hidden_size = len(char_dic)\n",
        "learning_rate = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGeUgRajIbwA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data setting\n",
        "sample_idx = [char_dic[c] for c in sample]\n",
        "x_data = [sample_idx[:-1]]\n",
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data]    # one_hot embedding 해주기\n",
        "y_data = [sample_idx[1:]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwkPmBpjIbqA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# transform as torch tensor variable\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAwyd18uIjKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# declare RNN\n",
        "rnn = torch.nn.RNN(dic_size, hidden_size, batch_first=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oOd2cFsIjUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss & optimizer setting\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(rnn.parameters(), learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLl3ClCaIjSh",
        "colab_type": "code",
        "outputId": "fe498219-39f0-4239-d58d-ac07d7ab5a60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        }
      },
      "source": [
        "# start training\n",
        "for i in range(50):\n",
        "    optimizer.zero_grad()\n",
        "    outputs, _status = rnn(X)\n",
        "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
        "    loss.backward()    # backpropagation이 진행되어 gradient를 계산\n",
        "    optimizer.step()\n",
        "\n",
        "    result = outputs.data.numpy().argmax(axis=2)\n",
        "    result_str = ''.join([char_set[c] for c in np.squeeze(result)])\n",
        "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 loss:  2.4004712104797363 prediction:  [[6 1 2 2 2 9 2 2 6 2 5 2 2 3 6]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  niooowoonofooan\n",
            "1 loss:  2.1820285320281982 prediction:  [[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  ooooooooooooooo\n",
            "2 loss:  1.959804892539978 prediction:  [[2 8 8 8 8 8 8 8 8 8 8 8 8 8 8]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  o              \n",
            "3 loss:  1.7191250324249268 prediction:  [[9 8 8 9 8 8 8 9 3 6 8 8 8 8 8]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  w  w   wan     \n",
            "4 loss:  1.5726513862609863 prediction:  [[9 8 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  w  you want you\n",
            "5 loss:  1.4719620943069458 prediction:  [[9 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  wf you want you\n",
            "6 loss:  1.3825010061264038 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "7 loss:  1.3052494525909424 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "8 loss:  1.239822268486023 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "9 loss:  1.176225185394287 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "10 loss:  1.1157336235046387 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "11 loss:  1.0640463829040527 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "12 loss:  1.0167349576950073 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "13 loss:  0.9909738302230835 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "14 loss:  0.9754155278205872 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "15 loss:  0.9633699655532837 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "16 loss:  0.9471043348312378 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "17 loss:  0.9298933148384094 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "18 loss:  0.9150229096412659 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "19 loss:  0.902299702167511 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "20 loss:  0.8941835761070251 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "21 loss:  0.8844041228294373 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "22 loss:  0.8754839301109314 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "23 loss:  0.8699010610580444 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "24 loss:  0.8648304343223572 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "25 loss:  0.8600592017173767 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "26 loss:  0.8560102581977844 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "27 loss:  0.8525880575180054 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "28 loss:  0.8495550751686096 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "29 loss:  0.8469916582107544 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "30 loss:  0.8449980616569519 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "31 loss:  0.8428932428359985 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "32 loss:  0.840404748916626 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "33 loss:  0.8383214473724365 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "34 loss:  0.8366862535476685 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "35 loss:  0.8352165222167969 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "36 loss:  0.8338825106620789 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "37 loss:  0.832729697227478 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "38 loss:  0.8316598534584045 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "39 loss:  0.8304979801177979 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "40 loss:  0.8292334675788879 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "41 loss:  0.8280163407325745 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "42 loss:  0.8269363045692444 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "43 loss:  0.8259714245796204 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "44 loss:  0.8251150250434875 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "45 loss:  0.8244360685348511 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "46 loss:  0.8239482045173645 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "47 loss:  0.823502242565155 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "48 loss:  0.8230105042457581 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n",
            "49 loss:  0.8225291967391968 prediction:  [[1 5 8 7 2 0 8 9 3 6 4 8 7 2 0]] true Y:  [[1, 5, 8, 7, 2, 0, 8, 9, 3, 6, 4, 8, 7, 2, 0]] prediction str:  if you want you\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dBC99juVk-j",
        "colab_type": "text"
      },
      "source": [
        "### Long sequence RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pA6Y9Z9FPNLx",
        "outputId": "316b8d24-5385-464e-8f2d-caf90e12603a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "######################################################################################################################\n",
        "############################################ Step.1 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "######################################################################################################################\n",
        "############################################ Step.2 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
        "            \"collect wood and don't assign them tasks and work, but rather \"\n",
        "            \"teach them to long for the endless immensity of the sea.\")\n",
        "\n",
        "# make dictionary\n",
        "char_set = list(set(sentence))\n",
        "char_dic = {c: i for i, c in enumerate(char_set)}\n",
        "\n",
        "char_dic"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{' ': 5,\n",
              " \"'\": 2,\n",
              " ',': 10,\n",
              " '.': 24,\n",
              " 'a': 1,\n",
              " 'b': 23,\n",
              " 'c': 18,\n",
              " 'd': 16,\n",
              " 'e': 17,\n",
              " 'f': 3,\n",
              " 'g': 6,\n",
              " 'h': 14,\n",
              " 'i': 0,\n",
              " 'k': 9,\n",
              " 'l': 12,\n",
              " 'm': 4,\n",
              " 'n': 21,\n",
              " 'o': 13,\n",
              " 'p': 15,\n",
              " 'r': 11,\n",
              " 's': 7,\n",
              " 't': 20,\n",
              " 'u': 19,\n",
              " 'w': 22,\n",
              " 'y': 8}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDTBlhamUaKY",
        "colab_type": "code",
        "outputId": "b86c5731-ce96-471e-e2f4-83b9cab63eda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# hyper parameters\n",
        "dic_size = len(char_dic)\n",
        "hidden_size = len(char_dic)\n",
        "sequence_length = 10  # Any arbitrary number\n",
        "learning_rate = 0.1\n",
        "\n",
        "# data setting\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(0, len(sentence) - sequence_length):\n",
        "    x_str = sentence[i:i + sequence_length]\n",
        "    y_str = sentence[i + 1: i + sequence_length + 1]\n",
        "    print(i, x_str, '->', y_str)\n",
        "\n",
        "    x_data.append([char_dic[c] for c in x_str])  # x str to index\n",
        "    y_data.append([char_dic[c] for c in y_str])  # y str to index\n",
        "\n",
        "# one_hot_vector로 만들어주기\n",
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 if you wan -> f you want\n",
            "1 f you want ->  you want \n",
            "2  you want  -> you want t\n",
            "3 you want t -> ou want to\n",
            "4 ou want to -> u want to \n",
            "5 u want to  ->  want to b\n",
            "6  want to b -> want to bu\n",
            "7 want to bu -> ant to bui\n",
            "8 ant to bui -> nt to buil\n",
            "9 nt to buil -> t to build\n",
            "10 t to build ->  to build \n",
            "11  to build  -> to build a\n",
            "12 to build a -> o build a \n",
            "13 o build a  ->  build a s\n",
            "14  build a s -> build a sh\n",
            "15 build a sh -> uild a shi\n",
            "16 uild a shi -> ild a ship\n",
            "17 ild a ship -> ld a ship,\n",
            "18 ld a ship, -> d a ship, \n",
            "19 d a ship,  ->  a ship, d\n",
            "20  a ship, d -> a ship, do\n",
            "21 a ship, do ->  ship, don\n",
            "22  ship, don -> ship, don'\n",
            "23 ship, don' -> hip, don't\n",
            "24 hip, don't -> ip, don't \n",
            "25 ip, don't  -> p, don't d\n",
            "26 p, don't d -> , don't dr\n",
            "27 , don't dr ->  don't dru\n",
            "28  don't dru -> don't drum\n",
            "29 don't drum -> on't drum \n",
            "30 on't drum  -> n't drum u\n",
            "31 n't drum u -> 't drum up\n",
            "32 't drum up -> t drum up \n",
            "33 t drum up  ->  drum up p\n",
            "34  drum up p -> drum up pe\n",
            "35 drum up pe -> rum up peo\n",
            "36 rum up peo -> um up peop\n",
            "37 um up peop -> m up peopl\n",
            "38 m up peopl ->  up people\n",
            "39  up people -> up people \n",
            "40 up people  -> p people t\n",
            "41 p people t ->  people to\n",
            "42  people to -> people tog\n",
            "43 people tog -> eople toge\n",
            "44 eople toge -> ople toget\n",
            "45 ople toget -> ple togeth\n",
            "46 ple togeth -> le togethe\n",
            "47 le togethe -> e together\n",
            "48 e together ->  together \n",
            "49  together  -> together t\n",
            "50 together t -> ogether to\n",
            "51 ogether to -> gether to \n",
            "52 gether to  -> ether to c\n",
            "53 ether to c -> ther to co\n",
            "54 ther to co -> her to col\n",
            "55 her to col -> er to coll\n",
            "56 er to coll -> r to colle\n",
            "57 r to colle ->  to collec\n",
            "58  to collec -> to collect\n",
            "59 to collect -> o collect \n",
            "60 o collect  ->  collect w\n",
            "61  collect w -> collect wo\n",
            "62 collect wo -> ollect woo\n",
            "63 ollect woo -> llect wood\n",
            "64 llect wood -> lect wood \n",
            "65 lect wood  -> ect wood a\n",
            "66 ect wood a -> ct wood an\n",
            "67 ct wood an -> t wood and\n",
            "68 t wood and ->  wood and \n",
            "69  wood and  -> wood and d\n",
            "70 wood and d -> ood and do\n",
            "71 ood and do -> od and don\n",
            "72 od and don -> d and don'\n",
            "73 d and don' ->  and don't\n",
            "74  and don't -> and don't \n",
            "75 and don't  -> nd don't a\n",
            "76 nd don't a -> d don't as\n",
            "77 d don't as ->  don't ass\n",
            "78  don't ass -> don't assi\n",
            "79 don't assi -> on't assig\n",
            "80 on't assig -> n't assign\n",
            "81 n't assign -> 't assign \n",
            "82 't assign  -> t assign t\n",
            "83 t assign t ->  assign th\n",
            "84  assign th -> assign the\n",
            "85 assign the -> ssign them\n",
            "86 ssign them -> sign them \n",
            "87 sign them  -> ign them t\n",
            "88 ign them t -> gn them ta\n",
            "89 gn them ta -> n them tas\n",
            "90 n them tas ->  them task\n",
            "91  them task -> them tasks\n",
            "92 them tasks -> hem tasks \n",
            "93 hem tasks  -> em tasks a\n",
            "94 em tasks a -> m tasks an\n",
            "95 m tasks an ->  tasks and\n",
            "96  tasks and -> tasks and \n",
            "97 tasks and  -> asks and w\n",
            "98 asks and w -> sks and wo\n",
            "99 sks and wo -> ks and wor\n",
            "100 ks and wor -> s and work\n",
            "101 s and work ->  and work,\n",
            "102  and work, -> and work, \n",
            "103 and work,  -> nd work, b\n",
            "104 nd work, b -> d work, bu\n",
            "105 d work, bu ->  work, but\n",
            "106  work, but -> work, but \n",
            "107 work, but  -> ork, but r\n",
            "108 ork, but r -> rk, but ra\n",
            "109 rk, but ra -> k, but rat\n",
            "110 k, but rat -> , but rath\n",
            "111 , but rath ->  but rathe\n",
            "112  but rathe -> but rather\n",
            "113 but rather -> ut rather \n",
            "114 ut rather  -> t rather t\n",
            "115 t rather t ->  rather te\n",
            "116  rather te -> rather tea\n",
            "117 rather tea -> ather teac\n",
            "118 ather teac -> ther teach\n",
            "119 ther teach -> her teach \n",
            "120 her teach  -> er teach t\n",
            "121 er teach t -> r teach th\n",
            "122 r teach th ->  teach the\n",
            "123  teach the -> teach them\n",
            "124 teach them -> each them \n",
            "125 each them  -> ach them t\n",
            "126 ach them t -> ch them to\n",
            "127 ch them to -> h them to \n",
            "128 h them to  ->  them to l\n",
            "129  them to l -> them to lo\n",
            "130 them to lo -> hem to lon\n",
            "131 hem to lon -> em to long\n",
            "132 em to long -> m to long \n",
            "133 m to long  ->  to long f\n",
            "134  to long f -> to long fo\n",
            "135 to long fo -> o long for\n",
            "136 o long for ->  long for \n",
            "137  long for  -> long for t\n",
            "138 long for t -> ong for th\n",
            "139 ong for th -> ng for the\n",
            "140 ng for the -> g for the \n",
            "141 g for the  ->  for the e\n",
            "142  for the e -> for the en\n",
            "143 for the en -> or the end\n",
            "144 or the end -> r the endl\n",
            "145 r the endl ->  the endle\n",
            "146  the endle -> the endles\n",
            "147 the endles -> he endless\n",
            "148 he endless -> e endless \n",
            "149 e endless  ->  endless i\n",
            "150  endless i -> endless im\n",
            "151 endless im -> ndless imm\n",
            "152 ndless imm -> dless imme\n",
            "153 dless imme -> less immen\n",
            "154 less immen -> ess immens\n",
            "155 ess immens -> ss immensi\n",
            "156 ss immensi -> s immensit\n",
            "157 s immensit ->  immensity\n",
            "158  immensity -> immensity \n",
            "159 immensity  -> mmensity o\n",
            "160 mmensity o -> mensity of\n",
            "161 mensity of -> ensity of \n",
            "162 ensity of  -> nsity of t\n",
            "163 nsity of t -> sity of th\n",
            "164 sity of th -> ity of the\n",
            "165 ity of the -> ty of the \n",
            "166 ty of the  -> y of the s\n",
            "167 y of the s ->  of the se\n",
            "168  of the se -> of the sea\n",
            "169 of the sea -> f the sea.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRPmcAarUaWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# transform as torch tensor variable\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUPsFqsuUabm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################################################################################################\n",
        "############################################ Step.3 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# declare RNN + FC\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layers):\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _status = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net(dic_size, hidden_size, 2)\n",
        "\n",
        "# loss & optimizer setting\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5dzRTCeUpf7",
        "colab_type": "code",
        "outputId": "01a73452-f734-4219-e181-0ae237ec6816",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "######################################################################################################################\n",
        "############################################ Step.4 ###################################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# start training\n",
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X)\n",
        "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    results = outputs.argmax(dim=2)\n",
        "    predict_str = \"\"\n",
        "    for j, result in enumerate(results):\n",
        "        # print(i, j, ''.join([char_set[t] for t in result]), loss.item())\n",
        "        if j == 0:\n",
        "            predict_str += ''.join([char_set[t] for t in result])\n",
        "        else:\n",
        "            predict_str += char_set[result[-1]]\n",
        "\n",
        "    print(predict_str)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lsfoocslllsffolofossfffslslolfflssflolcocslllolsssfollsololfolososssslooolffflffclssfoollsllsololslllllfllfooolofofslolsololfllollsolslsolooslfcoolfolllllssslfocoolslsoloslsollsll\n",
            " o ooo ok omoomoomok omomo omoo oooo ooookooo ooo ooko oo ooo omoo  ooo omoo  moo oooo  oo oomo oom  omo omoo ooooo oom mokoookomoomo ooo oo o oo oomouokomo ooo  o omo oom omo om \n",
            "   o o o d i  oio   i   ooooi   ioo       i   i  d      o i   i o  i i   oo   io  i  o  i       io  i o   io oi  o  oo o  oo oii o    i  o  o i   io  i   i ioio o o i  o  io o    \n",
            "  kl  kl  k   s                 s             t      ks  s    s  k   s       kt   s         s  st  s   s     ss  s   s         t  s          k   kt   s   s   s  s   s      s      \n",
            "t t t t t tt t t tm t  t  t t t ttt t tt   tttt t tt t t tt ptt tttt  t    t  t t t t tmtt    ttt t t  t t  t t t tt  t tt t stt tttt tt tt  t tp t t t t tt  t t t tttt t t tmt t \n",
            "n t n t t tt ttt ttttttt tt t t ttt ttttt ttttttttnt ttt ttnttttttttttt t  t tt ttt t ttttt tttttptntttnnttnt t t tt  t tt tnttttttttttt tttdtetttt t t ttttttt t t tttttt t ttt tt\n",
            "n  on                                                      e       s                                a      e         t                    y    y               o       l           \n",
            "en o t o t to  eo t te opo o o t o o o   t t    n oit   n te  e    te on ttetoeo t o o t   t t      teto o o t o o tto t nt  n      o   n o t  o l o o o t o o o t t  et  t o e oio\n",
            "e  o t tetoelet mopolo o o   oetooeo teooloeo oom oelooom tooloo oomoel o molo oetomom t o tomo oo  roto t oot t tem  otop oom oo oel oomtootopo o mel o te  t o lolooo oototeo o t\n",
            "tstoa  o   t  lt lo     aoa  oa    o  goo      o   m   g   o  oe o  o o l  osoeoa       go  o   ug    to  goa    oa g      to   o ow  oo  oe  g a t g     g   goa    oom    og     \n",
            "trtot ton teot    totod tod ton t   s t o  t   o   a o o  to  o to   to   tos tot t   s tos o  doo  totod dot t tot t o o s o  oodo od o to   to  t t dos o s tot     ot o to  d   \n",
            "t.tot te s tm to he tmd too ton tot    todho   o       o  to to th t eon  t s tms t t s thntoe d o  t dos dhn t tot t   t   o  todh  d o  o   om  t e t   o   trt     mt   to  t  t\n",
            "t.tor ttn  tottoehe t d eot tor t to  eeodte   ent     o  theto t  tttor  tod tor t t d tmn ee d o  t dod don t tor t d t   o  totot d o tt   tor t e t d en  totl   ape    h  d   \n",
            "t.tor t n ttoetoe t d d dod doe t to  ttod e   endoe t e  t ett  t   toe ttod tor t tod tnd ee d e  t dod dor t tor t d tnd o  t dor d e tt  ttoe t e tod and tor    ent e toe doe \n",
            "t.tor ton ttoebt  e d d dod doe t to   ee le   e doe t en toete  t   to  ttnd toe t tod tnd e  d e  t dod tor t toe t d en te 't doe d e tt  taoe toe dod a d torle tentpe toe doe \n",
            "t.tor ton tto bo le d d dod doe't to   to o   te dor t en toebe le l won ttns wor't ao  wndt e  te  d dod wor t toe e  len to 't doe  te te   aoe toe do  e d wodle ttntoe toe doe \n",
            "ontor tonsewonbo ee d d do, don't to l to o   le ton lten toebe le t won ttns wor't e s wndthe  th  d dos won't toe e  len to 'w doe  to be   eon'toend   ens wodon ltntoe toe do  \n",
            "ontor tons wonbo ee t t to, don't ao   toeee  le ton   en toeke le t ton ttns won't t s wnsthe  ths d dns won t the t   en toa t toe  toeke   aon the t s e s wogon' tyeoe the d   \n",
            "ontor tonshaontusee t t to, don't ao h toeeen le thn   en toeko le t ton ttns tonkt tns ensthe  tos d tns ton t the e she  to  t the  toele   aon the t s e s iodpn' tyehe the t   \n",
            "ontorltonshao tusee t dhtod don't ao e theeen le th l hen toeco lo thtonl tns don't ans instheoetos t dns don t thehenshen to  t them toecorlhton the dh he stioepn' aytoa the d   \n",
            "pytonltonthao bus e t dheoa don't ao h to eenece to l hen tonlonle t tonl tns don't ans ansthe etos t t s don t thd endhen to ct toem torlo lhton the do he s iorpn' aydof toe d   \n",
            "nytonltont donbus e t dhdma don't aoro todeeoele to l her to lonle t donl tnd don't t d fn toe etos t t d don t dhr tnd en to ct toe  toglo l don the dod e s iyron' tydof the d d \n",
            "pytorlte t do buspe tndhrme don't aorp todeeoele to l her toelonle t ton' and don't a dirn toe etos s tnd donkt dhd tndher to lt the  to lo l don the dhd e s iyrpn' iydof themd  c\n",
            "pytonltent wo busee a dhrmw don't aorl to peoele to   her toulonle t ton' ans won't a siwn toe etos s and workt thd enshem to lt ahe  to lo l won the dhd e   imnpn' iy ef themd  c\n",
            "pyton test wo bus e t dhdpw don't aorl to people to   ler to lople t ton' tns won't a siwn tee etos shans workt wus enshem tosct ahem to torl won the dos e s imner' iy of the dhac\n",
            "pyworlton' wo busee a dhdp, don't aorl tr people to lther to conlect ton' ans don't ansitnsther toscs ans dork, wus enshem to ct ahem toalonc wonkthe  os ens imnen' iy of the dsic\n",
            "pyworl,on' wo buiee a dhep, don't aoul toeeeople to lthe  to conlent tond and don't a sirnsther toscs and dork, dud t dher to ct ahe  to lonc won the thd e s immen' ty of the tsic\n",
            "pyworltont wo buiee a dhip, don't aorr tp eeople to ether to lonlent dond and don't a signsther toskd and dork, dus tndher to kt ahe  to long ton the erd e s immen' iy of the dsic\n",
            "pywo ltont fo bui e d dhim, don't aoum to people to lther to tonlect wond and don't assignsther tosksiand dork, wus tndher to kh them to bong for them nd ens immen' iy of toemesis\n",
            "pybooltont do buile a dhip, don't aoum tp peophe to lther to collect aond and dor't a sign them tosks and dork, wui rasher to kh them to cong for the end e siimmpn' ty of the esik\n",
            "pybooltont ao built a dhip, dor't aoum tp people to ether to collect wond and don't a sign the  tosks and dork, dud rasher to kh the  to cong eor the end e s immpn' ty of the ehic\n",
            "pybooltant wo buald a ship, don't aoum tp people to lther to collect wond and won't a sign them toskt and work, wum rasher to ch them to bong tor the  nd e s immpnd ty of the dhic\n",
            "pywooetant to buiie a ship, don't aoum tp people to ether to collect woad and won't a sign them tosks and wors, wud radher to ch them to ceng wor the  nd e s immpns ty of the dhic\n",
            "py,ooetant wo lui d a ship, don't aoum tp people to ether to collect wood and won't ansign them tosks and work, wud radher to ch them to long fon the end e s immens ty of the ehic\n",
            "py,oo tant ao build a dhip, don't aoum tp people to ether to lollect wood and don't assign them tosks and dork, bud radher to ch them to long for the end e s immens ty of the enak\n",
            "tm,oo tant to build a dhip, don't aoum tpepeople to ether te collect woad and don't assign them tosks and dork, bud radher th kh them to long for the dnd ess immens ty of the dnak\n",
            "tm,ooetant to buile a dhip, don't aoum tp people to ether te collect wood and won't assign them tosks and work, but rather te ch them to long for the endless immensity of the enis\n",
            "tm,ooetant to luild a dhip, don't aoup up people to ether te collect wood and won't assign them tosks and work, but rather teach them to long for the endless immensity of the ehac\n",
            "tm,oo want to buiid a ship, don't aoum tp people to ether to collect wood and won't assign them tosks and work, but rather toach them to long for the endless immensity of the ehac\n",
            "tn,oo want to build asship, don't aoum up people to ether to collect wood and don't ansign them tosks and work, but rather toach them to long for the endless immensity of the ehac\n",
            "tn,oo want to build a ship, don't aoum up people together to collect wood and don't assign them tosks and dork, but rather toach them to long for the endless immensity of the dhae\n",
            "tn,oo want to build a ship, don't aoum up people together to collect wood and don't a sign them tosks and dork, but rather toach them to long for the endless immensity of the ehae\n",
            "tn,oo want to build a ship, don't aoum up people together to collect wood and won't a sign them tosks and work, but rather toach them to long for the endless immensity of the dhae\n",
            "tn,oo want to build a ship, don't arum up people together to collect wood and won't a sign them tosks and work, but rather teach them to long for the endless immensity of the drac\n",
            "tn,oo want to build a ship, don't arum up people together te collect wood and won't a sign them tasks and work, but rather teach them ta long for the endless immensity of the dhac\n",
            "tn,oo want to build a ship, don't arum up people together te collect wood and won't assign them tasks and work, but rather teach them ta long for the endless immensity of the shac\n",
            "tnyoo want to build a ship, don't arum up people together te collect wood and won't ansign them tasks and work, but rather teach them ta long for the endless immensity of the srac\n",
            "tnyou want to build a ship, don't arum up people together te collect wood and won't assign them tasks and work, but rather teach them ta long for the endless immensity of the dhac\n",
            "tnyou want to build a ship, don't arum up people together te collect wood and won't assign them tosks and work, but rather teach them to long for the endless immensity of the drac\n",
            "tnyou want to build a ship, don't arum up people together te collect wood and won't assign them tosks and work, but rather teach them to long for the endless immensity of the sra.\n",
            "tiyouewant to build a ship, don't arum up people together te collect wood and won't assign them tosks and work, but rather teach them to long for the endless immensity of the srac\n",
            "tnyou want to build a ship, don't arum up people together te collect wood and won't assign them tosks and work, but rather teach them to long for the endless immensity of the sra.\n",
            "tnyou want to build a ship, don't arum up people together te collect wood and don't assign them tasks and dork, but rather teach them to long for the endless immensity of the sha.\n",
            "tnyouewant to build a ship, don't arum up people together teacollect wood and won't ansign them tasks and work, but rather teach them to long for themendless immensity of the erac\n",
            "tnyou want to build a dhip, don't aoum up people together to collect wood asd don't assign ther tasks and dork, but rather toach them to long for the sndless immensity ef the srai\n",
            "tiyou want to build a ship, don't arui up people together to collect wood ans son't assign ther tosks ans sork, but rather toach them to long for the snd ess immensity of the shac\n",
            "lnyou want to build asship, don't arum up people together to collect eord and won't assign them tosks and work, but rather toach them to bong for the endless immensity of the erad\n",
            "tryou want to build d ship, don't arum up people together te collect wood and won't ans gn them tosks and work, but aather teach them to cong for the endless immensity of the sra.\n",
            "tyyou want to build a ship, don't arum up people together te collect wood and won't ass gn them tosks and work, but rather teach them to long for the endless immengity of the shad\n",
            "ty,ou want to build asship, don't arum up people together to collect wood and won't assign them tasks and dork, but rather toach them ta long for the endless immensity of the shad\n",
            "ty,ou want to build asship, bon't arum up people together to collect wood and don't assign ther tosks and dork, but rather toach them to long for the endless immensity of the elad\n",
            "ty,ou tant to build a ship, don't arum up people together to collect wood and don't assign them tosks and dork, but rather toach them to long for the endless immensity of the elad\n",
            "tysou tant to build a ship, don't arum up people together te collect wood and don't dssign them tosks ind dork, but rather teach them to long for the sndless immensity of the sla.\n",
            "tysou want to build a ship, don't arum up people together te bollect wood and won't assign them tosks ind work, but rather teach them to long for the endless immensity of the sla.\n",
            "pnyou want to build a ship, don't arum up people together te collect wood and won't assign them tosks and work, but rather teach them to long for the endless immensity of the sla.\n",
            "pnyou want to build a ship, don't arum up people together to collect wood and won't assign them tosks and work, but rather teach them to long for the endless immensity of the sla.\n",
            "tnyou want to build a ship, don't arum up people together to collect wood and don't assign them tosks and dork, but rather teach them to long for the endless immensity of the sla.\n",
            "tnyou want to build a ship, don't arum up people together te collect wood and don't assign them tasks and dork, but rather teach them ta long for the endless immensity of the sla.\n",
            "tnyou want to build a ship, don't arum up people together te collect wood and don't assign them tasks and dork, but rather teach them ta long for the endless immensity of the sla.\n",
            "tnyou want to build a ship, don't arum up people together to collect wood and don't assign them tasks and dork, but rather teach them ta long for the endless immensity of the sla.\n",
            "lnyou want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them ta long for the endless immensity of the slan\n",
            "lnyou want to build a ship, don't arum up people together to collect wood and won't assign them tosks and work, but rather teach them to long for the endless immensity of the slan\n",
            "lnyou want to build a ship, don't arum up people together to collect wood and won't assign them tosks and dork, but rather teach them to long for the endless immensity of the sla.\n",
            "l you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and dork, but rather teach them to long for the endless immensity of the slac\n",
            "l you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and dork, but rather teach them ta long for the endless immensity of the sla.\n",
            "l you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them ta long for the endless immensity of the sla.\n",
            "p you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them ta long for the endless immensity of the sla.\n",
            "p you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them ta long for the endless immensity of the sla.\n",
            "p you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sla.\n",
            "p you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them ta long for the endless immensity of the sla.\n",
            "m you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them ta long for the endless immensity of the sla.\n",
            "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sla.\n",
            "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "f you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "f you want to build a ship, don't drum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "f you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "f you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "f you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjH0kB6VZ_ta",
        "colab_type": "text"
      },
      "source": [
        "### Time series data (Many-to-One) (주식 종가 예측)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9dieN51aDuN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzHLa0TBaI2B",
        "colab_type": "code",
        "outputId": "5e5aaf94-f1ab-4ffd-9e5b-ae1c3a56f251",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "torch.manual_seed(0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ff22ad0fa30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E30W1bHnaJnB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# scaling function for input data\n",
        "def minmax_scaler(data):\n",
        "    numerator = data - np.min(data, 0)\n",
        "    denominator = np.max(data, 0) - np.min(data, 0)\n",
        "    return numerator / (denominator + 1e-7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKNgO7xhaJY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make dataset to input\n",
        "def build_dataset(time_series, seq_length):\n",
        "    dataX = []\n",
        "    dataY = []\n",
        "    for i in range(0, len(time_series) - seq_length):\n",
        "        _x = time_series[i:i + seq_length, :]\n",
        "        _y = time_series[i + seq_length, [-1]]  # Next close price\n",
        "        print(_x, \"->\", _y)\n",
        "        dataX.append(_x)\n",
        "        dataY.append(_y)\n",
        "    return np.array(dataX), np.array(dataY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bl9kR2aeaJXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hyper parameters\n",
        "seq_length = 7\n",
        "data_dim = 5\n",
        "hidden_dim = 10\n",
        "output_dim = 1\n",
        "learning_rate = 0.01\n",
        "iterations = 500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZQOTczqaIzV",
        "colab_type": "code",
        "outputId": "c1540638-ae19-4959-b478-9d50e8dca251",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# load data\n",
        "xy = np.loadtxt(\"data/data-02-stock_daily.csv\", delimiter=\",\")\n",
        "xy = xy[::-1]  # reverse order\n",
        "\n",
        "# split train-test set\n",
        "train_size = int(len(xy) * 0.7)\n",
        "train_set = xy[0:train_size]\n",
        "test_set = xy[train_size - seq_length:]\n",
        "\n",
        "# scaling data\n",
        "train_set = minmax_scaler(train_set)\n",
        "test_set = minmax_scaler(test_set)\n",
        "\n",
        "# make train-test dataset to input\n",
        "trainX, trainY = build_dataset(train_set, seq_length)\n",
        "testX, testY = build_dataset(test_set, seq_length)\n",
        "\n",
        "# convert to tensor\n",
        "trainX_tensor = torch.FloatTensor(trainX)\n",
        "trainY_tensor = torch.FloatTensor(trainY)\n",
        "\n",
        "testX_tensor = torch.FloatTensor(testX)\n",
        "testY_tensor = torch.FloatTensor(testY)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2.53065030e-01 2.45070970e-01 2.33983036e-01 4.66075110e-04\n",
            "  2.32039560e-01]\n",
            " [2.29604366e-01 2.39728936e-01 2.54567513e-01 2.98467330e-03\n",
            "  2.37426028e-01]\n",
            " [2.49235510e-01 2.41668371e-01 2.48338489e-01 2.59926504e-04\n",
            "  2.26793794e-01]\n",
            " [2.21013495e-01 2.46602231e-01 2.54710584e-01 0.00000000e+00\n",
            "  2.62668239e-01]\n",
            " [3.63433786e-01 3.70389871e-01 2.67168847e-01 1.24764722e-02\n",
            "  2.62105010e-01]\n",
            " [2.59447633e-01 3.10673724e-01 2.74113889e-01 4.56323384e-01\n",
            "  2.71751265e-01]\n",
            " [2.76008150e-01 2.78314566e-01 1.98470380e-01 5.70171193e-01\n",
            "  1.78104644e-01]] -> [0.16053716]\n",
            "[[2.29604366e-01 2.39728936e-01 2.54567513e-01 2.98467330e-03\n",
            "  2.37426028e-01]\n",
            " [2.49235510e-01 2.41668371e-01 2.48338489e-01 2.59926504e-04\n",
            "  2.26793794e-01]\n",
            " [2.21013495e-01 2.46602231e-01 2.54710584e-01 0.00000000e+00\n",
            "  2.62668239e-01]\n",
            " [3.63433786e-01 3.70389871e-01 2.67168847e-01 1.24764722e-02\n",
            "  2.62105010e-01]\n",
            " [2.59447633e-01 3.10673724e-01 2.74113889e-01 4.56323384e-01\n",
            "  2.71751265e-01]\n",
            " [2.76008150e-01 2.78314566e-01 1.98470380e-01 5.70171193e-01\n",
            "  1.78104644e-01]\n",
            " [1.59015228e-01 1.78651664e-01 1.41728657e-01 3.93806579e-01\n",
            "  1.60537160e-01]] -> [0.21950626]\n",
            "[[2.49235510e-01 2.41668371e-01 2.48338489e-01 2.59926504e-04\n",
            "  2.26793794e-01]\n",
            " [2.21013495e-01 2.46602231e-01 2.54710584e-01 0.00000000e+00\n",
            "  2.62668239e-01]\n",
            " [3.63433786e-01 3.70389871e-01 2.67168847e-01 1.24764722e-02\n",
            "  2.62105010e-01]\n",
            " [2.59447633e-01 3.10673724e-01 2.74113889e-01 4.56323384e-01\n",
            "  2.71751265e-01]\n",
            " [2.76008150e-01 2.78314566e-01 1.98470380e-01 5.70171193e-01\n",
            "  1.78104644e-01]\n",
            " [1.59015228e-01 1.78651664e-01 1.41728657e-01 3.93806579e-01\n",
            "  1.60537160e-01]\n",
            " [1.65432462e-01 2.00836760e-01 1.93494176e-01 2.81733441e-01\n",
            "  2.19506258e-01]] -> [0.25203622]\n",
            "[[0.22101349 0.24660223 0.25471058 0.         0.26266824]\n",
            " [0.36343379 0.37038987 0.26716885 0.01247647 0.26210501]\n",
            " [0.25944763 0.31067372 0.27411389 0.45632338 0.27175127]\n",
            " [0.27600815 0.27831457 0.19847038 0.57017119 0.17810464]\n",
            " [0.15901523 0.17865166 0.14172866 0.39380658 0.16053716]\n",
            " [0.16543246 0.20083676 0.19349418 0.28173344 0.21950626]\n",
            " [0.22415317 0.23612204 0.2340904  0.29783096 0.25203622]] -> [0.17039458]\n",
            "[[0.36343379 0.37038987 0.26716885 0.01247647 0.26210501]\n",
            " [0.25944763 0.31067372 0.27411389 0.45632338 0.27175127]\n",
            " [0.27600815 0.27831457 0.19847038 0.57017119 0.17810464]\n",
            " [0.15901523 0.17865166 0.14172866 0.39380658 0.16053716]\n",
            " [0.16543246 0.20083676 0.19349418 0.28173344 0.21950626]\n",
            " [0.22415317 0.23612204 0.2340904  0.29783096 0.25203622]\n",
            " [0.24271481 0.23486317 0.18737251 0.36110962 0.17039458]] -> [0.1339569]\n",
            "[[0.25944763 0.31067372 0.27411389 0.45632338 0.27175127]\n",
            " [0.27600815 0.27831457 0.19847038 0.57017119 0.17810464]\n",
            " [0.15901523 0.17865166 0.14172866 0.39380658 0.16053716]\n",
            " [0.16543246 0.20083676 0.19349418 0.28173344 0.21950626]\n",
            " [0.22415317 0.23612204 0.2340904  0.29783096 0.25203622]\n",
            " [0.24271481 0.23486317 0.18737251 0.36110962 0.17039458]\n",
            " [0.13075879 0.14979736 0.13950917 0.35107108 0.1339569 ]] -> [0.14071632]\n",
            "[[0.27600815 0.27831457 0.19847038 0.57017119 0.17810464]\n",
            " [0.15901523 0.17865166 0.14172866 0.39380658 0.16053716]\n",
            " [0.16543246 0.20083676 0.19349418 0.28173344 0.21950626]\n",
            " [0.22415317 0.23612204 0.2340904  0.29783096 0.25203622]\n",
            " [0.24271481 0.23486317 0.18737251 0.36110962 0.17039458]\n",
            " [0.13075879 0.14979736 0.13950917 0.35107108 0.1339569 ]\n",
            " [0.15042457 0.16374824 0.15035617 0.23008873 0.14071632]] -> [0.154517]\n",
            "[[0.15901523 0.17865166 0.14172866 0.39380658 0.16053716]\n",
            " [0.16543246 0.20083676 0.19349418 0.28173344 0.21950626]\n",
            " [0.22415317 0.23612204 0.2340904  0.29783096 0.25203622]\n",
            " [0.24271481 0.23486317 0.18737251 0.36110962 0.17039458]\n",
            " [0.13075879 0.14979736 0.13950917 0.35107108 0.1339569 ]\n",
            " [0.15042457 0.16374824 0.15035617 0.23008873 0.14071632]\n",
            " [0.14549092 0.14452321 0.11061926 0.34481491 0.154517  ]] -> [0.22527992]\n",
            "[[0.16543246 0.20083676 0.19349418 0.28173344 0.21950626]\n",
            " [0.22415317 0.23612204 0.2340904  0.29783096 0.25203622]\n",
            " [0.24271481 0.23486317 0.18737251 0.36110962 0.17039458]\n",
            " [0.13075879 0.14979736 0.13950917 0.35107108 0.1339569 ]\n",
            " [0.15042457 0.16374824 0.15035617 0.23008873 0.14071632]\n",
            " [0.14549092 0.14452321 0.11061926 0.34481491 0.154517  ]\n",
            " [0.16681257 0.20764196 0.18773052 0.4378686  0.22527992]] -> [0.15331987]\n",
            "[[0.22415317 0.23612204 0.2340904  0.29783096 0.25203622]\n",
            " [0.24271481 0.23486317 0.18737251 0.36110962 0.17039458]\n",
            " [0.13075879 0.14979736 0.13950917 0.35107108 0.1339569 ]\n",
            " [0.15042457 0.16374824 0.15035617 0.23008873 0.14071632]\n",
            " [0.14549092 0.14452321 0.11061926 0.34481491 0.154517  ]\n",
            " [0.16681257 0.20764196 0.18773052 0.4378686  0.22527992]\n",
            " [0.18685758 0.18212236 0.1560484  0.60962624 0.15331987]] -> [0.12698631]\n",
            "[[0.24271481 0.23486317 0.18737251 0.36110962 0.17039458]\n",
            " [0.13075879 0.14979736 0.13950917 0.35107108 0.1339569 ]\n",
            " [0.15042457 0.16374824 0.15035617 0.23008873 0.14071632]\n",
            " [0.14549092 0.14452321 0.11061926 0.34481491 0.154517  ]\n",
            " [0.16681257 0.20764196 0.18773052 0.4378686  0.22527992]\n",
            " [0.18685758 0.18212236 0.1560484  0.60962624 0.15331987]\n",
            " [0.14300667 0.13856872 0.13617972 0.22934481 0.12698631]] -> [0.14877846]\n",
            "[[0.13075879 0.14979736 0.13950917 0.35107108 0.1339569 ]\n",
            " [0.15042457 0.16374824 0.15035617 0.23008873 0.14071632]\n",
            " [0.14549092 0.14452321 0.11061926 0.34481491 0.154517  ]\n",
            " [0.16681257 0.20764196 0.18773052 0.4378686  0.22527992]\n",
            " [0.18685758 0.18212236 0.1560484  0.60962624 0.15331987]\n",
            " [0.14300667 0.13856872 0.13617972 0.22934481 0.12698631]\n",
            " [0.1172689  0.14037216 0.1430174  0.21129336 0.14877846]] -> [0.12107173]\n",
            "[[0.15042457 0.16374824 0.15035617 0.23008873 0.14071632]\n",
            " [0.14549092 0.14452321 0.11061926 0.34481491 0.154517  ]\n",
            " [0.16681257 0.20764196 0.18773052 0.4378686  0.22527992]\n",
            " [0.18685758 0.18212236 0.1560484  0.60962624 0.15331987]\n",
            " [0.14300667 0.13856872 0.13617972 0.22934481 0.12698631]\n",
            " [0.1172689  0.14037216 0.1430174  0.21129336 0.14877846]\n",
            " [0.13503702 0.12893923 0.13850679 0.18323922 0.12107173]] -> [0.11480508]\n",
            "[[0.14549092 0.14452321 0.11061926 0.34481491 0.154517  ]\n",
            " [0.16681257 0.20764196 0.18773052 0.4378686  0.22527992]\n",
            " [0.18685758 0.18212236 0.1560484  0.60962624 0.15331987]\n",
            " [0.14300667 0.13856872 0.13617972 0.22934481 0.12698631]\n",
            " [0.1172689  0.14037216 0.1430174  0.21129336 0.14877846]\n",
            " [0.13503702 0.12893923 0.13850679 0.18323922 0.12107173]\n",
            " [0.12220255 0.12138537 0.12372168 0.16807385 0.11480508]] -> [0.08319064]\n",
            "[[0.16681257 0.20764196 0.18773052 0.4378686  0.22527992]\n",
            " [0.18685758 0.18212236 0.1560484  0.60962624 0.15331987]\n",
            " [0.14300667 0.13856872 0.13617972 0.22934481 0.12698631]\n",
            " [0.1172689  0.14037216 0.1430174  0.21129336 0.14877846]\n",
            " [0.13503702 0.12893923 0.13850679 0.18323922 0.12107173]\n",
            " [0.12220255 0.12138537 0.12372168 0.16807385 0.11480508]\n",
            " [0.09611997 0.09773711 0.09973633 0.18754145 0.08319064]] -> [0.08660558]\n",
            "[[0.18685758 0.18212236 0.1560484  0.60962624 0.15331987]\n",
            " [0.14300667 0.13856872 0.13617972 0.22934481 0.12698631]\n",
            " [0.1172689  0.14037216 0.1430174  0.21129336 0.14877846]\n",
            " [0.13503702 0.12893923 0.13850679 0.18323922 0.12107173]\n",
            " [0.12220255 0.12138537 0.12372168 0.16807385 0.11480508]\n",
            " [0.09611997 0.09773711 0.09973633 0.18754145 0.08319064]\n",
            " [0.07773084 0.07698102 0.05455784 0.29825222 0.08660558]] -> [0.12374741]\n",
            "[[0.14300667 0.13856872 0.13617972 0.22934481 0.12698631]\n",
            " [0.1172689  0.14037216 0.1430174  0.21129336 0.14877846]\n",
            " [0.13503702 0.12893923 0.13850679 0.18323922 0.12107173]\n",
            " [0.12220255 0.12138537 0.12372168 0.16807385 0.11480508]\n",
            " [0.09611997 0.09773711 0.09973633 0.18754145 0.08319064]\n",
            " [0.07773084 0.07698102 0.05455784 0.29825222 0.08660558]\n",
            " [0.0767648  0.11393376 0.1029582  0.24120283 0.12374741]] -> [0.12008597]\n",
            "[[0.1172689  0.14037216 0.1430174  0.21129336 0.14877846]\n",
            " [0.13503702 0.12893923 0.13850679 0.18323922 0.12107173]\n",
            " [0.12220255 0.12138537 0.12372168 0.16807385 0.11480508]\n",
            " [0.09611997 0.09773711 0.09973633 0.18754145 0.08319064]\n",
            " [0.07773084 0.07698102 0.05455784 0.29825222 0.08660558]\n",
            " [0.0767648  0.11393376 0.1029582  0.24120283 0.12374741]\n",
            " [0.11368095 0.10896575 0.12515372 0.15624272 0.12008597]] -> [0.13659745]\n",
            "[[0.13503702 0.12893923 0.13850679 0.18323922 0.12107173]\n",
            " [0.12220255 0.12138537 0.12372168 0.16807385 0.11480508]\n",
            " [0.09611997 0.09773711 0.09973633 0.18754145 0.08319064]\n",
            " [0.07773084 0.07698102 0.05455784 0.29825222 0.08660558]\n",
            " [0.0767648  0.11393376 0.1029582  0.24120283 0.12374741]\n",
            " [0.11368095 0.10896575 0.12515372 0.15624272 0.12008597]\n",
            " [0.11199043 0.12574072 0.13002235 0.17008156 0.13659745]] -> [0.12455713]\n",
            "[[0.12220255 0.12138537 0.12372168 0.16807385 0.11480508]\n",
            " [0.09611997 0.09773711 0.09973633 0.18754145 0.08319064]\n",
            " [0.07773084 0.07698102 0.05455784 0.29825222 0.08660558]\n",
            " [0.0767648  0.11393376 0.1029582  0.24120283 0.12374741]\n",
            " [0.11368095 0.10896575 0.12515372 0.15624272 0.12008597]\n",
            " [0.11199043 0.12574072 0.13002235 0.17008156 0.13659745]\n",
            " [0.13493355 0.12938155 0.13621566 0.15063189 0.12455713]] -> [0.1241346]\n",
            "[[0.09611997 0.09773711 0.09973633 0.18754145 0.08319064]\n",
            " [0.07773084 0.07698102 0.05455784 0.29825222 0.08660558]\n",
            " [0.0767648  0.11393376 0.1029582  0.24120283 0.12374741]\n",
            " [0.11368095 0.10896575 0.12515372 0.15624272 0.12008597]\n",
            " [0.11199043 0.12574072 0.13002235 0.17008156 0.13659745]\n",
            " [0.13493355 0.12938155 0.13621566 0.15063189 0.12455713]\n",
            " [0.10408962 0.11202828 0.12085782 0.09108183 0.1241346 ]] -> [0.07952921]\n",
            "[[0.07773084 0.07698102 0.05455784 0.29825222 0.08660558]\n",
            " [0.0767648  0.11393376 0.1029582  0.24120283 0.12374741]\n",
            " [0.11368095 0.10896575 0.12515372 0.15624272 0.12008597]\n",
            " [0.11199043 0.12574072 0.13002235 0.17008156 0.13659745]\n",
            " [0.13493355 0.12938155 0.13621566 0.15063189 0.12455713]\n",
            " [0.10408962 0.11202828 0.12085782 0.09108183 0.1241346 ]\n",
            " [0.10550415 0.10491674 0.09844758 0.15067671 0.07952921]] -> [0.0612928]\n",
            "[[0.0767648  0.11393376 0.1029582  0.24120283 0.12374741]\n",
            " [0.11368095 0.10896575 0.12515372 0.15624272 0.12008597]\n",
            " [0.11199043 0.12574072 0.13002235 0.17008156 0.13659745]\n",
            " [0.13493355 0.12938155 0.13621566 0.15063189 0.12455713]\n",
            " [0.10408962 0.11202828 0.12085782 0.09108183 0.1241346 ]\n",
            " [0.10550415 0.10491674 0.09844758 0.15067671 0.07952921]\n",
            " [0.07293507 0.07044801 0.05634778 0.28828538 0.0612928 ]] -> [0.06495413]\n",
            "[[0.11368095 0.10896575 0.12515372 0.15624272 0.12008597]\n",
            " [0.11199043 0.12574072 0.13002235 0.17008156 0.13659745]\n",
            " [0.13493355 0.12938155 0.13621566 0.15063189 0.12455713]\n",
            " [0.10408962 0.11202828 0.12085782 0.09108183 0.1241346 ]\n",
            " [0.10550415 0.10491674 0.09844758 0.15067671 0.07952921]\n",
            " [0.07293507 0.07044801 0.05634778 0.28828538 0.0612928 ]\n",
            " [0.04764591 0.07231955 0.06762455 0.1804607  0.06495413]] -> [0.09216809]\n",
            "[[0.11199043 0.12574072 0.13002235 0.17008156 0.13659745]\n",
            " [0.13493355 0.12938155 0.13621566 0.15063189 0.12455713]\n",
            " [0.10408962 0.11202828 0.12085782 0.09108183 0.1241346 ]\n",
            " [0.10550415 0.10491674 0.09844758 0.15067671 0.07952921]\n",
            " [0.07293507 0.07044801 0.05634778 0.28828538 0.0612928 ]\n",
            " [0.04764591 0.07231955 0.06762455 0.1804607  0.06495413]\n",
            " [0.05554661 0.08140447 0.05956976 0.21793493 0.09216809]] -> [0.13156305]\n",
            "[[0.13493355 0.12938155 0.13621566 0.15063189 0.12455713]\n",
            " [0.10408962 0.11202828 0.12085782 0.09108183 0.1241346 ]\n",
            " [0.10550415 0.10491674 0.09844758 0.15067671 0.07952921]\n",
            " [0.07293507 0.07044801 0.05634778 0.28828538 0.0612928 ]\n",
            " [0.04764591 0.07231955 0.06762455 0.1804607  0.06495413]\n",
            " [0.05554661 0.08140447 0.05956976 0.21793493 0.09216809]\n",
            " [0.09957004 0.11641756 0.11258831 0.17070897 0.13156305]] -> [0.14272319]\n",
            "[[0.10408962 0.11202828 0.12085782 0.09108183 0.1241346 ]\n",
            " [0.10550415 0.10491674 0.09844758 0.15067671 0.07952921]\n",
            " [0.07293507 0.07044801 0.05634778 0.28828538 0.0612928 ]\n",
            " [0.04764591 0.07231955 0.06762455 0.1804607  0.06495413]\n",
            " [0.05554661 0.08140447 0.05956976 0.21793493 0.09216809]\n",
            " [0.09957004 0.11641756 0.11258831 0.17070897 0.13156305]\n",
            " [0.12503183 0.13642499 0.15017717 0.14748588 0.14272319]] -> [0.12005085]\n",
            "[[0.10550415 0.10491674 0.09844758 0.15067671 0.07952921]\n",
            " [0.07293507 0.07044801 0.05634778 0.28828538 0.0612928 ]\n",
            " [0.04764591 0.07231955 0.06762455 0.1804607  0.06495413]\n",
            " [0.05554661 0.08140447 0.05956976 0.21793493 0.09216809]\n",
            " [0.09957004 0.11641756 0.11258831 0.17070897 0.13156305]\n",
            " [0.12503183 0.13642499 0.15017717 0.14748588 0.14272319]\n",
            " [0.13231142 0.12597895 0.13506998 0.10610379 0.12005085]] -> [0.09656857]\n",
            "[[0.07293507 0.07044801 0.05634778 0.28828538 0.0612928 ]\n",
            " [0.04764591 0.07231955 0.06762455 0.1804607  0.06495413]\n",
            " [0.05554661 0.08140447 0.05956976 0.21793493 0.09216809]\n",
            " [0.09957004 0.11641756 0.11258831 0.17070897 0.13156305]\n",
            " [0.12503183 0.13642499 0.15017717 0.14748588 0.14272319]\n",
            " [0.13231142 0.12597895 0.13506998 0.10610379 0.12005085]\n",
            " [0.10712562 0.10171823 0.10689609 0.15204804 0.09656857]] -> [0.09885706]\n",
            "[[0.04764591 0.07231955 0.06762455 0.1804607  0.06495413]\n",
            " [0.05554661 0.08140447 0.05956976 0.21793493 0.09216809]\n",
            " [0.09957004 0.11641756 0.11258831 0.17070897 0.13156305]\n",
            " [0.12503183 0.13642499 0.15017717 0.14748588 0.14272319]\n",
            " [0.13231142 0.12597895 0.13506998 0.10610379 0.12005085]\n",
            " [0.10712562 0.10171823 0.10689609 0.15204804 0.09656857]\n",
            " [0.09225582 0.08786959 0.09980797 0.13241015 0.09885706]] -> [0.12783116]\n",
            "[[0.05554661 0.08140447 0.05956976 0.21793493 0.09216809]\n",
            " [0.09957004 0.11641756 0.11258831 0.17070897 0.13156305]\n",
            " [0.12503183 0.13642499 0.15017717 0.14748588 0.14272319]\n",
            " [0.13231142 0.12597895 0.13506998 0.10610379 0.12005085]\n",
            " [0.10712562 0.10171823 0.10689609 0.15204804 0.09656857]\n",
            " [0.09225582 0.08786959 0.09980797 0.13241015 0.09885706]\n",
            " [0.08642518 0.11502249 0.10747977 0.11381196 0.12783116]] -> [0.13103494]\n",
            "[[0.09957004 0.11641756 0.11258831 0.17070897 0.13156305]\n",
            " [0.12503183 0.13642499 0.15017717 0.14748588 0.14272319]\n",
            " [0.13231142 0.12597895 0.13506998 0.10610379 0.12005085]\n",
            " [0.10712562 0.10171823 0.10689609 0.15204804 0.09656857]\n",
            " [0.09225582 0.08786959 0.09980797 0.13241015 0.09885706]\n",
            " [0.08642518 0.11502249 0.10747977 0.11381196 0.12783116]\n",
            " [0.121064   0.13696956 0.13868579 0.15925428 0.13103494]] -> [0.16331841]\n",
            "[[0.12503183 0.13642499 0.15017717 0.14748588 0.14272319]\n",
            " [0.13231142 0.12597895 0.13506998 0.10610379 0.12005085]\n",
            " [0.10712562 0.10171823 0.10689609 0.15204804 0.09656857]\n",
            " [0.09225582 0.08786959 0.09980797 0.13241015 0.09885706]\n",
            " [0.08642518 0.11502249 0.10747977 0.11381196 0.12783116]\n",
            " [0.121064   0.13696956 0.13868579 0.15925428 0.13103494]\n",
            " [0.13196639 0.14701743 0.15876896 0.10650713 0.16331841]] -> [0.18486407]\n",
            "[[0.13231142 0.12597895 0.13506998 0.10610379 0.12005085]\n",
            " [0.10712562 0.10171823 0.10689609 0.15204804 0.09656857]\n",
            " [0.09225582 0.08786959 0.09980797 0.13241015 0.09885706]\n",
            " [0.08642518 0.11502249 0.10747977 0.11381196 0.12783116]\n",
            " [0.121064   0.13696956 0.13868579 0.15925428 0.13103494]\n",
            " [0.13196639 0.14701743 0.15876896 0.10650713 0.16331841]\n",
            " [0.16036071 0.17565744 0.19052296 0.1441158  0.18486407]] -> [0.21176107]\n",
            "[[0.10712562 0.10171823 0.10689609 0.15204804 0.09656857]\n",
            " [0.09225582 0.08786959 0.09980797 0.13241015 0.09885706]\n",
            " [0.08642518 0.11502249 0.10747977 0.11381196 0.12783116]\n",
            " [0.121064   0.13696956 0.13868579 0.15925428 0.13103494]\n",
            " [0.13196639 0.14701743 0.15876896 0.10650713 0.16331841]\n",
            " [0.16036071 0.17565744 0.19052296 0.1441158  0.18486407]\n",
            " [0.18150985 0.19620923 0.20097623 0.17246572 0.21176107]] -> [0.25840845]\n",
            "[[0.09225582 0.08786959 0.09980797 0.13241015 0.09885706]\n",
            " [0.08642518 0.11502249 0.10747977 0.11381196 0.12783116]\n",
            " [0.121064   0.13696956 0.13868579 0.15925428 0.13103494]\n",
            " [0.13196639 0.14701743 0.15876896 0.10650713 0.16331841]\n",
            " [0.16036071 0.17565744 0.19052296 0.1441158  0.18486407]\n",
            " [0.18150985 0.19620923 0.20097623 0.17246572 0.21176107]\n",
            " [0.21166373 0.23826577 0.23910232 0.18788205 0.25840845]] -> [0.24337573]\n",
            "[[0.08642518 0.11502249 0.10747977 0.11381196 0.12783116]\n",
            " [0.121064   0.13696956 0.13868579 0.15925428 0.13103494]\n",
            " [0.13196639 0.14701743 0.15876896 0.10650713 0.16331841]\n",
            " [0.16036071 0.17565744 0.19052296 0.1441158  0.18486407]\n",
            " [0.18150985 0.19620923 0.20097623 0.17246572 0.21176107]\n",
            " [0.21166373 0.23826577 0.23910232 0.18788205 0.25840845]\n",
            " [0.24123123 0.2445266  0.26290866 0.1473604  0.24337573]] -> [0.23774277]\n",
            "[[0.121064   0.13696956 0.13868579 0.15925428 0.13103494]\n",
            " [0.13196639 0.14701743 0.15876896 0.10650713 0.16331841]\n",
            " [0.16036071 0.17565744 0.19052296 0.1441158  0.18486407]\n",
            " [0.18150985 0.19620923 0.20097623 0.17246572 0.21176107]\n",
            " [0.21166373 0.23826577 0.23910232 0.18788205 0.25840845]\n",
            " [0.24123123 0.2445266  0.26290866 0.1473604  0.24337573]\n",
            " [0.23702205 0.23146036 0.25471058 0.12065968 0.23774277]] -> [0.23707395]\n",
            "[[0.13196639 0.14701743 0.15876896 0.10650713 0.16331841]\n",
            " [0.16036071 0.17565744 0.19052296 0.1441158  0.18486407]\n",
            " [0.18150985 0.19620923 0.20097623 0.17246572 0.21176107]\n",
            " [0.21166373 0.23826577 0.23910232 0.18788205 0.25840845]\n",
            " [0.24123123 0.2445266  0.26290866 0.1473604  0.24337573]\n",
            " [0.23702205 0.23146036 0.25471058 0.12065968 0.23774277]\n",
            " [0.22822425 0.22244355 0.24468696 0.15803531 0.23707395]] -> [0.21609131]\n",
            "[[0.16036071 0.17565744 0.19052296 0.1441158  0.18486407]\n",
            " [0.18150985 0.19620923 0.20097623 0.17246572 0.21176107]\n",
            " [0.21166373 0.23826577 0.23910232 0.18788205 0.25840845]\n",
            " [0.24123123 0.2445266  0.26290866 0.1473604  0.24337573]\n",
            " [0.23702205 0.23146036 0.25471058 0.12065968 0.23774277]\n",
            " [0.22822425 0.22244355 0.24468696 0.15803531 0.23707395]\n",
            " [0.22787922 0.22091228 0.20824335 0.12791073 0.21609131]] -> [0.18444154]\n",
            "[[0.18150985 0.19620923 0.20097623 0.17246572 0.21176107]\n",
            " [0.21166373 0.23826577 0.23910232 0.18788205 0.25840845]\n",
            " [0.24123123 0.2445266  0.26290866 0.1473604  0.24337573]\n",
            " [0.23702205 0.23146036 0.25471058 0.12065968 0.23774277]\n",
            " [0.22822425 0.22244355 0.24468696 0.15803531 0.23707395]\n",
            " [0.22787922 0.22091228 0.20824335 0.12791073 0.21609131]\n",
            " [0.19437874 0.19178579 0.19685934 0.16659496 0.18444154]] -> [0.18345577]\n",
            "[[0.21166373 0.23826577 0.23910232 0.18788205 0.25840845]\n",
            " [0.24123123 0.2445266  0.26290866 0.1473604  0.24337573]\n",
            " [0.23702205 0.23146036 0.25471058 0.12065968 0.23774277]\n",
            " [0.22822425 0.22244355 0.24468696 0.15803531 0.23707395]\n",
            " [0.22787922 0.22091228 0.20824335 0.12791073 0.21609131]\n",
            " [0.19437874 0.19178579 0.19685934 0.16659496 0.18444154]\n",
            " [0.16163736 0.17909399 0.18325562 0.16209555 0.18345577]] -> [0.21598574]\n",
            "[[0.24123123 0.2445266  0.26290866 0.1473604  0.24337573]\n",
            " [0.23702205 0.23146036 0.25471058 0.12065968 0.23774277]\n",
            " [0.22822425 0.22244355 0.24468696 0.15803531 0.23707395]\n",
            " [0.22787922 0.22091228 0.20824335 0.12791073 0.21609131]\n",
            " [0.19437874 0.19178579 0.19685934 0.16659496 0.18444154]\n",
            " [0.16163736 0.17909399 0.18325562 0.16209555 0.18345577]\n",
            " [0.17854291 0.20066662 0.20366109 0.15068567 0.21598574]] -> [0.22454065]\n",
            "[[0.23702205 0.23146036 0.25471058 0.12065968 0.23774277]\n",
            " [0.22822425 0.22244355 0.24468696 0.15803531 0.23707395]\n",
            " [0.22787922 0.22091228 0.20824335 0.12791073 0.21609131]\n",
            " [0.19437874 0.19178579 0.19685934 0.16659496 0.18444154]\n",
            " [0.16163736 0.17909399 0.18325562 0.16209555 0.18345577]\n",
            " [0.17854291 0.20066662 0.20366109 0.15068567 0.21598574]\n",
            " [0.21877102 0.21124885 0.21969923 0.15495205 0.22454065]] -> [0.24492472]\n",
            "[[0.22822425 0.22244355 0.24468696 0.15803531 0.23707395]\n",
            " [0.22787922 0.22091228 0.20824335 0.12791073 0.21609131]\n",
            " [0.19437874 0.19178579 0.19685934 0.16659496 0.18444154]\n",
            " [0.16163736 0.17909399 0.18325562 0.16209555 0.18345577]\n",
            " [0.17854291 0.20066662 0.20366109 0.15068567 0.21598574]\n",
            " [0.21877102 0.21124885 0.21969923 0.15495205 0.22454065]\n",
            " [0.21563135 0.22771748 0.24515233 0.1308237  0.24492472]] -> [0.23939734]\n",
            "[[0.22787922 0.22091228 0.20824335 0.12791073 0.21609131]\n",
            " [0.19437874 0.19178579 0.19685934 0.16659496 0.18444154]\n",
            " [0.16163736 0.17909399 0.18325562 0.16209555 0.18345577]\n",
            " [0.17854291 0.20066662 0.20366109 0.15068567 0.21598574]\n",
            " [0.21877102 0.21124885 0.21969923 0.15495205 0.22454065]\n",
            " [0.21563135 0.22771748 0.24515233 0.1308237  0.24492472]\n",
            " [0.2272238  0.23009945 0.25181101 0.12044456 0.23939734]] -> [0.23337718]\n",
            "[[0.19437874 0.19178579 0.19685934 0.16659496 0.18444154]\n",
            " [0.16163736 0.17909399 0.18325562 0.16209555 0.18345577]\n",
            " [0.17854291 0.20066662 0.20366109 0.15068567 0.21598574]\n",
            " [0.21877102 0.21124885 0.21969923 0.15495205 0.22454065]\n",
            " [0.21563135 0.22771748 0.24515233 0.1308237  0.24492472]\n",
            " [0.2272238  0.23009945 0.25181101 0.12044456 0.23939734]\n",
            " [0.21856409 0.21744159 0.24150081 0.0978937  0.23337718]] -> [0.20700829]\n",
            "[[0.16163736 0.17909399 0.18325562 0.16209555 0.18345577]\n",
            " [0.17854291 0.20066662 0.20366109 0.15068567 0.21598574]\n",
            " [0.21877102 0.21124885 0.21969923 0.15495205 0.22454065]\n",
            " [0.21563135 0.22771748 0.24515233 0.1308237  0.24492472]\n",
            " [0.2272238  0.23009945 0.25181101 0.12044456 0.23939734]\n",
            " [0.21856409 0.21744159 0.24150081 0.0978937  0.23337718]\n",
            " [0.21614889 0.21101061 0.21801654 0.13000807 0.20700829]] -> [0.20845171]\n",
            "[[0.17854291 0.20066662 0.20366109 0.15068567 0.21598574]\n",
            " [0.21877102 0.21124885 0.21969923 0.15495205 0.22454065]\n",
            " [0.21563135 0.22771748 0.24515233 0.1308237  0.24492472]\n",
            " [0.2272238  0.23009945 0.25181101 0.12044456 0.23939734]\n",
            " [0.21856409 0.21744159 0.24150081 0.0978937  0.23337718]\n",
            " [0.21614889 0.21101061 0.21801654 0.13000807 0.20700829]\n",
            " [0.19876043 0.19164981 0.20763491 0.10868513 0.20845171]] -> [0.18211815]\n",
            "[[0.21877102 0.21124885 0.21969923 0.15495205 0.22454065]\n",
            " [0.21563135 0.22771748 0.24515233 0.1308237  0.24492472]\n",
            " [0.2272238  0.23009945 0.25181101 0.12044456 0.23939734]\n",
            " [0.21856409 0.21744159 0.24150081 0.0978937  0.23337718]\n",
            " [0.21614889 0.21101061 0.21801654 0.13000807 0.20700829]\n",
            " [0.19876043 0.19164981 0.20763491 0.10868513 0.20845171]\n",
            " [0.18841021 0.18253074 0.1931721  0.15188671 0.18211815]] -> [0.17764699]\n",
            "[[0.21563135 0.22771748 0.24515233 0.1308237  0.24492472]\n",
            " [0.2272238  0.23009945 0.25181101 0.12044456 0.23939734]\n",
            " [0.21856409 0.21744159 0.24150081 0.0978937  0.23337718]\n",
            " [0.21614889 0.21101061 0.21801654 0.13000807 0.20700829]\n",
            " [0.19876043 0.19164981 0.20763491 0.10868513 0.20845171]\n",
            " [0.18841021 0.18253074 0.1931721  0.15188671 0.18211815]\n",
            " [0.17095249 0.16789929 0.18533203 0.12876221 0.17764699]] -> [0.21411979]\n",
            "[[0.2272238  0.23009945 0.25181101 0.12044456 0.23939734]\n",
            " [0.21856409 0.21744159 0.24150081 0.0978937  0.23337718]\n",
            " [0.21614889 0.21101061 0.21801654 0.13000807 0.20700829]\n",
            " [0.19876043 0.19164981 0.20763491 0.10868513 0.20845171]\n",
            " [0.18841021 0.18253074 0.1931721  0.15188671 0.18211815]\n",
            " [0.17095249 0.16789929 0.18533203 0.12876221 0.17764699]\n",
            " [0.1732296  0.19593705 0.20205026 0.1554002  0.21411979]] -> [0.21950626]\n",
            "[[0.21856409 0.21744159 0.24150081 0.0978937  0.23337718]\n",
            " [0.21614889 0.21101061 0.21801654 0.13000807 0.20700829]\n",
            " [0.19876043 0.19164981 0.20763491 0.10868513 0.20845171]\n",
            " [0.18841021 0.18253074 0.1931721  0.15188671 0.18211815]\n",
            " [0.17095249 0.16789929 0.18533203 0.12876221 0.17764699]\n",
            " [0.1732296  0.19593705 0.20205026 0.1554002  0.21411979]\n",
            " [0.20559153 0.20083676 0.21819555 0.21949449 0.21950626]] -> [0.22464623]\n",
            "[[0.21614889 0.21101061 0.21801654 0.13000807 0.20700829]\n",
            " [0.19876043 0.19164981 0.20763491 0.10868513 0.20845171]\n",
            " [0.18841021 0.18253074 0.1931721  0.15188671 0.18211815]\n",
            " [0.17095249 0.16789929 0.18533203 0.12876221 0.17764699]\n",
            " [0.1732296  0.19593705 0.20205026 0.1554002  0.21411979]\n",
            " [0.20559153 0.20083676 0.21819555 0.21949449 0.21950626]\n",
            " [0.21459647 0.20961555 0.22493288 0.40337008 0.22464623]] -> [0.25488793]\n",
            "[[0.19876043 0.19164981 0.20763491 0.10868513 0.20845171]\n",
            " [0.18841021 0.18253074 0.1931721  0.15188671 0.18211815]\n",
            " [0.17095249 0.16789929 0.18533203 0.12876221 0.17764699]\n",
            " [0.1732296  0.19593705 0.20205026 0.1554002  0.21411979]\n",
            " [0.20559153 0.20083676 0.21819555 0.21949449 0.21950626]\n",
            " [0.21459647 0.20961555 0.22493288 0.40337008 0.22464623]\n",
            " [0.2087312  0.23486317 0.23874431 0.13703505 0.25488793]] -> [0.25372613]\n",
            "[[0.18841021 0.18253074 0.1931721  0.15188671 0.18211815]\n",
            " [0.17095249 0.16789929 0.18533203 0.12876221 0.17764699]\n",
            " [0.1732296  0.19593705 0.20205026 0.1554002  0.21411979]\n",
            " [0.20559153 0.20083676 0.21819555 0.21949449 0.21950626]\n",
            " [0.21459647 0.20961555 0.22493288 0.40337008 0.22464623]\n",
            " [0.2087312  0.23486317 0.23874431 0.13703505 0.25488793]\n",
            " [0.24337024 0.26087965 0.26294437 0.19711392 0.25372613]] -> [0.30311943]\n",
            "[[0.17095249 0.16789929 0.18533203 0.12876221 0.17764699]\n",
            " [0.1732296  0.19593705 0.20205026 0.1554002  0.21411979]\n",
            " [0.20559153 0.20083676 0.21819555 0.21949449 0.21950626]\n",
            " [0.21459647 0.20961555 0.22493288 0.40337008 0.22464623]\n",
            " [0.2087312  0.23486317 0.23874431 0.13703505 0.25488793]\n",
            " [0.24337024 0.26087965 0.26294437 0.19711392 0.25372613]\n",
            " [0.2436118  0.28576638 0.27801585 0.17579995 0.30311943]] -> [0.29378991]\n",
            "[[0.1732296  0.19593705 0.20205026 0.1554002  0.21411979]\n",
            " [0.20559153 0.20083676 0.21819555 0.21949449 0.21950626]\n",
            " [0.21459647 0.20961555 0.22493288 0.40337008 0.22464623]\n",
            " [0.2087312  0.23486317 0.23874431 0.13703505 0.25488793]\n",
            " [0.24337024 0.26087965 0.26294437 0.19711392 0.25372613]\n",
            " [0.2436118  0.28576638 0.27801585 0.17579995 0.30311943]\n",
            " [0.2979164  0.29423904 0.30175077 0.15541812 0.29378991]] -> [0.29815549]\n",
            "[[0.20559153 0.20083676 0.21819555 0.21949449 0.21950626]\n",
            " [0.21459647 0.20961555 0.22493288 0.40337008 0.22464623]\n",
            " [0.2087312  0.23486317 0.23874431 0.13703505 0.25488793]\n",
            " [0.24337024 0.26087965 0.26294437 0.19711392 0.25372613]\n",
            " [0.2436118  0.28576638 0.27801585 0.17579995 0.30311943]\n",
            " [0.2979164  0.29423904 0.30175077 0.15541812 0.29378991]\n",
            " [0.28473691 0.28546025 0.30873152 0.19978489 0.29815549]] -> [0.29125515]\n",
            "[[0.21459647 0.20961555 0.22493288 0.40337008 0.22464623]\n",
            " [0.2087312  0.23486317 0.23874431 0.13703505 0.25488793]\n",
            " [0.24337024 0.26087965 0.26294437 0.19711392 0.25372613]\n",
            " [0.2436118  0.28576638 0.27801585 0.17579995 0.30311943]\n",
            " [0.2979164  0.29423904 0.30175077 0.15541812 0.29378991]\n",
            " [0.28473691 0.28546025 0.30873152 0.19978489 0.29815549]\n",
            " [0.28984307 0.28443941 0.3121324  0.11703863 0.29125515]] -> [0.31727197]\n",
            "[[0.2087312  0.23486317 0.23874431 0.13703505 0.25488793]\n",
            " [0.24337024 0.26087965 0.26294437 0.19711392 0.25372613]\n",
            " [0.2436118  0.28576638 0.27801585 0.17579995 0.30311943]\n",
            " [0.2979164  0.29423904 0.30175077 0.15541812 0.29378991]\n",
            " [0.28473691 0.28546025 0.30873152 0.19978489 0.29815549]\n",
            " [0.28984307 0.28443941 0.3121324  0.11703863 0.29125515]\n",
            " [0.2886701  0.3008741  0.31893437 0.12906695 0.31727197]] -> [0.31609272]\n",
            "[[0.24337024 0.26087965 0.26294437 0.19711392 0.25372613]\n",
            " [0.2436118  0.28576638 0.27801585 0.17579995 0.30311943]\n",
            " [0.2979164  0.29423904 0.30175077 0.15541812 0.29378991]\n",
            " [0.28473691 0.28546025 0.30873152 0.19978489 0.29815549]\n",
            " [0.28984307 0.28443941 0.3121324  0.11703863 0.29125515]\n",
            " [0.2886701  0.3008741  0.31893437 0.12906695 0.31727197]\n",
            " [0.30602415 0.30441289 0.33232315 0.09396791 0.31609272]] -> [0.32452438]\n",
            "[[0.2436118  0.28576638 0.27801585 0.17579995 0.30311943]\n",
            " [0.2979164  0.29423904 0.30175077 0.15541812 0.29378991]\n",
            " [0.28473691 0.28546025 0.30873152 0.19978489 0.29815549]\n",
            " [0.28984307 0.28443941 0.3121324  0.11703863 0.29125515]\n",
            " [0.2886701  0.3008741  0.31893437 0.12906695 0.31727197]\n",
            " [0.30602415 0.30441289 0.33232315 0.09396791 0.31609272]\n",
            " [0.30602415 0.30294973 0.33422056 0.06329659 0.32452438]] -> [0.31579343]\n",
            "[[0.2979164  0.29423904 0.30175077 0.15541812 0.29378991]\n",
            " [0.28473691 0.28546025 0.30873152 0.19978489 0.29815549]\n",
            " [0.28984307 0.28443941 0.3121324  0.11703863 0.29125515]\n",
            " [0.2886701  0.3008741  0.31893437 0.12906695 0.31727197]\n",
            " [0.30602415 0.30441289 0.33232315 0.09396791 0.31609272]\n",
            " [0.30602415 0.30294973 0.33422056 0.06329659 0.32452438]\n",
            " [0.30743868 0.30778134 0.32945929 0.09471184 0.31579343]] -> [0.27650404]\n",
            "[[0.28473691 0.28546025 0.30873152 0.19978489 0.29815549]\n",
            " [0.28984307 0.28443941 0.3121324  0.11703863 0.29125515]\n",
            " [0.2886701  0.3008741  0.31893437 0.12906695 0.31727197]\n",
            " [0.30602415 0.30441289 0.33232315 0.09396791 0.31609272]\n",
            " [0.30602415 0.30294973 0.33422056 0.06329659 0.32452438]\n",
            " [0.30743868 0.30778134 0.32945929 0.09471184 0.31579343]\n",
            " [0.286393   0.28429655 0.28129162 0.17044008 0.27650404]] -> [0.29407152]\n",
            "[[0.28984307 0.28443941 0.3121324  0.11703863 0.29125515]\n",
            " [0.2886701  0.3008741  0.31893437 0.12906695 0.31727197]\n",
            " [0.30602415 0.30441289 0.33232315 0.09396791 0.31609272]\n",
            " [0.30602415 0.30294973 0.33422056 0.06329659 0.32452438]\n",
            " [0.30743868 0.30778134 0.32945929 0.09471184 0.31579343]\n",
            " [0.286393   0.28429655 0.28129162 0.17044008 0.27650404]\n",
            " [0.26541637 0.27474183 0.29289407 0.09938155 0.29407152]] -> [0.27653916]\n",
            "[[0.2886701  0.3008741  0.31893437 0.12906695 0.31727197]\n",
            " [0.30602415 0.30441289 0.33232315 0.09396791 0.31609272]\n",
            " [0.30602415 0.30294973 0.33422056 0.06329659 0.32452438]\n",
            " [0.30743868 0.30778134 0.32945929 0.09471184 0.31579343]\n",
            " [0.286393   0.28429655 0.28129162 0.17044008 0.27650404]\n",
            " [0.26541637 0.27474183 0.29289407 0.09938155 0.29407152]\n",
            " [0.24585427 0.2742995  0.27726411 0.12089271 0.27653916]] -> [0.30498538]\n",
            "[[0.30602415 0.30441289 0.33232315 0.09396791 0.31609272]\n",
            " [0.30602415 0.30294973 0.33422056 0.06329659 0.32452438]\n",
            " [0.30743868 0.30778134 0.32945929 0.09471184 0.31579343]\n",
            " [0.286393   0.28429655 0.28129162 0.17044008 0.27650404]\n",
            " [0.26541637 0.27474183 0.29289407 0.09938155 0.29407152]\n",
            " [0.24585427 0.2742995  0.27726411 0.12089271 0.27653916]\n",
            " [0.26655492 0.28879475 0.30021137 0.14464462 0.30498538]] -> [0.32501716]\n",
            "[[0.30602415 0.30294973 0.33422056 0.06329659 0.32452438]\n",
            " [0.30743868 0.30778134 0.32945929 0.09471184 0.31579343]\n",
            " [0.286393   0.28429655 0.28129162 0.17044008 0.27650404]\n",
            " [0.26541637 0.27474183 0.29289407 0.09938155 0.29407152]\n",
            " [0.24585427 0.2742995  0.27726411 0.12089271 0.27653916]\n",
            " [0.26655492 0.28879475 0.30021137 0.14464462 0.30498538]\n",
            " [0.30343643 0.30363029 0.32387464 0.16546563 0.32501716]] -> [0.32470042]\n",
            "[[0.30743868 0.30778134 0.32945929 0.09471184 0.31579343]\n",
            " [0.286393   0.28429655 0.28129162 0.17044008 0.27650404]\n",
            " [0.26541637 0.27474183 0.29289407 0.09938155 0.29407152]\n",
            " [0.24585427 0.2742995  0.27726411 0.12089271 0.27653916]\n",
            " [0.26655492 0.28879475 0.30021137 0.14464462 0.30498538]\n",
            " [0.30343643 0.30363029 0.32387464 0.16546563 0.32501716]\n",
            " [0.31426977 0.30565469 0.31861207 0.14476114 0.32470042]] -> [0.31723685]\n",
            "[[0.286393   0.28429655 0.28129162 0.17044008 0.27650404]\n",
            " [0.26541637 0.27474183 0.29289407 0.09938155 0.29407152]\n",
            " [0.24585427 0.2742995  0.27726411 0.12089271 0.27653916]\n",
            " [0.26655492 0.28879475 0.30021137 0.14464462 0.30498538]\n",
            " [0.30343643 0.30363029 0.32387464 0.16546563 0.32501716]\n",
            " [0.31426977 0.30565469 0.31861207 0.14476114 0.32470042]\n",
            " [0.32206712 0.31448449 0.33880282 0.12451376 0.31723685]] -> [0.28579823]\n",
            "[[0.26541637 0.27474183 0.29289407 0.09938155 0.29407152]\n",
            " [0.24585427 0.2742995  0.27726411 0.12089271 0.27653916]\n",
            " [0.26655492 0.28879475 0.30021137 0.14464462 0.30498538]\n",
            " [0.30343643 0.30363029 0.32387464 0.16546563 0.32501716]\n",
            " [0.31426977 0.30565469 0.31861207 0.14476114 0.32470042]\n",
            " [0.32206712 0.31448449 0.33880282 0.12451376 0.31723685]\n",
            " [0.29284465 0.28927102 0.29015181 0.2696603  0.28579823]] -> [0.36096206]\n",
            "[[0.24585427 0.2742995  0.27726411 0.12089271 0.27653916]\n",
            " [0.26655492 0.28879475 0.30021137 0.14464462 0.30498538]\n",
            " [0.30343643 0.30363029 0.32387464 0.16546563 0.32501716]\n",
            " [0.31426977 0.30565469 0.31861207 0.14476114 0.32470042]\n",
            " [0.32206712 0.31448449 0.33880282 0.12451376 0.31723685]\n",
            " [0.29284465 0.28927102 0.29015181 0.2696603  0.28579823]\n",
            " [0.3393177  0.34306662 0.3380868  0.35907502 0.36096206]] -> [0.34121168]\n",
            "[[0.26655492 0.28879475 0.30021137 0.14464462 0.30498538]\n",
            " [0.30343643 0.30363029 0.32387464 0.16546563 0.32501716]\n",
            " [0.31426977 0.30565469 0.31861207 0.14476114 0.32470042]\n",
            " [0.32206712 0.31448449 0.33880282 0.12451376 0.31723685]\n",
            " [0.29284465 0.28927102 0.29015181 0.2696603  0.28579823]\n",
            " [0.3393177  0.34306662 0.3380868  0.35907502 0.36096206]\n",
            " [0.33500505 0.3349003  0.34966056 0.18411759 0.34121168]] -> [0.35976514]\n",
            "[[0.30343643 0.30363029 0.32387464 0.16546563 0.32501716]\n",
            " [0.31426977 0.30565469 0.31861207 0.14476114 0.32470042]\n",
            " [0.32206712 0.31448449 0.33880282 0.12451376 0.31723685]\n",
            " [0.29284465 0.28927102 0.29015181 0.2696603  0.28579823]\n",
            " [0.3393177  0.34306662 0.3380868  0.35907502 0.36096206]\n",
            " [0.33500505 0.3349003  0.34966056 0.18411759 0.34121168]\n",
            " [0.3314513  0.35276421 0.36887412 0.15158197 0.35976514]] -> [0.36413051]\n",
            "[[0.31426977 0.30565469 0.31861207 0.14476114 0.32470042]\n",
            " [0.32206712 0.31448449 0.33880282 0.12451376 0.31723685]\n",
            " [0.29284465 0.28927102 0.29015181 0.2696603  0.28579823]\n",
            " [0.3393177  0.34306662 0.3380868  0.35907502 0.36096206]\n",
            " [0.33500505 0.3349003  0.34966056 0.18411759 0.34121168]\n",
            " [0.3314513  0.35276421 0.36887412 0.15158197 0.35976514]\n",
            " [0.34011122 0.34663936 0.37567587 0.10981447 0.36413051]] -> [0.35487145]\n",
            "[[0.32206712 0.31448449 0.33880282 0.12451376 0.31723685]\n",
            " [0.29284465 0.28927102 0.29015181 0.2696603  0.28579823]\n",
            " [0.3393177  0.34306662 0.3380868  0.35907502 0.36096206]\n",
            " [0.33500505 0.3349003  0.34966056 0.18411759 0.34121168]\n",
            " [0.3314513  0.35276421 0.36887412 0.15158197 0.35976514]\n",
            " [0.34011122 0.34663936 0.37567587 0.10981447 0.36413051]\n",
            " [0.35122054 0.35225378 0.37306244 0.09206776 0.35487145]] -> [0.33962757]\n",
            "[[0.29284465 0.28927102 0.29015181 0.2696603  0.28579823]\n",
            " [0.3393177  0.34306662 0.3380868  0.35907502 0.36096206]\n",
            " [0.33500505 0.3349003  0.34966056 0.18411759 0.34121168]\n",
            " [0.3314513  0.35276421 0.36887412 0.15158197 0.35976514]\n",
            " [0.34011122 0.34663936 0.37567587 0.10981447 0.36413051]\n",
            " [0.35122054 0.35225378 0.37306244 0.09206776 0.35487145]\n",
            " [0.33034738 0.32625771 0.35609378 0.08286278 0.33962757]] -> [0.34519007]\n",
            "[[0.3393177  0.34306662 0.3380868  0.35907502 0.36096206]\n",
            " [0.33500505 0.3349003  0.34966056 0.18411759 0.34121168]\n",
            " [0.3314513  0.35276421 0.36887412 0.15158197 0.35976514]\n",
            " [0.34011122 0.34663936 0.37567587 0.10981447 0.36413051]\n",
            " [0.35122054 0.35225378 0.37306244 0.09206776 0.35487145]\n",
            " [0.33034738 0.32625771 0.35609378 0.08286278 0.33962757]\n",
            " [0.32230868 0.32843538 0.34794237 0.08773864 0.34519007]] -> [0.32762237]\n",
            "[[0.33500505 0.3349003  0.34966056 0.18411759 0.34121168]\n",
            " [0.3314513  0.35276421 0.36887412 0.15158197 0.35976514]\n",
            " [0.34011122 0.34663936 0.37567587 0.10981447 0.36413051]\n",
            " [0.35122054 0.35225378 0.37306244 0.09206776 0.35487145]\n",
            " [0.33034738 0.32625771 0.35609378 0.08286278 0.33962757]\n",
            " [0.32230868 0.32843538 0.34794237 0.08773864 0.34519007]\n",
            " [0.32465462 0.31890794 0.34351041 0.12027427 0.32762237]] -> [0.3339946]\n",
            "[[0.3314513  0.35276421 0.36887412 0.15158197 0.35976514]\n",
            " [0.34011122 0.34663936 0.37567587 0.10981447 0.36413051]\n",
            " [0.35122054 0.35225378 0.37306244 0.09206776 0.35487145]\n",
            " [0.33034738 0.32625771 0.35609378 0.08286278 0.33962757]\n",
            " [0.32230868 0.32843538 0.34794237 0.08773864 0.34519007]\n",
            " [0.32465462 0.31890794 0.34351041 0.12027427 0.32762237]\n",
            " [0.31706442 0.31822758 0.34524656 0.09039168 0.3339946 ]] -> [0.27829953]\n",
            "[[0.34011122 0.34663936 0.37567587 0.10981447 0.36413051]\n",
            " [0.35122054 0.35225378 0.37306244 0.09206776 0.35487145]\n",
            " [0.33034738 0.32625771 0.35609378 0.08286278 0.33962757]\n",
            " [0.32230868 0.32843538 0.34794237 0.08773864 0.34519007]\n",
            " [0.32465462 0.31890794 0.34351041 0.12027427 0.32762237]\n",
            " [0.31706442 0.31822758 0.34524656 0.09039168 0.3339946 ]\n",
            " [0.29653629 0.2983222  0.2951278  0.18775657 0.27829953]] -> [0.25883077]\n",
            "[[0.35122054 0.35225378 0.37306244 0.09206776 0.35487145]\n",
            " [0.33034738 0.32625771 0.35609378 0.08286278 0.33962757]\n",
            " [0.32230868 0.32843538 0.34794237 0.08773864 0.34519007]\n",
            " [0.32465462 0.31890794 0.34351041 0.12027427 0.32762237]\n",
            " [0.31706442 0.31822758 0.34524656 0.09039168 0.3339946 ]\n",
            " [0.29653629 0.2983222  0.2951278  0.18775657 0.27829953]\n",
            " [0.26134529 0.27215599 0.26953141 0.17453617 0.25883077]] -> [0.28375646]\n",
            "[[0.33034738 0.32625771 0.35609378 0.08286278 0.33962757]\n",
            " [0.32230868 0.32843538 0.34794237 0.08773864 0.34519007]\n",
            " [0.32465462 0.31890794 0.34351041 0.12027427 0.32762237]\n",
            " [0.31706442 0.31822758 0.34524656 0.09039168 0.3339946 ]\n",
            " [0.29653629 0.2983222  0.2951278  0.18775657 0.27829953]\n",
            " [0.26134529 0.27215599 0.26953141 0.17453617 0.25883077]\n",
            " [0.2566532  0.27008035 0.27400631 0.12722058 0.28375646]] -> [0.25531025]\n",
            "[[0.32230868 0.32843538 0.34794237 0.08773864 0.34519007]\n",
            " [0.32465462 0.31890794 0.34351041 0.12027427 0.32762237]\n",
            " [0.31706442 0.31822758 0.34524656 0.09039168 0.3339946 ]\n",
            " [0.29653629 0.2983222  0.2951278  0.18775657 0.27829953]\n",
            " [0.26134529 0.27215599 0.26953141 0.17453617 0.25883077]\n",
            " [0.2566532  0.27008035 0.27400631 0.12722058 0.28375646]\n",
            " [0.26013769 0.25861348 0.26867231 0.13832571 0.25531025]] -> [0.25990121]\n",
            "[[0.32465462 0.31890794 0.34351041 0.12027427 0.32762237]\n",
            " [0.31706442 0.31822758 0.34524656 0.09039168 0.3339946 ]\n",
            " [0.29653629 0.2983222  0.2951278  0.18775657 0.27829953]\n",
            " [0.26134529 0.27215599 0.26953141 0.17453617 0.25883077]\n",
            " [0.2566532  0.27008035 0.27400631 0.12722058 0.28375646]\n",
            " [0.26013769 0.25861348 0.26867231 0.13832571 0.25531025]\n",
            " [0.23160549 0.25425813 0.25932878 0.118885   0.25990121]] -> [0.24929009]\n",
            "[[0.31706442 0.31822758 0.34524656 0.09039168 0.3339946 ]\n",
            " [0.29653629 0.2983222  0.2951278  0.18775657 0.27829953]\n",
            " [0.26134529 0.27215599 0.26953141 0.17453617 0.25883077]\n",
            " [0.2566532  0.27008035 0.27400631 0.12722058 0.28375646]\n",
            " [0.26013769 0.25861348 0.26867231 0.13832571 0.25531025]\n",
            " [0.23160549 0.25425813 0.25932878 0.118885   0.25990121]\n",
            " [0.25306503 0.25150194 0.26326667 0.0988617  0.24929009]] -> [0.26833633]\n",
            "[[0.29653629 0.2983222  0.2951278  0.18775657 0.27829953]\n",
            " [0.26134529 0.27215599 0.26953141 0.17453617 0.25883077]\n",
            " [0.2566532  0.27008035 0.27400631 0.12722058 0.28375646]\n",
            " [0.26013769 0.25861348 0.26867231 0.13832571 0.25531025]\n",
            " [0.23160549 0.25425813 0.25932878 0.118885   0.25990121]\n",
            " [0.25306503 0.25150194 0.26326667 0.0988617  0.24929009]\n",
            " [0.23774653 0.25272687 0.26058182 0.13326163 0.26833633]] -> [0.265203]\n",
            "[[0.26134529 0.27215599 0.26953141 0.17453617 0.25883077]\n",
            " [0.2566532  0.27008035 0.27400631 0.12722058 0.28375646]\n",
            " [0.26013769 0.25861348 0.26867231 0.13832571 0.25531025]\n",
            " [0.23160549 0.25425813 0.25932878 0.118885   0.25990121]\n",
            " [0.25306503 0.25150194 0.26326667 0.0988617  0.24929009]\n",
            " [0.23774653 0.25272687 0.26058182 0.13326163 0.26833633]\n",
            " [0.25993076 0.25354342 0.28080828 0.10816528 0.265203  ]] -> [0.24707228]\n",
            "[[0.2566532  0.27008035 0.27400631 0.12722058 0.28375646]\n",
            " [0.26013769 0.25861348 0.26867231 0.13832571 0.25531025]\n",
            " [0.23160549 0.25425813 0.25932878 0.118885   0.25990121]\n",
            " [0.25306503 0.25150194 0.26326667 0.0988617  0.24929009]\n",
            " [0.23774653 0.25272687 0.26058182 0.13326163 0.26833633]\n",
            " [0.25993076 0.25354342 0.28080828 0.10816528 0.265203  ]\n",
            " [0.24105872 0.23792549 0.26247901 0.13750112 0.24707228]] -> [0.28949478]\n",
            "[[0.26013769 0.25861348 0.26867231 0.13832571 0.25531025]\n",
            " [0.23160549 0.25425813 0.25932878 0.118885   0.25990121]\n",
            " [0.25306503 0.25150194 0.26326667 0.0988617  0.24929009]\n",
            " [0.23774653 0.25272687 0.26058182 0.13326163 0.26833633]\n",
            " [0.25993076 0.25354342 0.28080828 0.10816528 0.265203  ]\n",
            " [0.24105872 0.23792549 0.26247901 0.13750112 0.24707228]\n",
            " [0.25068446 0.26888937 0.27991326 0.12828717 0.28949478]] -> [0.28903713]\n",
            "[[0.23160549 0.25425813 0.25932878 0.118885   0.25990121]\n",
            " [0.25306503 0.25150194 0.26326667 0.0988617  0.24929009]\n",
            " [0.23774653 0.25272687 0.26058182 0.13326163 0.26833633]\n",
            " [0.25993076 0.25354342 0.28080828 0.10816528 0.265203  ]\n",
            " [0.24105872 0.23792549 0.26247901 0.13750112 0.24707228]\n",
            " [0.25068446 0.26888937 0.27991326 0.12828717 0.28949478]\n",
            " [0.28128683 0.2787571  0.29827824 0.08761316 0.28903713]] -> [0.28491826]\n",
            "[[0.25306503 0.25150194 0.26326667 0.0988617  0.24929009]\n",
            " [0.23774653 0.25272687 0.26058182 0.13326163 0.26833633]\n",
            " [0.25993076 0.25354342 0.28080828 0.10816528 0.265203  ]\n",
            " [0.24105872 0.23792549 0.26247901 0.13750112 0.24707228]\n",
            " [0.25068446 0.26888937 0.27991326 0.12828717 0.28949478]\n",
            " [0.28128683 0.2787571  0.29827824 0.08761316 0.28903713]\n",
            " [0.28708306 0.28379279 0.2969895  0.1354486  0.28491826]] -> [0.31547648]\n",
            "[[0.23774653 0.25272687 0.26058182 0.13326163 0.26833633]\n",
            " [0.25993076 0.25354342 0.28080828 0.10816528 0.265203  ]\n",
            " [0.24105872 0.23792549 0.26247901 0.13750112 0.24707228]\n",
            " [0.25068446 0.26888937 0.27991326 0.12828717 0.28949478]\n",
            " [0.28128683 0.2787571  0.29827824 0.08761316 0.28903713]\n",
            " [0.28708306 0.28379279 0.2969895  0.1354486  0.28491826]\n",
            " [0.28104527 0.30124832 0.3166073  0.11438559 0.31547648]] -> [0.33202308]\n",
            "[[0.25993076 0.25354342 0.28080828 0.10816528 0.265203  ]\n",
            " [0.24105872 0.23792549 0.26247901 0.13750112 0.24707228]\n",
            " [0.25068446 0.26888937 0.27991326 0.12828717 0.28949478]\n",
            " [0.28128683 0.2787571  0.29827824 0.08761316 0.28903713]\n",
            " [0.28708306 0.28379279 0.2969895  0.1354486  0.28491826]\n",
            " [0.28104527 0.30124832 0.3166073  0.11438559 0.31547648]\n",
            " [0.31171669 0.31087781 0.34524656 0.08700367 0.33202308]] -> [0.32367932]\n",
            "[[0.24105872 0.23792549 0.26247901 0.13750112 0.24707228]\n",
            " [0.25068446 0.26888937 0.27991326 0.12828717 0.28949478]\n",
            " [0.28128683 0.2787571  0.29827824 0.08761316 0.28903713]\n",
            " [0.28708306 0.28379279 0.2969895  0.1354486  0.28491826]\n",
            " [0.28104527 0.30124832 0.3166073  0.11438559 0.31547648]\n",
            " [0.31171669 0.31087781 0.34524656 0.08700367 0.33202308]\n",
            " [0.3147529  0.30870014 0.34012728 0.09221117 0.32367932]] -> [0.31973648]\n",
            "[[0.25068446 0.26888937 0.27991326 0.12828717 0.28949478]\n",
            " [0.28128683 0.2787571  0.29827824 0.08761316 0.28903713]\n",
            " [0.28708306 0.28379279 0.2969895  0.1354486  0.28491826]\n",
            " [0.28104527 0.30124832 0.3166073  0.11438559 0.31547648]\n",
            " [0.31171669 0.31087781 0.34524656 0.08700367 0.33202308]\n",
            " [0.3147529  0.30870014 0.34012728 0.09221117 0.32367932]\n",
            " [0.30764561 0.30121438 0.335008   0.08128529 0.31973648]] -> [0.31688477]\n",
            "[[0.28128683 0.2787571  0.29827824 0.08761316 0.28903713]\n",
            " [0.28708306 0.28379279 0.2969895  0.1354486  0.28491826]\n",
            " [0.28104527 0.30124832 0.3166073  0.11438559 0.31547648]\n",
            " [0.31171669 0.31087781 0.34524656 0.08700367 0.33202308]\n",
            " [0.3147529  0.30870014 0.34012728 0.09221117 0.32367932]\n",
            " [0.30764561 0.30121438 0.335008   0.08128529 0.31973648]\n",
            " [0.30685209 0.30371858 0.33321817 0.07000986 0.31688477]] -> [0.30857635]\n",
            "[[0.28708306 0.28379279 0.2969895  0.1354486  0.28491826]\n",
            " [0.28104527 0.30124832 0.3166073  0.11438559 0.31547648]\n",
            " [0.31171669 0.31087781 0.34524656 0.08700367 0.33202308]\n",
            " [0.3147529  0.30870014 0.34012728 0.09221117 0.32367932]\n",
            " [0.30764561 0.30121438 0.335008   0.08128529 0.31973648]\n",
            " [0.30685209 0.30371858 0.33321817 0.07000986 0.31688477]\n",
            " [0.31075065 0.30291557 0.32734716 0.12131397 0.30857635]] -> [0.30033818]\n",
            "[[0.28104527 0.30124832 0.3166073  0.11438559 0.31547648]\n",
            " [0.31171669 0.31087781 0.34524656 0.08700367 0.33202308]\n",
            " [0.3147529  0.30870014 0.34012728 0.09221117 0.32367932]\n",
            " [0.30764561 0.30121438 0.335008   0.08128529 0.31973648]\n",
            " [0.30685209 0.30371858 0.33321817 0.07000986 0.31688477]\n",
            " [0.31075065 0.30291557 0.32734716 0.12131397 0.30857635]\n",
            " [0.29881339 0.29202721 0.31868372 0.14624899 0.30033818]] -> [0.27618709]\n",
            "[[0.31171669 0.31087781 0.34524656 0.08700367 0.33202308]\n",
            " [0.3147529  0.30870014 0.34012728 0.09221117 0.32367932]\n",
            " [0.30764561 0.30121438 0.335008   0.08128529 0.31973648]\n",
            " [0.30685209 0.30371858 0.33321817 0.07000986 0.31688477]\n",
            " [0.31075065 0.30291557 0.32734716 0.12131397 0.30857635]\n",
            " [0.29881339 0.29202721 0.31868372 0.14624899 0.30033818]\n",
            " [0.28504752 0.28076463 0.29549655 0.15195841 0.27618709]] -> [0.2698502]\n",
            "[[0.3147529  0.30870014 0.34012728 0.09221117 0.32367932]\n",
            " [0.30764561 0.30121438 0.335008   0.08128529 0.31973648]\n",
            " [0.30685209 0.30371858 0.33321817 0.07000986 0.31688477]\n",
            " [0.31075065 0.30291557 0.32734716 0.12131397 0.30857635]\n",
            " [0.29881339 0.29202721 0.31868372 0.14624899 0.30033818]\n",
            " [0.28504752 0.28076463 0.29549655 0.15195841 0.27618709]\n",
            " [0.25844718 0.26293488 0.28474617 0.11517433 0.2698502 ]] -> [0.27829953]\n",
            "[[0.30764561 0.30121438 0.335008   0.08128529 0.31973648]\n",
            " [0.30685209 0.30371858 0.33321817 0.07000986 0.31688477]\n",
            " [0.31075065 0.30291557 0.32734716 0.12131397 0.30857635]\n",
            " [0.29881339 0.29202721 0.31868372 0.14624899 0.30033818]\n",
            " [0.28504752 0.28076463 0.29549655 0.15195841 0.27618709]\n",
            " [0.25844718 0.26293488 0.28474617 0.11517433 0.2698502 ]\n",
            " [0.2645538  0.25881756 0.28463509 0.09642377 0.27829953]] -> [0.29847223]\n",
            "[[0.30685209 0.30371858 0.33321817 0.07000986 0.31688477]\n",
            " [0.31075065 0.30291557 0.32734716 0.12131397 0.30857635]\n",
            " [0.29881339 0.29202721 0.31868372 0.14624899 0.30033818]\n",
            " [0.28504752 0.28076463 0.29549655 0.15195841 0.27618709]\n",
            " [0.25844718 0.26293488 0.28474617 0.11517433 0.2698502 ]\n",
            " [0.2645538  0.25881756 0.28463509 0.09642377 0.27829953]\n",
            " [0.26634799 0.27851886 0.29938799 0.14076365 0.29847223]] -> [0.30061979]\n",
            "[[0.31075065 0.30291557 0.32734716 0.12131397 0.30857635]\n",
            " [0.29881339 0.29202721 0.31868372 0.14624899 0.30033818]\n",
            " [0.28504752 0.28076463 0.29549655 0.15195841 0.27618709]\n",
            " [0.25844718 0.26293488 0.28474617 0.11517433 0.2698502 ]\n",
            " [0.2645538  0.25881756 0.28463509 0.09642377 0.27829953]\n",
            " [0.26634799 0.27851886 0.29938799 0.14076365 0.29847223]\n",
            " [0.29446633 0.29607643 0.31302742 0.10820113 0.30061979]] -> [0.31484279]\n",
            "[[0.29881339 0.29202721 0.31868372 0.14624899 0.30033818]\n",
            " [0.28504752 0.28076463 0.29549655 0.15195841 0.27618709]\n",
            " [0.25844718 0.26293488 0.28474617 0.11517433 0.2698502 ]\n",
            " [0.2645538  0.25881756 0.28463509 0.09642377 0.27829953]\n",
            " [0.26634799 0.27851886 0.29938799 0.14076365 0.29847223]\n",
            " [0.29446633 0.29607643 0.31302742 0.10820113 0.30061979]\n",
            " [0.29446633 0.30631838 0.32813461 0.12998118 0.31484279]] -> [0.32927716]\n",
            "[[0.28504752 0.28076463 0.29549655 0.15195841 0.27618709]\n",
            " [0.25844718 0.26293488 0.28474617 0.11517433 0.2698502 ]\n",
            " [0.2645538  0.25881756 0.28463509 0.09642377 0.27829953]\n",
            " [0.26634799 0.27851886 0.29938799 0.14076365 0.29847223]\n",
            " [0.29446633 0.29607643 0.31302742 0.10820113 0.30061979]\n",
            " [0.29446633 0.30631838 0.32813461 0.12998118 0.31484279]\n",
            " [0.30819757 0.30818972 0.3379078  0.14559469 0.32927716]] -> [0.34209186]\n",
            "[[0.25844718 0.26293488 0.28474617 0.11517433 0.2698502 ]\n",
            " [0.2645538  0.25881756 0.28463509 0.09642377 0.27829953]\n",
            " [0.26634799 0.27851886 0.29938799 0.14076365 0.29847223]\n",
            " [0.29446633 0.29607643 0.31302742 0.10820113 0.30061979]\n",
            " [0.29446633 0.30631838 0.32813461 0.12998118 0.31484279]\n",
            " [0.30819757 0.30818972 0.3379078  0.14559469 0.32927716]\n",
            " [0.31723694 0.32595137 0.35348035 0.12755221 0.34209186]] -> [0.31142785]\n",
            "[[0.2645538  0.25881756 0.28463509 0.09642377 0.27829953]\n",
            " [0.26634799 0.27851886 0.29938799 0.14076365 0.29847223]\n",
            " [0.29446633 0.29607643 0.31302742 0.10820113 0.30061979]\n",
            " [0.29446633 0.30631838 0.32813461 0.12998118 0.31484279]\n",
            " [0.30819757 0.30818972 0.3379078  0.14559469 0.32927716]\n",
            " [0.31723694 0.32595137 0.35348035 0.12755221 0.34209186]\n",
            " [0.32517217 0.31652618 0.33092704 0.11466344 0.31142785]] -> [0.31878584]\n",
            "[[0.26634799 0.27851886 0.29938799 0.14076365 0.29847223]\n",
            " [0.29446633 0.29607643 0.31302742 0.10820113 0.30061979]\n",
            " [0.29446633 0.30631838 0.32813461 0.12998118 0.31484279]\n",
            " [0.30819757 0.30818972 0.3379078  0.14559469 0.32927716]\n",
            " [0.31723694 0.32595137 0.35348035 0.12755221 0.34209186]\n",
            " [0.32517217 0.31652618 0.33092704 0.11466344 0.31142785]\n",
            " [0.29964133 0.29781178 0.31997246 0.08688716 0.31878584]] -> [0.31262499]\n",
            "[[0.29446633 0.29607643 0.31302742 0.10820113 0.30061979]\n",
            " [0.29446633 0.30631838 0.32813461 0.12998118 0.31484279]\n",
            " [0.30819757 0.30818972 0.3379078  0.14559469 0.32927716]\n",
            " [0.31723694 0.32595137 0.35348035 0.12755221 0.34209186]\n",
            " [0.32517217 0.31652618 0.33092704 0.11466344 0.31142785]\n",
            " [0.29964133 0.29781178 0.31997246 0.08688716 0.31878584]\n",
            " [0.29570834 0.29206137 0.31753804 0.10872995 0.31262499]] -> [0.29245207]\n",
            "[[0.29446633 0.30631838 0.32813461 0.12998118 0.31484279]\n",
            " [0.30819757 0.30818972 0.3379078  0.14559469 0.32927716]\n",
            " [0.31723694 0.32595137 0.35348035 0.12755221 0.34209186]\n",
            " [0.32517217 0.31652618 0.33092704 0.11466344 0.31142785]\n",
            " [0.29964133 0.29781178 0.31997246 0.08688716 0.31878584]\n",
            " [0.29570834 0.29206137 0.31753804 0.10872995 0.31262499]\n",
            " [0.2979164  0.29148285 0.3110943  0.14284306 0.29245207]] -> [0.28358042]\n",
            "[[0.30819757 0.30818972 0.3379078  0.14559469 0.32927716]\n",
            " [0.31723694 0.32595137 0.35348035 0.12755221 0.34209186]\n",
            " [0.32517217 0.31652618 0.33092704 0.11466344 0.31142785]\n",
            " [0.29964133 0.29781178 0.31997246 0.08688716 0.31878584]\n",
            " [0.29570834 0.29206137 0.31753804 0.10872995 0.31262499]\n",
            " [0.2979164  0.29148285 0.3110943  0.14284306 0.29245207]\n",
            " [0.27010846 0.26871923 0.28871977 0.14247558 0.28358042]] -> [0.30769617]\n",
            "[[0.31723694 0.32595137 0.35348035 0.12755221 0.34209186]\n",
            " [0.32517217 0.31652618 0.33092704 0.11466344 0.31142785]\n",
            " [0.29964133 0.29781178 0.31997246 0.08688716 0.31878584]\n",
            " [0.29570834 0.29206137 0.31753804 0.10872995 0.31262499]\n",
            " [0.2979164  0.29148285 0.3110943  0.14284306 0.29245207]\n",
            " [0.27010846 0.26871923 0.28871977 0.14247558 0.28358042]\n",
            " [0.26948745 0.29100637 0.30465034 0.13197096 0.30769617]] -> [0.32466509]\n",
            "[[0.32517217 0.31652618 0.33092704 0.11466344 0.31142785]\n",
            " [0.29964133 0.29781178 0.31997246 0.08688716 0.31878584]\n",
            " [0.29570834 0.29206137 0.31753804 0.10872995 0.31262499]\n",
            " [0.2979164  0.29148285 0.3110943  0.14284306 0.29245207]\n",
            " [0.27010846 0.26871923 0.28871977 0.14247558 0.28358042]\n",
            " [0.26948745 0.29100637 0.30465034 0.13197096 0.30769617]\n",
            " [0.29450074 0.31149027 0.32654153 0.1510173  0.32466509]] -> [0.34050775]\n",
            "[[0.29964133 0.29781178 0.31997246 0.08688716 0.31878584]\n",
            " [0.29570834 0.29206137 0.31753804 0.10872995 0.31262499]\n",
            " [0.2979164  0.29148285 0.3110943  0.14284306 0.29245207]\n",
            " [0.27010846 0.26871923 0.28871977 0.14247558 0.28358042]\n",
            " [0.26948745 0.29100637 0.30465034 0.13197096 0.30769617]\n",
            " [0.29450074 0.31149027 0.32654153 0.1510173  0.32466509]\n",
            " [0.31861705 0.31836357 0.34882644 0.12876221 0.34050775]] -> [0.36448258]\n",
            "[[0.29570834 0.29206137 0.31753804 0.10872995 0.31262499]\n",
            " [0.2979164  0.29148285 0.3110943  0.14284306 0.29245207]\n",
            " [0.27010846 0.26871923 0.28871977 0.14247558 0.28358042]\n",
            " [0.26948745 0.29100637 0.30465034 0.13197096 0.30769617]\n",
            " [0.29450074 0.31149027 0.32654153 0.1510173  0.32466509]\n",
            " [0.31861705 0.31836357 0.34882644 0.12876221 0.34050775]\n",
            " [0.33414248 0.34197789 0.36493623 0.33420274 0.36448258]] -> [0.33381857]\n",
            "[[0.2979164  0.29148285 0.3110943  0.14284306 0.29245207]\n",
            " [0.27010846 0.26871923 0.28871977 0.14247558 0.28358042]\n",
            " [0.26948745 0.29100637 0.30465034 0.13197096 0.30769617]\n",
            " [0.29450074 0.31149027 0.32654153 0.1510173  0.32466509]\n",
            " [0.31861705 0.31836357 0.34882644 0.12876221 0.34050775]\n",
            " [0.33414248 0.34197789 0.36493623 0.33420274 0.36448258]\n",
            " [0.34214676 0.3333657  0.34331344 0.15072152 0.33381857]] -> [0.31185038]\n",
            "[[0.27010846 0.26871923 0.28871977 0.14247558 0.28358042]\n",
            " [0.26948745 0.29100637 0.30465034 0.13197096 0.30769617]\n",
            " [0.29450074 0.31149027 0.32654153 0.1510173  0.32466509]\n",
            " [0.31861705 0.31836357 0.34882644 0.12876221 0.34050775]\n",
            " [0.33414248 0.34197789 0.36493623 0.33420274 0.36448258]\n",
            " [0.34214676 0.3333657  0.34331344 0.15072152 0.33381857]\n",
            " [0.31809951 0.30921056 0.33450692 0.13117325 0.31185038]] -> [0.33600125]\n",
            "[[0.26948745 0.29100637 0.30465034 0.13197096 0.30769617]\n",
            " [0.29450074 0.31149027 0.32654153 0.1510173  0.32466509]\n",
            " [0.31861705 0.31836357 0.34882644 0.12876221 0.34050775]\n",
            " [0.33414248 0.34197789 0.36493623 0.33420274 0.36448258]\n",
            " [0.34214676 0.3333657  0.34331344 0.15072152 0.33381857]\n",
            " [0.31809951 0.30921056 0.33450692 0.13117325 0.31185038]\n",
            " [0.29950345 0.31866991 0.33278852 0.15418123 0.33600125]] -> [0.29048055]\n",
            "[[0.29450074 0.31149027 0.32654153 0.1510173  0.32466509]\n",
            " [0.31861705 0.31836357 0.34882644 0.12876221 0.34050775]\n",
            " [0.33414248 0.34197789 0.36493623 0.33420274 0.36448258]\n",
            " [0.34214676 0.3333657  0.34331344 0.15072152 0.33381857]\n",
            " [0.31809951 0.30921056 0.33450692 0.13117325 0.31185038]\n",
            " [0.29950345 0.31866991 0.33278852 0.15418123 0.33600125]\n",
            " [0.32051449 0.31305548 0.31009191 0.17191001 0.29048055]] -> [0.29766251]\n",
            "[[0.31861705 0.31836357 0.34882644 0.12876221 0.34050775]\n",
            " [0.33414248 0.34197789 0.36493623 0.33420274 0.36448258]\n",
            " [0.34214676 0.3333657  0.34331344 0.15072152 0.33381857]\n",
            " [0.31809951 0.30921056 0.33450692 0.13117325 0.31185038]\n",
            " [0.29950345 0.31866991 0.33278852 0.15418123 0.33600125]\n",
            " [0.32051449 0.31305548 0.31009191 0.17191001 0.29048055]\n",
            " [0.28087297 0.28335047 0.31181032 0.12868155 0.29766251]] -> [0.29505729]\n",
            "[[0.33414248 0.34197789 0.36493623 0.33420274 0.36448258]\n",
            " [0.34214676 0.3333657  0.34331344 0.15072152 0.33381857]\n",
            " [0.31809951 0.30921056 0.33450692 0.13117325 0.31185038]\n",
            " [0.29950345 0.31866991 0.33278852 0.15418123 0.33600125]\n",
            " [0.32051449 0.31305548 0.31009191 0.17191001 0.29048055]\n",
            " [0.28087297 0.28335047 0.31181032 0.12868155 0.29766251]\n",
            " [0.26600296 0.27974379 0.29931634 0.11423322 0.29505729]] -> [0.29857781]\n",
            "[[0.34214676 0.3333657  0.34331344 0.15072152 0.33381857]\n",
            " [0.31809951 0.30921056 0.33450692 0.13117325 0.31185038]\n",
            " [0.29950345 0.31866991 0.33278852 0.15418123 0.33600125]\n",
            " [0.32051449 0.31305548 0.31009191 0.17191001 0.29048055]\n",
            " [0.28087297 0.28335047 0.31181032 0.12868155 0.29766251]\n",
            " [0.26600296 0.27974379 0.29931634 0.11423322 0.29505729]\n",
            " [0.28387455 0.28539215 0.30533065 0.14464462 0.29857781]] -> [0.26657617]\n",
            "[[0.31809951 0.30921056 0.33450692 0.13117325 0.31185038]\n",
            " [0.29950345 0.31866991 0.33278852 0.15418123 0.33600125]\n",
            " [0.32051449 0.31305548 0.31009191 0.17191001 0.29048055]\n",
            " [0.28087297 0.28335047 0.31181032 0.12868155 0.29766251]\n",
            " [0.26600296 0.27974379 0.29931634 0.11423322 0.29505729]\n",
            " [0.28387455 0.28539215 0.30533065 0.14464462 0.29857781]\n",
            " [0.28070045 0.27766816 0.28442388 0.12884288 0.26657617]] -> [0.27294819]\n",
            "[[0.29950345 0.31866991 0.33278852 0.15418123 0.33600125]\n",
            " [0.32051449 0.31305548 0.31009191 0.17191001 0.29048055]\n",
            " [0.28087297 0.28335047 0.31181032 0.12868155 0.29766251]\n",
            " [0.26600296 0.27974379 0.29931634 0.11423322 0.29505729]\n",
            " [0.28387455 0.28539215 0.30533065 0.14464462 0.29857781]\n",
            " [0.28070045 0.27766816 0.28442388 0.12884288 0.26657617]\n",
            " [0.25068446 0.25837524 0.2712141  0.10491171 0.27294819]] -> [0.29125515]\n",
            "[[0.32051449 0.31305548 0.31009191 0.17191001 0.29048055]\n",
            " [0.28087297 0.28335047 0.31181032 0.12868155 0.29766251]\n",
            " [0.26600296 0.27974379 0.29931634 0.11423322 0.29505729]\n",
            " [0.28387455 0.28539215 0.30533065 0.14464462 0.29857781]\n",
            " [0.28070045 0.27766816 0.28442388 0.12884288 0.26657617]\n",
            " [0.25068446 0.25837524 0.2712141  0.10491171 0.27294819]\n",
            " [0.27048812 0.2764603  0.30407761 0.1016223  0.29125515]] -> [0.29854269]\n",
            "[[0.28087297 0.28335047 0.31181032 0.12868155 0.29766251]\n",
            " [0.26600296 0.27974379 0.29931634 0.11423322 0.29505729]\n",
            " [0.28387455 0.28539215 0.30533065 0.14464462 0.29857781]\n",
            " [0.28070045 0.27766816 0.28442388 0.12884288 0.26657617]\n",
            " [0.25068446 0.25837524 0.2712141  0.10491171 0.27294819]\n",
            " [0.27048812 0.2764603  0.30407761 0.1016223  0.29125515]\n",
            " [0.2903262  0.28930518 0.31102265 0.10815631 0.29854269]] -> [0.25062793]\n",
            "[[0.26600296 0.27974379 0.29931634 0.11423322 0.29505729]\n",
            " [0.28387455 0.28539215 0.30533065 0.14464462 0.29857781]\n",
            " [0.28070045 0.27766816 0.28442388 0.12884288 0.26657617]\n",
            " [0.25068446 0.25837524 0.2712141  0.10491171 0.27294819]\n",
            " [0.27048812 0.2764603  0.30407761 0.1016223  0.29125515]\n",
            " [0.2903262  0.28930518 0.31102265 0.10815631 0.29854269]\n",
            " [0.27514579 0.26980817 0.27271756 0.17060142 0.25062793]] -> [0.28146798]\n",
            "[[0.28387455 0.28539215 0.30533065 0.14464462 0.29857781]\n",
            " [0.28070045 0.27766816 0.28442388 0.12884288 0.26657617]\n",
            " [0.25068446 0.25837524 0.2712141  0.10491171 0.27294819]\n",
            " [0.27048812 0.2764603  0.30407761 0.1016223  0.29125515]\n",
            " [0.2903262  0.28930518 0.31102265 0.10815631 0.29854269]\n",
            " [0.27514579 0.26980817 0.27271756 0.17060142 0.25062793]\n",
            " [0.24468131 0.26507839 0.25034326 0.17772699 0.28146798]] -> [0.24055914]\n",
            "[[0.28070045 0.27766816 0.28442388 0.12884288 0.26657617]\n",
            " [0.25068446 0.25837524 0.2712141  0.10491171 0.27294819]\n",
            " [0.27048812 0.2764603  0.30407761 0.1016223  0.29125515]\n",
            " [0.2903262  0.28930518 0.31102265 0.10815631 0.29854269]\n",
            " [0.27514579 0.26980817 0.27271756 0.17060142 0.25062793]\n",
            " [0.24468131 0.26507839 0.25034326 0.17772699 0.28146798]\n",
            " [0.26403626 0.25694623 0.25596362 0.22558932 0.24055914]] -> [0.18285742]\n",
            "[[0.25068446 0.25837524 0.2712141  0.10491171 0.27294819]\n",
            " [0.27048812 0.2764603  0.30407761 0.1016223  0.29125515]\n",
            " [0.2903262  0.28930518 0.31102265 0.10815631 0.29854269]\n",
            " [0.27514579 0.26980817 0.27271756 0.17060142 0.25062793]\n",
            " [0.24468131 0.26507839 0.25034326 0.17772699 0.28146798]\n",
            " [0.26403626 0.25694623 0.25596362 0.22558932 0.24055914]\n",
            " [0.21759805 0.23530549 0.20222927 0.27552209 0.18285742]] -> [0.1431455]\n",
            "[[0.27048812 0.2764603  0.30407761 0.1016223  0.29125515]\n",
            " [0.2903262  0.28930518 0.31102265 0.10815631 0.29854269]\n",
            " [0.27514579 0.26980817 0.27271756 0.17060142 0.25062793]\n",
            " [0.24468131 0.26507839 0.25034326 0.17772699 0.28146798]\n",
            " [0.26403626 0.25694623 0.25596362 0.22558932 0.24055914]\n",
            " [0.21759805 0.23530549 0.20222927 0.27552209 0.18285742]\n",
            " [0.17367809 0.18212236 0.16302915 0.23068029 0.1431455 ]] -> [0.15979768]\n",
            "[[0.2903262  0.28930518 0.31102265 0.10815631 0.29854269]\n",
            " [0.27514579 0.26980817 0.27271756 0.17060142 0.25062793]\n",
            " [0.24468131 0.26507839 0.25034326 0.17772699 0.28146798]\n",
            " [0.26403626 0.25694623 0.25596362 0.22558932 0.24055914]\n",
            " [0.21759805 0.23530549 0.20222927 0.27552209 0.18285742]\n",
            " [0.17367809 0.18212236 0.16302915 0.23068029 0.1431455 ]\n",
            " [0.15266704 0.17426238 0.1632798  0.19849422 0.15979768]] -> [0.13195025]\n",
            "[[0.27514579 0.26980817 0.27271756 0.17060142 0.25062793]\n",
            " [0.24468131 0.26507839 0.25034326 0.17772699 0.28146798]\n",
            " [0.26403626 0.25694623 0.25596362 0.22558932 0.24055914]\n",
            " [0.21759805 0.23530549 0.20222927 0.27552209 0.18285742]\n",
            " [0.17367809 0.18212236 0.16302915 0.23068029 0.1431455 ]\n",
            " [0.15266704 0.17426238 0.1632798  0.19849422 0.15979768]\n",
            " [0.12544569 0.12529839 0.11004653 0.33265215 0.13195025]] -> [0.11251682]\n",
            "[[0.24468131 0.26507839 0.25034326 0.17772699 0.28146798]\n",
            " [0.26403626 0.25694623 0.25596362 0.22558932 0.24055914]\n",
            " [0.21759805 0.23530549 0.20222927 0.27552209 0.18285742]\n",
            " [0.17367809 0.18212236 0.16302915 0.23068029 0.1431455 ]\n",
            " [0.15266704 0.17426238 0.1632798  0.19849422 0.15979768]\n",
            " [0.12544569 0.12529839 0.11004653 0.33265215 0.13195025]\n",
            " [0.08400998 0.11383151 0.09823286 0.33168414 0.11251682]] -> [0.06555259]\n",
            "[[0.26403626 0.25694623 0.25596362 0.22558932 0.24055914]\n",
            " [0.21759805 0.23530549 0.20222927 0.27552209 0.18285742]\n",
            " [0.17367809 0.18212236 0.16302915 0.23068029 0.1431455 ]\n",
            " [0.15266704 0.17426238 0.1632798  0.19849422 0.15979768]\n",
            " [0.12544569 0.12529839 0.11004653 0.33265215 0.13195025]\n",
            " [0.08400998 0.11383151 0.09823286 0.33168414 0.11251682]\n",
            " [0.11247335 0.11910566 0.07507078 0.49577844 0.06555259]] -> [0.09959632]\n",
            "[[0.21759805 0.23530549 0.20222927 0.27552209 0.18285742]\n",
            " [0.17367809 0.18212236 0.16302915 0.23068029 0.1431455 ]\n",
            " [0.15266704 0.17426238 0.1632798  0.19849422 0.15979768]\n",
            " [0.12544569 0.12529839 0.11004653 0.33265215 0.13195025]\n",
            " [0.08400998 0.11383151 0.09823286 0.33168414 0.11251682]\n",
            " [0.11247335 0.11910566 0.07507078 0.49577844 0.06555259]\n",
            " [0.05106156 0.0877334  0.07353138 0.2330017  0.09959632]] -> [0.11966344]\n",
            "[[0.17367809 0.18212236 0.16302915 0.23068029 0.1431455 ]\n",
            " [0.15266704 0.17426238 0.1632798  0.19849422 0.15979768]\n",
            " [0.12544569 0.12529839 0.11004653 0.33265215 0.13195025]\n",
            " [0.08400998 0.11383151 0.09823286 0.33168414 0.11251682]\n",
            " [0.11247335 0.11910566 0.07507078 0.49577844 0.06555259]\n",
            " [0.05106156 0.0877334  0.07353138 0.2330017  0.09959632]\n",
            " [0.10536606 0.10484864 0.11294611 0.20868513 0.11966344]] -> [0.14138535]\n",
            "[[0.15266704 0.17426238 0.1632798  0.19849422 0.15979768]\n",
            " [0.12544569 0.12529839 0.11004653 0.33265215 0.13195025]\n",
            " [0.08400998 0.11383151 0.09823286 0.33168414 0.11251682]\n",
            " [0.11247335 0.11910566 0.07507078 0.49577844 0.06555259]\n",
            " [0.05106156 0.0877334  0.07353138 0.2330017  0.09959632]\n",
            " [0.10536606 0.10484864 0.11294611 0.20868513 0.11966344]\n",
            " [0.12158155 0.14911679 0.1476356  0.26093932 0.14138535]] -> [0.18106193]\n",
            "[[0.12544569 0.12529839 0.11004653 0.33265215 0.13195025]\n",
            " [0.08400998 0.11383151 0.09823286 0.33168414 0.11251682]\n",
            " [0.11247335 0.11910566 0.07507078 0.49577844 0.06555259]\n",
            " [0.05106156 0.0877334  0.07353138 0.2330017  0.09959632]\n",
            " [0.10536606 0.10484864 0.11294611 0.20868513 0.11966344]\n",
            " [0.12158155 0.14911679 0.1476356  0.26093932 0.14138535]\n",
            " [0.15411599 0.17436442 0.17287399 0.20981447 0.18106193]] -> [0.16627549]\n",
            "[[0.08400998 0.11383151 0.09823286 0.33168414 0.11251682]\n",
            " [0.11247335 0.11910566 0.07507078 0.49577844 0.06555259]\n",
            " [0.05106156 0.0877334  0.07353138 0.2330017  0.09959632]\n",
            " [0.10536606 0.10484864 0.11294611 0.20868513 0.11966344]\n",
            " [0.12158155 0.14911679 0.1476356  0.26093932 0.14138535]\n",
            " [0.15411599 0.17436442 0.17287399 0.20981447 0.18106193]\n",
            " [0.17150466 0.16640218 0.17265905 0.17614054 0.16627549]] -> [0.16976089]\n",
            "[[0.11247335 0.11910566 0.07507078 0.49577844 0.06555259]\n",
            " [0.05106156 0.0877334  0.07353138 0.2330017  0.09959632]\n",
            " [0.10536606 0.10484864 0.11294611 0.20868513 0.11966344]\n",
            " [0.12158155 0.14911679 0.1476356  0.26093932 0.14138535]\n",
            " [0.15411599 0.17436442 0.17287399 0.20981447 0.18106193]\n",
            " [0.17150466 0.16640218 0.17265905 0.17614054 0.16627549]\n",
            " [0.14621539 0.16480303 0.17709824 0.10553016 0.16976089]] -> [0.19838292]\n",
            "[[0.05106156 0.0877334  0.07353138 0.2330017  0.09959632]\n",
            " [0.10536606 0.10484864 0.11294611 0.20868513 0.11966344]\n",
            " [0.12158155 0.14911679 0.1476356  0.26093932 0.14138535]\n",
            " [0.15411599 0.17436442 0.17287399 0.20981447 0.18106193]\n",
            " [0.17150466 0.16640218 0.17265905 0.17614054 0.16627549]\n",
            " [0.14621539 0.16480303 0.17709824 0.10553016 0.16976089]\n",
            " [0.16681257 0.18035307 0.19352989 0.11320247 0.19838292]] -> [0.19989679]\n",
            "[[0.10536606 0.10484864 0.11294611 0.20868513 0.11966344]\n",
            " [0.12158155 0.14911679 0.1476356  0.26093932 0.14138535]\n",
            " [0.15411599 0.17436442 0.17287399 0.20981447 0.18106193]\n",
            " [0.17150466 0.16640218 0.17265905 0.17614054 0.16627549]\n",
            " [0.14621539 0.16480303 0.17709824 0.10553016 0.16976089]\n",
            " [0.16681257 0.18035307 0.19352989 0.11320247 0.19838292]\n",
            " [0.19096308 0.19808057 0.21271826 0.15798154 0.19989679]] -> [0.20334707]\n",
            "[[0.12158155 0.14911679 0.1476356  0.26093932 0.14138535]\n",
            " [0.15411599 0.17436442 0.17287399 0.20981447 0.18106193]\n",
            " [0.17150466 0.16640218 0.17265905 0.17614054 0.16627549]\n",
            " [0.14621539 0.16480303 0.17709824 0.10553016 0.16976089]\n",
            " [0.16681257 0.18035307 0.19352989 0.11320247 0.19838292]\n",
            " [0.19096308 0.19808057 0.21271826 0.15798154 0.19989679]\n",
            " [0.1873405  0.193351   0.20029593 0.1297571  0.20334707]] -> [0.23422225]\n",
            "[[0.15411599 0.17436442 0.17287399 0.20981447 0.18106193]\n",
            " [0.17150466 0.16640218 0.17265905 0.17614054 0.16627549]\n",
            " [0.14621539 0.16480303 0.17709824 0.10553016 0.16976089]\n",
            " [0.16681257 0.18035307 0.19352989 0.11320247 0.19838292]\n",
            " [0.19096308 0.19808057 0.21271826 0.15798154 0.19989679]\n",
            " [0.1873405  0.193351   0.20029593 0.1297571  0.20334707]\n",
            " [0.22322155 0.2163868  0.24053414 0.18168863 0.23422225]] -> [0.22063294]\n",
            "[[0.17150466 0.16640218 0.17265905 0.17614054 0.16627549]\n",
            " [0.14621539 0.16480303 0.17709824 0.10553016 0.16976089]\n",
            " [0.16681257 0.18035307 0.19352989 0.11320247 0.19838292]\n",
            " [0.19096308 0.19808057 0.21271826 0.15798154 0.19989679]\n",
            " [0.1873405  0.193351   0.20029593 0.1297571  0.20334707]\n",
            " [0.22322155 0.2163868  0.24053414 0.18168863 0.23422225]\n",
            " [0.2099388  0.21070449 0.23509278 0.12317827 0.22063294]] -> [0.216725]\n",
            "[[0.14621539 0.16480303 0.17709824 0.10553016 0.16976089]\n",
            " [0.16681257 0.18035307 0.19352989 0.11320247 0.19838292]\n",
            " [0.19096308 0.19808057 0.21271826 0.15798154 0.19989679]\n",
            " [0.1873405  0.193351   0.20029593 0.1297571  0.20334707]\n",
            " [0.22322155 0.2163868  0.24053414 0.18168863 0.23422225]\n",
            " [0.2099388  0.21070449 0.23509278 0.12317827 0.22063294]\n",
            " [0.20131351 0.20253817 0.22102369 0.11080936 0.216725  ]] -> [0.18789182]\n",
            "[[0.16681257 0.18035307 0.19352989 0.11320247 0.19838292]\n",
            " [0.19096308 0.19808057 0.21271826 0.15798154 0.19989679]\n",
            " [0.1873405  0.193351   0.20029593 0.1297571  0.20334707]\n",
            " [0.22322155 0.2163868  0.24053414 0.18168863 0.23422225]\n",
            " [0.2099388  0.21070449 0.23509278 0.12317827 0.22063294]\n",
            " [0.20131351 0.20253817 0.22102369 0.11080936 0.216725  ]\n",
            " [0.21442396 0.20696161 0.20222927 0.18143766 0.18789182]] -> [0.17423205]\n",
            "[[0.19096308 0.19808057 0.21271826 0.15798154 0.19989679]\n",
            " [0.1873405  0.193351   0.20029593 0.1297571  0.20334707]\n",
            " [0.22322155 0.2163868  0.24053414 0.18168863 0.23422225]\n",
            " [0.2099388  0.21070449 0.23509278 0.12317827 0.22063294]\n",
            " [0.20131351 0.20253817 0.22102369 0.11080936 0.216725  ]\n",
            " [0.21442396 0.20696161 0.20222927 0.18143766 0.18789182]\n",
            " [0.17543765 0.17322446 0.19120304 0.11878641 0.17423205]] -> [0.17060595]\n",
            "[[0.1873405  0.193351   0.20029593 0.1297571  0.20334707]\n",
            " [0.22322155 0.2163868  0.24053414 0.18168863 0.23422225]\n",
            " [0.2099388  0.21070449 0.23509278 0.12317827 0.22063294]\n",
            " [0.20131351 0.20253817 0.22102369 0.11080936 0.216725  ]\n",
            " [0.21442396 0.20696161 0.20222927 0.18143766 0.18789182]\n",
            " [0.17543765 0.17322446 0.19120304 0.11878641 0.17423205]\n",
            " [0.17788727 0.17092767 0.18296925 0.14572018 0.17060595]] -> [0.19341898]\n",
            "[[0.22322155 0.2163868  0.24053414 0.18168863 0.23422225]\n",
            " [0.2099388  0.21070449 0.23509278 0.12317827 0.22063294]\n",
            " [0.20131351 0.20253817 0.22102369 0.11080936 0.216725  ]\n",
            " [0.21442396 0.20696161 0.20222927 0.18143766 0.18789182]\n",
            " [0.17543765 0.17322446 0.19120304 0.11878641 0.17423205]\n",
            " [0.17788727 0.17092767 0.18296925 0.14572018 0.17060595]\n",
            " [0.16149926 0.18242849 0.19138205 0.10098593 0.19341898]] -> [0.20327661]\n",
            "[[0.2099388  0.21070449 0.23509278 0.12317827 0.22063294]\n",
            " [0.20131351 0.20253817 0.22102369 0.11080936 0.216725  ]\n",
            " [0.21442396 0.20696161 0.20222927 0.18143766 0.18789182]\n",
            " [0.17543765 0.17322446 0.19120304 0.11878641 0.17423205]\n",
            " [0.17788727 0.17092767 0.18296925 0.14572018 0.17060595]\n",
            " [0.16149926 0.18242849 0.19138205 0.10098593 0.19341898]\n",
            " [0.18575345 0.19042467 0.21028405 0.08582952 0.20327661]] -> [0.19278529]\n",
            "[[0.20131351 0.20253817 0.22102369 0.11080936 0.216725  ]\n",
            " [0.21442396 0.20696161 0.20222927 0.18143766 0.18789182]\n",
            " [0.17543765 0.17322446 0.19120304 0.11878641 0.17423205]\n",
            " [0.17788727 0.17092767 0.18296925 0.14572018 0.17060595]\n",
            " [0.16149926 0.18242849 0.19138205 0.10098593 0.19341898]\n",
            " [0.18575345 0.19042467 0.21028405 0.08582952 0.20327661]\n",
            " [0.19230878 0.18538877 0.20623858 0.10053778 0.19278529]] -> [0.18599075]\n",
            "[[0.21442396 0.20696161 0.20222927 0.18143766 0.18789182]\n",
            " [0.17543765 0.17322446 0.19120304 0.11878641 0.17423205]\n",
            " [0.17788727 0.17092767 0.18296925 0.14572018 0.17060595]\n",
            " [0.16149926 0.18242849 0.19138205 0.10098593 0.19341898]\n",
            " [0.18575345 0.19042467 0.21028405 0.08582952 0.20327661]\n",
            " [0.19230878 0.18538877 0.20623858 0.10053778 0.19278529]\n",
            " [0.19027324 0.18314321 0.20018856 0.11933315 0.18599075]] -> [0.18254047]\n",
            "[[0.17543765 0.17322446 0.19120304 0.11878641 0.17423205]\n",
            " [0.17788727 0.17092767 0.18296925 0.14572018 0.17060595]\n",
            " [0.16149926 0.18242849 0.19138205 0.10098593 0.19341898]\n",
            " [0.18575345 0.19042467 0.21028405 0.08582952 0.20327661]\n",
            " [0.19230878 0.18538877 0.20623858 0.10053778 0.19278529]\n",
            " [0.19027324 0.18314321 0.20018856 0.11933315 0.18599075]\n",
            " [0.17950894 0.17252703 0.1954273  0.11486959 0.18254047]] -> [0.15476328]\n",
            "[[0.17788727 0.17092767 0.18296925 0.14572018 0.17060595]\n",
            " [0.16149926 0.18242849 0.19138205 0.10098593 0.19341898]\n",
            " [0.18575345 0.19042467 0.21028405 0.08582952 0.20327661]\n",
            " [0.19230878 0.18538877 0.20623858 0.10053778 0.19278529]\n",
            " [0.19027324 0.18314321 0.20018856 0.11933315 0.18599075]\n",
            " [0.17950894 0.17252703 0.1954273  0.11486959 0.18254047]\n",
            " [0.16881348 0.16269325 0.16646946 0.15399301 0.15476328]] -> [0.14955307]\n",
            "[[0.16149926 0.18242849 0.19138205 0.10098593 0.19341898]\n",
            " [0.18575345 0.19042467 0.21028405 0.08582952 0.20327661]\n",
            " [0.19230878 0.18538877 0.20623858 0.10053778 0.19278529]\n",
            " [0.19027324 0.18314321 0.20018856 0.11933315 0.18599075]\n",
            " [0.17950894 0.17252703 0.1954273  0.11486959 0.18254047]\n",
            " [0.16881348 0.16269325 0.16646946 0.15399301 0.15476328]\n",
            " [0.14783686 0.15639847 0.16685968 0.17519943 0.14955307]] -> [0.15645319]\n",
            "[[0.18575345 0.19042467 0.21028405 0.08582952 0.20327661]\n",
            " [0.19230878 0.18538877 0.20623858 0.10053778 0.19278529]\n",
            " [0.19027324 0.18314321 0.20018856 0.11933315 0.18599075]\n",
            " [0.17950894 0.17252703 0.1954273  0.11486959 0.18254047]\n",
            " [0.16881348 0.16269325 0.16646946 0.15399301 0.15476328]\n",
            " [0.14783686 0.15639847 0.16685968 0.17519943 0.14955307]\n",
            " [0.13921157 0.14380871 0.15221787 0.12407457 0.15645319]] -> [0.14884892]\n",
            "[[0.19230878 0.18538877 0.20623858 0.10053778 0.19278529]\n",
            " [0.19027324 0.18314321 0.20018856 0.11933315 0.18599075]\n",
            " [0.17950894 0.17252703 0.1954273  0.11486959 0.18254047]\n",
            " [0.16881348 0.16269325 0.16646946 0.15399301 0.15476328]\n",
            " [0.14783686 0.15639847 0.16685968 0.17519943 0.14955307]\n",
            " [0.13921157 0.14380871 0.15221787 0.12407457 0.15645319]\n",
            " [0.12627385 0.13315859 0.15579775 0.13941024 0.14884892]] -> [0.15824868]\n",
            "[[0.19027324 0.18314321 0.20018856 0.11933315 0.18599075]\n",
            " [0.17950894 0.17252703 0.1954273  0.11486959 0.18254047]\n",
            " [0.16881348 0.16269325 0.16646946 0.15399301 0.15476328]\n",
            " [0.14783686 0.15639847 0.16685968 0.17519943 0.14955307]\n",
            " [0.13921157 0.14380871 0.15221787 0.12407457 0.15645319]\n",
            " [0.12627385 0.13315859 0.15579775 0.13941024 0.14884892]\n",
            " [0.16201681 0.15707903 0.17541555 0.19864659 0.15824868]] -> [0.16448021]\n",
            "[[0.17950894 0.17252703 0.1954273  0.11486959 0.18254047]\n",
            " [0.16881348 0.16269325 0.16646946 0.15399301 0.15476328]\n",
            " [0.14783686 0.15639847 0.16685968 0.17519943 0.14955307]\n",
            " [0.13921157 0.14380871 0.15221787 0.12407457 0.15645319]\n",
            " [0.12627385 0.13315859 0.15579775 0.13941024 0.14884892]\n",
            " [0.16201681 0.15707903 0.17541555 0.19864659 0.15824868]\n",
            " [0.1483544  0.15898452 0.17205061 0.1522273  0.16448021]] -> [0.17085223]\n",
            "[[0.16881348 0.16269325 0.16646946 0.15399301 0.15476328]\n",
            " [0.14783686 0.15639847 0.16685968 0.17519943 0.14955307]\n",
            " [0.13921157 0.14380871 0.15221787 0.12407457 0.15645319]\n",
            " [0.12627385 0.13315859 0.15579775 0.13941024 0.14884892]\n",
            " [0.16201681 0.15707903 0.17541555 0.19864659 0.15824868]\n",
            " [0.1483544  0.15898452 0.17205061 0.1522273  0.16448021]\n",
            " [0.15301207 0.16333986 0.18272583 0.15972035 0.17085223]] -> [0.16835281]\n",
            "[[0.14783686 0.15639847 0.16685968 0.17519943 0.14955307]\n",
            " [0.13921157 0.14380871 0.15221787 0.12407457 0.15645319]\n",
            " [0.12627385 0.13315859 0.15579775 0.13941024 0.14884892]\n",
            " [0.16201681 0.15707903 0.17541555 0.19864659 0.15824868]\n",
            " [0.1483544  0.15898452 0.17205061 0.1522273  0.16448021]\n",
            " [0.15301207 0.16333986 0.18272583 0.15972035 0.17085223]\n",
            " [0.15949835 0.1550715  0.17714118 0.13578919 0.16835281]] -> [0.17349278]\n",
            "[[0.13921157 0.14380871 0.15221787 0.12407457 0.15645319]\n",
            " [0.12627385 0.13315859 0.15579775 0.13941024 0.14884892]\n",
            " [0.16201681 0.15707903 0.17541555 0.19864659 0.15824868]\n",
            " [0.1483544  0.15898452 0.17205061 0.1522273  0.16448021]\n",
            " [0.15301207 0.16333986 0.18272583 0.15972035 0.17085223]\n",
            " [0.15949835 0.1550715  0.17714118 0.13578919 0.16835281]\n",
            " [0.15860115 0.15660255 0.17555884 0.10221386 0.17349278]] -> [0.1452226]\n",
            "[[0.12627385 0.13315859 0.15579775 0.13941024 0.14884892]\n",
            " [0.16201681 0.15707903 0.17541555 0.19864659 0.15824868]\n",
            " [0.1483544  0.15898452 0.17205061 0.1522273  0.16448021]\n",
            " [0.15301207 0.16333986 0.18272583 0.15972035 0.17085223]\n",
            " [0.15949835 0.1550715  0.17714118 0.13578919 0.16835281]\n",
            " [0.15860115 0.15660255 0.17555884 0.10221386 0.17349278]\n",
            " [0.15266704 0.15459502 0.15858996 0.1888859  0.1452226 ]] -> [0.14504657]\n",
            "[[0.16201681 0.15707903 0.17541555 0.19864659 0.15824868]\n",
            " [0.1483544  0.15898452 0.17205061 0.1522273  0.16448021]\n",
            " [0.15301207 0.16333986 0.18272583 0.15972035 0.17085223]\n",
            " [0.15949835 0.1550715  0.17714118 0.13578919 0.16835281]\n",
            " [0.15860115 0.15660255 0.17555884 0.10221386 0.17349278]\n",
            " [0.15266704 0.15459502 0.15858996 0.1888859  0.1452226 ]\n",
            " [0.13407098 0.13448555 0.15121548 0.13612082 0.14504657]] -> [0.13649165]\n",
            "[[0.1483544  0.15898452 0.17205061 0.1522273  0.16448021]\n",
            " [0.15301207 0.16333986 0.18272583 0.15972035 0.17085223]\n",
            " [0.15949835 0.1550715  0.17714118 0.13578919 0.16835281]\n",
            " [0.15860115 0.15660255 0.17555884 0.10221386 0.17349278]\n",
            " [0.15266704 0.15459502 0.15858996 0.1888859  0.1452226 ]\n",
            " [0.13407098 0.13448555 0.15121548 0.13612082 0.14504657]\n",
            " [0.12692928 0.13617321 0.14928236 0.11382988 0.13649165]] -> [0.15757987]\n",
            "[[0.15301207 0.16333986 0.18272583 0.15972035 0.17085223]\n",
            " [0.15949835 0.1550715  0.17714118 0.13578919 0.16835281]\n",
            " [0.15860115 0.15660255 0.17555884 0.10221386 0.17349278]\n",
            " [0.15266704 0.15459502 0.15858996 0.1888859  0.1452226 ]\n",
            " [0.13407098 0.13448555 0.15121548 0.13612082 0.14504657]\n",
            " [0.12692928 0.13617321 0.14928236 0.11382988 0.13649165]\n",
            " [0.12596324 0.14074639 0.14688365 0.12406561 0.15757987]] -> [0.11515716]\n",
            "[[0.15949835 0.1550715  0.17714118 0.13578919 0.16835281]\n",
            " [0.15860115 0.15660255 0.17555884 0.10221386 0.17349278]\n",
            " [0.15266704 0.15459502 0.15858996 0.1888859  0.1452226 ]\n",
            " [0.13407098 0.13448555 0.15121548 0.13612082 0.14504657]\n",
            " [0.12692928 0.13617321 0.14928236 0.11382988 0.13649165]\n",
            " [0.12596324 0.14074639 0.14688365 0.12406561 0.15757987]\n",
            " [0.12541128 0.12560473 0.13145438 0.22924621 0.11515716]] -> [0.12121244]\n",
            "[[0.15860115 0.15660255 0.17555884 0.10221386 0.17349278]\n",
            " [0.15266704 0.15459502 0.15858996 0.1888859  0.1452226 ]\n",
            " [0.13407098 0.13448555 0.15121548 0.13612082 0.14504657]\n",
            " [0.12692928 0.13617321 0.14928236 0.11382988 0.13649165]\n",
            " [0.12596324 0.14074639 0.14688365 0.12406561 0.15757987]\n",
            " [0.12541128 0.12560473 0.13145438 0.22924621 0.11515716]\n",
            " [0.11205927 0.11917375 0.12970027 0.20806668 0.12121244]] -> [0.14370895]\n",
            "[[0.15266704 0.15459502 0.15858996 0.1888859  0.1452226 ]\n",
            " [0.13407098 0.13448555 0.15121548 0.13612082 0.14504657]\n",
            " [0.12692928 0.13617321 0.14928236 0.11382988 0.13649165]\n",
            " [0.12596324 0.14074639 0.14688365 0.12406561 0.15757987]\n",
            " [0.12541128 0.12560473 0.13145438 0.22924621 0.11515716]\n",
            " [0.11205927 0.11917375 0.12970027 0.20806668 0.12121244]\n",
            " [0.09484332 0.13002817 0.11792231 0.16701622 0.14370895]] -> [0.11797353]\n",
            "[[0.13407098 0.13448555 0.15121548 0.13612082 0.14504657]\n",
            " [0.12692928 0.13617321 0.14928236 0.11382988 0.13649165]\n",
            " [0.12596324 0.14074639 0.14688365 0.12406561 0.15757987]\n",
            " [0.12541128 0.12560473 0.13145438 0.22924621 0.11515716]\n",
            " [0.11205927 0.11917375 0.12970027 0.20806668 0.12121244]\n",
            " [0.09484332 0.13002817 0.11792231 0.16701622 0.14370895]\n",
            " [0.1325874  0.13730964 0.13603665 0.15275612 0.11797353]] -> [0.12600055]\n",
            "[[0.12692928 0.13617321 0.14928236 0.11382988 0.13649165]\n",
            " [0.12596324 0.14074639 0.14688365 0.12406561 0.15757987]\n",
            " [0.12541128 0.12560473 0.13145438 0.22924621 0.11515716]\n",
            " [0.11205927 0.11917375 0.12970027 0.20806668 0.12121244]\n",
            " [0.09484332 0.13002817 0.11792231 0.16701622 0.14370895]\n",
            " [0.1325874  0.13730964 0.13603665 0.15275612 0.11797353]\n",
            " [0.114371   0.12910937 0.14154965 0.14366765 0.12600055]] -> [0.09192159]\n",
            "[[0.12596324 0.14074639 0.14688365 0.12406561 0.15757987]\n",
            " [0.12541128 0.12560473 0.13145438 0.22924621 0.11515716]\n",
            " [0.11205927 0.11917375 0.12970027 0.20806668 0.12121244]\n",
            " [0.09484332 0.13002817 0.11792231 0.16701622 0.14370895]\n",
            " [0.1325874  0.13730964 0.13603665 0.15275612 0.11797353]\n",
            " [0.114371   0.12910937 0.14154965 0.14366765 0.12600055]\n",
            " [0.09957004 0.11066715 0.11133528 0.17805862 0.09192159]] -> [0.07481176]\n",
            "[[0.12541128 0.12560473 0.13145438 0.22924621 0.11515716]\n",
            " [0.11205927 0.11917375 0.12970027 0.20806668 0.12121244]\n",
            " [0.09484332 0.13002817 0.11792231 0.16701622 0.14370895]\n",
            " [0.1325874  0.13730964 0.13603665 0.15275612 0.11797353]\n",
            " [0.114371   0.12910937 0.14154965 0.14366765 0.12600055]\n",
            " [0.09957004 0.11066715 0.11133528 0.17805862 0.09192159]\n",
            " [0.09691349 0.09229303 0.09203955 0.25145648 0.07481176]] -> [0.00999833]\n",
            "[[0.11205927 0.11917375 0.12970027 0.20806668 0.12121244]\n",
            " [0.09484332 0.13002817 0.11792231 0.16701622 0.14370895]\n",
            " [0.1325874  0.13730964 0.13603665 0.15275612 0.11797353]\n",
            " [0.114371   0.12910937 0.14154965 0.14366765 0.12600055]\n",
            " [0.09957004 0.11066715 0.11133528 0.17805862 0.09192159]\n",
            " [0.09691349 0.09229303 0.09203955 0.25145648 0.07481176]\n",
            " [0.05834126 0.05809648 0.0051551  0.35460249 0.00999833]] -> [0.0434436]\n",
            "[[0.09484332 0.13002817 0.11792231 0.16701622 0.14370895]\n",
            " [0.1325874  0.13730964 0.13603665 0.15275612 0.11797353]\n",
            " [0.114371   0.12910937 0.14154965 0.14366765 0.12600055]\n",
            " [0.09957004 0.11066715 0.11133528 0.17805862 0.09192159]\n",
            " [0.09691349 0.09229303 0.09203955 0.25145648 0.07481176]\n",
            " [0.05834126 0.05809648 0.0051551  0.35460249 0.00999833]\n",
            " [0.00810774 0.03751064 0.03311417 0.25770368 0.0434436 ]] -> [0.06530621]\n",
            "[[0.1325874  0.13730964 0.13603665 0.15275612 0.11797353]\n",
            " [0.114371   0.12910937 0.14154965 0.14366765 0.12600055]\n",
            " [0.09957004 0.11066715 0.11133528 0.17805862 0.09192159]\n",
            " [0.09691349 0.09229303 0.09203955 0.25145648 0.07481176]\n",
            " [0.05834126 0.05809648 0.0051551  0.35460249 0.00999833]\n",
            " [0.00810774 0.03751064 0.03311417 0.25770368 0.0434436 ]\n",
            " [0.06313692 0.06088662 0.0613597  0.26161154 0.06530621]] -> [0.08378899]\n",
            "[[0.114371   0.12910937 0.14154965 0.14366765 0.12600055]\n",
            " [0.09957004 0.11066715 0.11133528 0.17805862 0.09192159]\n",
            " [0.09691349 0.09229303 0.09203955 0.25145648 0.07481176]\n",
            " [0.05834126 0.05809648 0.0051551  0.35460249 0.00999833]\n",
            " [0.00810774 0.03751064 0.03311417 0.25770368 0.0434436 ]\n",
            " [0.06313692 0.06088662 0.0613597  0.26161154 0.06530621]\n",
            " [0.05816875 0.0739868  0.06927481 0.33004392 0.08378899]] -> [0.1137842]\n",
            "[[0.09957004 0.11066715 0.11133528 0.17805862 0.09192159]\n",
            " [0.09691349 0.09229303 0.09203955 0.25145648 0.07481176]\n",
            " [0.05834126 0.05809648 0.0051551  0.35460249 0.00999833]\n",
            " [0.00810774 0.03751064 0.03311417 0.25770368 0.0434436 ]\n",
            " [0.06313692 0.06088662 0.0613597  0.26161154 0.06530621]\n",
            " [0.05816875 0.0739868  0.06927481 0.33004392 0.08378899]\n",
            " [0.07393574 0.10372576 0.10209911 0.24341669 0.1137842 ]] -> [0.13392177]\n",
            "[[0.09691349 0.09229303 0.09203955 0.25145648 0.07481176]\n",
            " [0.05834126 0.05809648 0.0051551  0.35460249 0.00999833]\n",
            " [0.00810774 0.03751064 0.03311417 0.25770368 0.0434436 ]\n",
            " [0.06313692 0.06088662 0.0613597  0.26161154 0.06530621]\n",
            " [0.05816875 0.0739868  0.06927481 0.33004392 0.08378899]\n",
            " [0.07393574 0.10372576 0.10209911 0.24341669 0.1137842 ]\n",
            " [0.11161077 0.13128704 0.13864986 0.19626244 0.13392177]] -> [0.12751442]\n",
            "[[0.05834126 0.05809648 0.0051551  0.35460249 0.00999833]\n",
            " [0.00810774 0.03751064 0.03311417 0.25770368 0.0434436 ]\n",
            " [0.06313692 0.06088662 0.0613597  0.26161154 0.06530621]\n",
            " [0.05816875 0.0739868  0.06927481 0.33004392 0.08378899]\n",
            " [0.07393574 0.10372576 0.10209911 0.24341669 0.1137842 ]\n",
            " [0.11161077 0.13128704 0.13864986 0.19626244 0.13392177]\n",
            " [0.12372077 0.12175627 0.14126329 0.06256162 0.12751442]] -> [0.14603233]\n",
            "[[0.00810774 0.03751064 0.03311417 0.25770368 0.0434436 ]\n",
            " [0.06313692 0.06088662 0.0613597  0.26161154 0.06530621]\n",
            " [0.05816875 0.0739868  0.06927481 0.33004392 0.08378899]\n",
            " [0.07393574 0.10372576 0.10209911 0.24341669 0.1137842 ]\n",
            " [0.11161077 0.13128704 0.13864986 0.19626244 0.13392177]\n",
            " [0.12372077 0.12175627 0.14126329 0.06256162 0.12751442]\n",
            " [0.11771761 0.13023225 0.14230139 0.09255176 0.14603233]] -> [0.13300647]\n",
            "[[0.06313692 0.06088662 0.0613597  0.26161154 0.06530621]\n",
            " [0.05816875 0.0739868  0.06927481 0.33004392 0.08378899]\n",
            " [0.07393574 0.10372576 0.10209911 0.24341669 0.1137842 ]\n",
            " [0.11161077 0.13128704 0.13864986 0.19626244 0.13392177]\n",
            " [0.12372077 0.12175627 0.14126329 0.06256162 0.12751442]\n",
            " [0.11771761 0.13023225 0.14230139 0.09255176 0.14603233]\n",
            " [0.12951678 0.13441746 0.15197072 0.20351349 0.13300647]] -> [0.13332321]\n",
            "[[0.05816875 0.0739868  0.06927481 0.33004392 0.08378899]\n",
            " [0.07393574 0.10372576 0.10209911 0.24341669 0.1137842 ]\n",
            " [0.11161077 0.13128704 0.13864986 0.19626244 0.13392177]\n",
            " [0.12372077 0.12175627 0.14126329 0.06256162 0.12751442]\n",
            " [0.11771761 0.13023225 0.14230139 0.09255176 0.14603233]\n",
            " [0.12951678 0.13441746 0.15197072 0.20351349 0.13300647]\n",
            " [0.11537146 0.11968418 0.14165701 0.07782558 0.13332321]] -> [0.11917067]\n",
            "[[0.07393574 0.10372576 0.10209911 0.24341669 0.1137842 ]\n",
            " [0.11161077 0.13128704 0.13864986 0.19626244 0.13392177]\n",
            " [0.12372077 0.12175627 0.14126329 0.06256162 0.12751442]\n",
            " [0.11771761 0.13023225 0.14230139 0.09255176 0.14603233]\n",
            " [0.12951678 0.13441746 0.15197072 0.20351349 0.13300647]\n",
            " [0.11537146 0.11968418 0.14165701 0.07782558 0.13332321]\n",
            " [0.12627385 0.12461783 0.13689574 0.12192346 0.11917067]] -> [0.11357304]\n",
            "[[0.11161077 0.13128704 0.13864986 0.19626244 0.13392177]\n",
            " [0.12372077 0.12175627 0.14126329 0.06256162 0.12751442]\n",
            " [0.11771761 0.13023225 0.14230139 0.09255176 0.14603233]\n",
            " [0.12951678 0.13441746 0.15197072 0.20351349 0.13300647]\n",
            " [0.11537146 0.11968418 0.14165701 0.07782558 0.13332321]\n",
            " [0.12627385 0.12461783 0.13689574 0.12192346 0.11917067]\n",
            " [0.11854555 0.12009235 0.13081001 0.1290311  0.11357304]] -> [0.07505804]\n",
            "[[0.12372077 0.12175627 0.14126329 0.06256162 0.12751442]\n",
            " [0.11771761 0.13023225 0.14230139 0.09255176 0.14603233]\n",
            " [0.12951678 0.13441746 0.15197072 0.20351349 0.13300647]\n",
            " [0.11537146 0.11968418 0.14165701 0.07782558 0.13332321]\n",
            " [0.12627385 0.12461783 0.13689574 0.12192346 0.11917067]\n",
            " [0.11854555 0.12009235 0.13081001 0.1290311  0.11357304]\n",
            " [0.09870747 0.09647824 0.09128771 0.18391145 0.07505804]] -> [0.03312832]\n",
            "[[0.11771761 0.13023225 0.14230139 0.09255176 0.14603233]\n",
            " [0.12951678 0.13441746 0.15197072 0.20351349 0.13300647]\n",
            " [0.11537146 0.11968418 0.14165701 0.07782558 0.13332321]\n",
            " [0.12627385 0.12461783 0.13689574 0.12192346 0.11917067]\n",
            " [0.11854555 0.12009235 0.13081001 0.1290311  0.11357304]\n",
            " [0.09870747 0.09647824 0.09128771 0.18391145 0.07505804]\n",
            " [0.07020969 0.06872974 0.048293   0.25920946 0.03312832]] -> [0.03010068]\n",
            "[[0.12951678 0.13441746 0.15197072 0.20351349 0.13300647]\n",
            " [0.11537146 0.11968418 0.14165701 0.07782558 0.13332321]\n",
            " [0.12627385 0.12461783 0.13689574 0.12192346 0.11917067]\n",
            " [0.11854555 0.12009235 0.13081001 0.1290311  0.11357304]\n",
            " [0.09870747 0.09647824 0.09128771 0.18391145 0.07505804]\n",
            " [0.07020969 0.06872974 0.048293   0.25920946 0.03312832]\n",
            " [0.04260879 0.03834083 0.04328108 0.18437752 0.03010068]] -> [0.03566318]\n",
            "[[0.11537146 0.11968418 0.14165701 0.07782558 0.13332321]\n",
            " [0.12627385 0.12461783 0.13689574 0.12192346 0.11917067]\n",
            " [0.11854555 0.12009235 0.13081001 0.1290311  0.11357304]\n",
            " [0.09870747 0.09647824 0.09128771 0.18391145 0.07505804]\n",
            " [0.07020969 0.06872974 0.048293   0.25920946 0.03312832]\n",
            " [0.04260879 0.03834083 0.04328108 0.18437752 0.03010068]\n",
            " [0.0115234  0.02553335 0.01231486 0.29986556 0.03566318]] -> [0.01274435]\n",
            "[[0.12627385 0.12461783 0.13689574 0.12192346 0.11917067]\n",
            " [0.11854555 0.12009235 0.13081001 0.1290311  0.11357304]\n",
            " [0.09870747 0.09647824 0.09128771 0.18391145 0.07505804]\n",
            " [0.07020969 0.06872974 0.048293   0.25920946 0.03312832]\n",
            " [0.04260879 0.03834083 0.04328108 0.18437752 0.03010068]\n",
            " [0.0115234  0.02553335 0.01231486 0.29986556 0.03566318]\n",
            " [0.0348806  0.03043316 0.02588276 0.18494219 0.01274435]] -> [0.]\n",
            "[[0.11854555 0.12009235 0.13081001 0.1290311  0.11357304]\n",
            " [0.09870747 0.09647824 0.09128771 0.18391145 0.07505804]\n",
            " [0.07020969 0.06872974 0.048293   0.25920946 0.03312832]\n",
            " [0.04260879 0.03834083 0.04328108 0.18437752 0.03010068]\n",
            " [0.0115234  0.02553335 0.01231486 0.29986556 0.03566318]\n",
            " [0.0348806  0.03043316 0.02588276 0.18494219 0.01274435]\n",
            " [0.00100056 0.         0.         0.20783365 0.        ]] -> [0.01277958]\n",
            "[[0.09870747 0.09647824 0.09128771 0.18391145 0.07505804]\n",
            " [0.07020969 0.06872974 0.048293   0.25920946 0.03312832]\n",
            " [0.04260879 0.03834083 0.04328108 0.18437752 0.03010068]\n",
            " [0.0115234  0.02553335 0.01231486 0.29986556 0.03566318]\n",
            " [0.0348806  0.03043316 0.02588276 0.18494219 0.01274435]\n",
            " [0.00100056 0.         0.         0.20783365 0.        ]\n",
            " [0.01445593 0.02383205 0.01729096 0.21175047 0.01277958]] -> [0.02929095]\n",
            "[[0.07020969 0.06872974 0.048293   0.25920946 0.03312832]\n",
            " [0.04260879 0.03834083 0.04328108 0.18437752 0.03010068]\n",
            " [0.0115234  0.02553335 0.01231486 0.29986556 0.03566318]\n",
            " [0.0348806  0.03043316 0.02588276 0.18494219 0.01274435]\n",
            " [0.00100056 0.         0.         0.20783365 0.        ]\n",
            " [0.01445593 0.02383205 0.01729096 0.21175047 0.01277958]\n",
            " [0.         0.02468275 0.01947473 0.19786681 0.02929095]] -> [0.03252986]\n",
            "[[0.04260879 0.03834083 0.04328108 0.18437752 0.03010068]\n",
            " [0.0115234  0.02553335 0.01231486 0.29986556 0.03566318]\n",
            " [0.0348806  0.03043316 0.02588276 0.18494219 0.01274435]\n",
            " [0.00100056 0.         0.         0.20783365 0.        ]\n",
            " [0.01445593 0.02383205 0.01729096 0.21175047 0.01277958]\n",
            " [0.         0.02468275 0.01947473 0.19786681 0.02929095]\n",
            " [0.03767514 0.03301921 0.03651515 0.24270861 0.03252986]] -> [0.05467408]\n",
            "[[0.0115234  0.02553335 0.01231486 0.29986556 0.03566318]\n",
            " [0.0348806  0.03043316 0.02588276 0.18494219 0.01274435]\n",
            " [0.00100056 0.         0.         0.20783365 0.        ]\n",
            " [0.01445593 0.02383205 0.01729096 0.21175047 0.01277958]\n",
            " [0.         0.02468275 0.01947473 0.19786681 0.02929095]\n",
            " [0.03767514 0.03301921 0.03651515 0.24270861 0.03252986]\n",
            " [0.01849259 0.04155976 0.04453411 0.2052792  0.05467408]] -> [0.05051987]\n",
            "[[0.0348806  0.03043316 0.02588276 0.18494219 0.01274435]\n",
            " [0.00100056 0.         0.         0.20783365 0.        ]\n",
            " [0.01445593 0.02383205 0.01729096 0.21175047 0.01277958]\n",
            " [0.         0.02468275 0.01947473 0.19786681 0.02929095]\n",
            " [0.03767514 0.03301921 0.03651515 0.24270861 0.03252986]\n",
            " [0.01849259 0.04155976 0.04453411 0.2052792  0.05467408]\n",
            " [0.05640918 0.05622504 0.0660708  0.1993457  0.05051987]] -> [0.08973891]\n",
            "[[0.00100056 0.         0.         0.20783365 0.        ]\n",
            " [0.01445593 0.02383205 0.01729096 0.21175047 0.01277958]\n",
            " [0.         0.02468275 0.01947473 0.19786681 0.02929095]\n",
            " [0.03767514 0.03301921 0.03651515 0.24270861 0.03252986]\n",
            " [0.01849259 0.04155976 0.04453411 0.2052792  0.05467408]\n",
            " [0.05640918 0.05622504 0.0660708  0.1993457  0.05051987]\n",
            " [0.04347125 0.07929489 0.06672952 0.20263512 0.08973891]] -> [0.14729971]\n",
            "[[0.01445593 0.02383205 0.01729096 0.21175047 0.01277958]\n",
            " [0.         0.02468275 0.01947473 0.19786681 0.02929095]\n",
            " [0.03767514 0.03301921 0.03651515 0.24270861 0.03252986]\n",
            " [0.01849259 0.04155976 0.04453411 0.2052792  0.05467408]\n",
            " [0.05640918 0.05622504 0.0660708  0.1993457  0.05051987]\n",
            " [0.04347125 0.07929489 0.06672952 0.20263512 0.08973891]\n",
            " [0.09256622 0.13730964 0.11505845 0.23922201 0.14729971]] -> [0.16687406]\n",
            "[[0.         0.02468275 0.01947473 0.19786681 0.02929095]\n",
            " [0.03767514 0.03301921 0.03651515 0.24270861 0.03252986]\n",
            " [0.01849259 0.04155976 0.04453411 0.2052792  0.05467408]\n",
            " [0.05640918 0.05622504 0.0660708  0.1993457  0.05051987]\n",
            " [0.04347125 0.07929489 0.06672952 0.20263512 0.08973891]\n",
            " [0.09256622 0.13730964 0.11505845 0.23922201 0.14729971]\n",
            " [0.14124711 0.15718107 0.16267114 0.2038003  0.16687406]] -> [0.15018676]\n",
            "[[0.03767514 0.03301921 0.03651515 0.24270861 0.03252986]\n",
            " [0.01849259 0.04155976 0.04453411 0.2052792  0.05467408]\n",
            " [0.05640918 0.05622504 0.0660708  0.1993457  0.05051987]\n",
            " [0.04347125 0.07929489 0.06672952 0.20263512 0.08973891]\n",
            " [0.09256622 0.13730964 0.11505845 0.23922201 0.14729971]\n",
            " [0.14124711 0.15718107 0.16267114 0.2038003  0.16687406]\n",
            " [0.15139061 0.14639476 0.15075011 0.13765349 0.15018676]] -> [0.09181601]\n",
            "[[0.01849259 0.04155976 0.04453411 0.2052792  0.05467408]\n",
            " [0.05640918 0.05622504 0.0660708  0.1993457  0.05051987]\n",
            " [0.04347125 0.07929489 0.06672952 0.20263512 0.08973891]\n",
            " [0.09256622 0.13730964 0.11505845 0.23922201 0.14729971]\n",
            " [0.14124711 0.15718107 0.16267114 0.2038003  0.16687406]\n",
            " [0.15139061 0.14639476 0.15075011 0.13765349 0.15018676]\n",
            " [0.12185753 0.11815291 0.10965259 0.16994712 0.09181601]] -> [0.06143361]\n",
            "[[0.05640918 0.05622504 0.0660708  0.1993457  0.05051987]\n",
            " [0.04347125 0.07929489 0.06672952 0.20263512 0.08973891]\n",
            " [0.09256622 0.13730964 0.11505845 0.23922201 0.14729971]\n",
            " [0.14124711 0.15718107 0.16267114 0.2038003  0.16687406]\n",
            " [0.15139061 0.14639476 0.15075011 0.13765349 0.15018676]\n",
            " [0.12185753 0.11815291 0.10965259 0.16994712 0.09181601]\n",
            " [0.09705138 0.0919186  0.08033324 0.15021063 0.06143361]] -> [0.0637571]\n",
            "[[0.04347125 0.07929489 0.06672952 0.20263512 0.08973891]\n",
            " [0.09256622 0.13730964 0.11505845 0.23922201 0.14729971]\n",
            " [0.14124711 0.15718107 0.16267114 0.2038003  0.16687406]\n",
            " [0.15139061 0.14639476 0.15075011 0.13765349 0.15018676]\n",
            " [0.12185753 0.11815291 0.10965259 0.16994712 0.09181601]\n",
            " [0.09705138 0.0919186  0.08033324 0.15021063 0.06143361]\n",
            " [0.05640918 0.05142738 0.04883001 0.37450928 0.0637571 ]] -> [0.14775758]\n",
            "[[0.09256622 0.13730964 0.11505845 0.23922201 0.14729971]\n",
            " [0.14124711 0.15718107 0.16267114 0.2038003  0.16687406]\n",
            " [0.15139061 0.14639476 0.15075011 0.13765349 0.15018676]\n",
            " [0.12185753 0.11815291 0.10965259 0.16994712 0.09181601]\n",
            " [0.09705138 0.0919186  0.08033324 0.15021063 0.06143361]\n",
            " [0.05640918 0.05142738 0.04883001 0.37450928 0.0637571 ]\n",
            " [0.07317664 0.14935503 0.10009434 0.50178363 0.14775758]] -> [0.12649332]\n",
            "[[0.14124711 0.15718107 0.16267114 0.2038003  0.16687406]\n",
            " [0.15139061 0.14639476 0.15075011 0.13765349 0.15018676]\n",
            " [0.12185753 0.11815291 0.10965259 0.16994712 0.09181601]\n",
            " [0.09705138 0.0919186  0.08033324 0.15021063 0.06143361]\n",
            " [0.05640918 0.05142738 0.04883001 0.37450928 0.0637571 ]\n",
            " [0.07317664 0.14935503 0.10009434 0.50178363 0.14775758]\n",
            " [0.12792973 0.12597895 0.11094155 0.25471901 0.12649332]] -> [0.129169]\n",
            "[[0.15139061 0.14639476 0.15075011 0.13765349 0.15018676]\n",
            " [0.12185753 0.11815291 0.10965259 0.16994712 0.09181601]\n",
            " [0.09705138 0.0919186  0.08033324 0.15021063 0.06143361]\n",
            " [0.05640918 0.05142738 0.04883001 0.37450928 0.0637571 ]\n",
            " [0.07317664 0.14935503 0.10009434 0.50178363 0.14775758]\n",
            " [0.12792973 0.12597895 0.11094155 0.25471901 0.12649332]\n",
            " [0.11506085 0.12734008 0.12780286 0.18201129 0.129169  ]] -> [0.10635575]\n",
            "[[0.12185753 0.11815291 0.10965259 0.16994712 0.09181601]\n",
            " [0.09705138 0.0919186  0.08033324 0.15021063 0.06143361]\n",
            " [0.05640918 0.05142738 0.04883001 0.37450928 0.0637571 ]\n",
            " [0.07317664 0.14935503 0.10009434 0.50178363 0.14775758]\n",
            " [0.12792973 0.12597895 0.11094155 0.25471901 0.12649332]\n",
            " [0.11506085 0.12734008 0.12780286 0.18201129 0.129169  ]\n",
            " [0.11933907 0.12486294 0.12067881 0.14840011 0.10635575]] -> [0.12332488]\n",
            "[[0.09705138 0.0919186  0.08033324 0.15021063 0.06143361]\n",
            " [0.05640918 0.05142738 0.04883001 0.37450928 0.0637571 ]\n",
            " [0.07317664 0.14935503 0.10009434 0.50178363 0.14775758]\n",
            " [0.12792973 0.12597895 0.11094155 0.25471901 0.12649332]\n",
            " [0.11506085 0.12734008 0.12780286 0.18201129 0.129169  ]\n",
            " [0.11933907 0.12486294 0.12067881 0.14840011 0.10635575]\n",
            " [0.10053608 0.11066715 0.12361432 0.16508022 0.12332488]] -> [0.13536519]\n",
            "[[0.05640918 0.05142738 0.04883001 0.37450928 0.0637571 ]\n",
            " [0.07317664 0.14935503 0.10009434 0.50178363 0.14775758]\n",
            " [0.12792973 0.12597895 0.11094155 0.25471901 0.12649332]\n",
            " [0.11506085 0.12734008 0.12780286 0.18201129 0.129169  ]\n",
            " [0.11933907 0.12486294 0.12067881 0.14840011 0.10635575]\n",
            " [0.10053608 0.11066715 0.12361432 0.16508022 0.12332488]\n",
            " [0.11381883 0.14026991 0.13907951 0.15735413 0.13536519]] -> [0.12420506]\n",
            "[[0.07317664 0.14935503 0.10009434 0.50178363 0.14775758]\n",
            " [0.12792973 0.12597895 0.11094155 0.25471901 0.12649332]\n",
            " [0.11506085 0.12734008 0.12780286 0.18201129 0.129169  ]\n",
            " [0.11933907 0.12486294 0.12067881 0.14840011 0.10635575]\n",
            " [0.10053608 0.11066715 0.12361432 0.16508022 0.12332488]\n",
            " [0.11381883 0.14026991 0.13907951 0.15735413 0.13536519]\n",
            " [0.11506085 0.12257635 0.13768341 0.11291566 0.12420506]] -> [0.15627716]\n",
            "[[0.12792973 0.12597895 0.11094155 0.25471901 0.12649332]\n",
            " [0.11506085 0.12734008 0.12780286 0.18201129 0.129169  ]\n",
            " [0.11933907 0.12486294 0.12067881 0.14840011 0.10635575]\n",
            " [0.10053608 0.11066715 0.12361432 0.16508022 0.12332488]\n",
            " [0.11381883 0.14026991 0.13907951 0.15735413 0.13536519]\n",
            " [0.11506085 0.12257635 0.13768341 0.11291566 0.12420506]\n",
            " [0.119546   0.14197132 0.14090528 0.1561262  0.15627716]] -> [0.15286221]\n",
            "[[0.11506085 0.12734008 0.12780286 0.18201129 0.129169  ]\n",
            " [0.11933907 0.12486294 0.12067881 0.14840011 0.10635575]\n",
            " [0.10053608 0.11066715 0.12361432 0.16508022 0.12332488]\n",
            " [0.11381883 0.14026991 0.13907951 0.15735413 0.13536519]\n",
            " [0.11506085 0.12257635 0.13768341 0.11291566 0.12420506]\n",
            " [0.119546   0.14197132 0.14090528 0.1561262  0.15627716]\n",
            " [0.14024665 0.14452321 0.16402431 0.12277494 0.15286221]] -> [0.17736537]\n",
            "[[0.11933907 0.12486294 0.12067881 0.14840011 0.10635575]\n",
            " [0.10053608 0.11066715 0.12361432 0.16508022 0.12332488]\n",
            " [0.11381883 0.14026991 0.13907951 0.15735413 0.13536519]\n",
            " [0.11506085 0.12257635 0.13768341 0.11291566 0.12420506]\n",
            " [0.119546   0.14197132 0.14090528 0.1561262  0.15627716]\n",
            " [0.14024665 0.14452321 0.16402431 0.12277494 0.15286221]\n",
            " [0.14697429 0.1661981  0.16866025 0.14451017 0.17736537]] -> [0.19877033]\n",
            "[[0.10053608 0.11066715 0.12361432 0.16508022 0.12332488]\n",
            " [0.11381883 0.14026991 0.13907951 0.15735413 0.13536519]\n",
            " [0.11506085 0.12257635 0.13768341 0.11291566 0.12420506]\n",
            " [0.119546   0.14197132 0.14090528 0.1561262  0.15627716]\n",
            " [0.14024665 0.14452321 0.16402431 0.12277494 0.15286221]\n",
            " [0.14697429 0.1661981  0.16866025 0.14451017 0.17736537]\n",
            " [0.16801996 0.18351743 0.19893575 0.16961549 0.19877033]] -> [0.17704842]\n",
            "[[0.11381883 0.14026991 0.13907951 0.15735413 0.13536519]\n",
            " [0.11506085 0.12257635 0.13768341 0.11291566 0.12420506]\n",
            " [0.119546   0.14197132 0.14090528 0.1561262  0.15627716]\n",
            " [0.14024665 0.14452321 0.16402431 0.12277494 0.15286221]\n",
            " [0.14697429 0.1661981  0.16866025 0.14451017 0.17736537]\n",
            " [0.16801996 0.18351743 0.19893575 0.16961549 0.19877033]\n",
            " [0.18002627 0.18382356 0.1916327  0.14420543 0.17704842]] -> [0.16599387]\n",
            "[[0.11506085 0.12257635 0.13768341 0.11291566 0.12420506]\n",
            " [0.119546   0.14197132 0.14090528 0.1561262  0.15627716]\n",
            " [0.14024665 0.14452321 0.16402431 0.12277494 0.15286221]\n",
            " [0.14697429 0.1661981  0.16866025 0.14451017 0.17736537]\n",
            " [0.16801996 0.18351743 0.19893575 0.16961549 0.19877033]\n",
            " [0.18002627 0.18382356 0.1916327  0.14420543 0.17704842]\n",
            " [0.16129233 0.16847781 0.17881664 0.12952407 0.16599387]] -> [0.177154]\n",
            "[[0.119546   0.14197132 0.14090528 0.1561262  0.15627716]\n",
            " [0.14024665 0.14452321 0.16402431 0.12277494 0.15286221]\n",
            " [0.14697429 0.1661981  0.16866025 0.14451017 0.17736537]\n",
            " [0.16801996 0.18351743 0.19893575 0.16961549 0.19877033]\n",
            " [0.18002627 0.18382356 0.1916327  0.14420543 0.17704842]\n",
            " [0.16129233 0.16847781 0.17881664 0.12952407 0.16599387]\n",
            " [0.14969988 0.16037959 0.18060648 0.08794479 0.177154  ]] -> [0.16335354]\n",
            "[[0.14024665 0.14452321 0.16402431 0.12277494 0.15286221]\n",
            " [0.14697429 0.1661981  0.16866025 0.14451017 0.17736537]\n",
            " [0.16801996 0.18351743 0.19893575 0.16961549 0.19877033]\n",
            " [0.18002627 0.18382356 0.1916327  0.14420543 0.17704842]\n",
            " [0.16129233 0.16847781 0.17881664 0.12952407 0.16599387]\n",
            " [0.14969988 0.16037959 0.18060648 0.08794479 0.177154  ]\n",
            " [0.16726107 0.16255726 0.17269498 0.12874429 0.16335354]] -> [0.13856876]\n",
            "[[0.14697429 0.1661981  0.16866025 0.14451017 0.17736537]\n",
            " [0.16801996 0.18351743 0.19893575 0.16961549 0.19877033]\n",
            " [0.18002627 0.18382356 0.1916327  0.14420543 0.17704842]\n",
            " [0.16129233 0.16847781 0.17881664 0.12952407 0.16599387]\n",
            " [0.14969988 0.16037959 0.18060648 0.08794479 0.177154  ]\n",
            " [0.16726107 0.16255726 0.17269498 0.12874429 0.16335354]\n",
            " [0.14283416 0.13768053 0.14981937 0.12995429 0.13856876]] -> [0.15328475]\n",
            "[[0.16801996 0.18351743 0.19893575 0.16961549 0.19877033]\n",
            " [0.18002627 0.18382356 0.1916327  0.14420543 0.17704842]\n",
            " [0.16129233 0.16847781 0.17881664 0.12952407 0.16599387]\n",
            " [0.14969988 0.16037959 0.18060648 0.08794479 0.177154  ]\n",
            " [0.16726107 0.16255726 0.17269498 0.12874429 0.16335354]\n",
            " [0.14283416 0.13768053 0.14981937 0.12995429 0.13856876]\n",
            " [0.1219612  0.13887484 0.14566655 0.0893699  0.15328475]] -> [0.18067474]\n",
            "[[0.18002627 0.18382356 0.1916327  0.14420543 0.17704842]\n",
            " [0.16129233 0.16847781 0.17881664 0.12952407 0.16599387]\n",
            " [0.14969988 0.16037959 0.18060648 0.08794479 0.177154  ]\n",
            " [0.16726107 0.16255726 0.17269498 0.12874429 0.16335354]\n",
            " [0.14283416 0.13768053 0.14981937 0.12995429 0.13856876]\n",
            " [0.1219612  0.13887484 0.14566655 0.0893699  0.15328475]\n",
            " [0.14231682 0.17096182 0.17142398 0.16294703 0.18067474]] -> [0.22154824]\n",
            "[[0.16129233 0.16847781 0.17881664 0.12952407 0.16599387]\n",
            " [0.14969988 0.16037959 0.18060648 0.08794479 0.177154  ]\n",
            " [0.16726107 0.16255726 0.17269498 0.12874429 0.16335354]\n",
            " [0.14283416 0.13768053 0.14981937 0.12995429 0.13856876]\n",
            " [0.1219612  0.13887484 0.14566655 0.0893699  0.15328475]\n",
            " [0.14231682 0.17096182 0.17142398 0.16294703 0.18067474]\n",
            " [0.16753705 0.20471584 0.19310045 0.20647127 0.22154824]] -> [0.23182819]\n",
            "[[0.14969988 0.16037959 0.18060648 0.08794479 0.177154  ]\n",
            " [0.16726107 0.16255726 0.17269498 0.12874429 0.16335354]\n",
            " [0.14283416 0.13768053 0.14981937 0.12995429 0.13856876]\n",
            " [0.1219612  0.13887484 0.14566655 0.0893699  0.15328475]\n",
            " [0.14231682 0.17096182 0.17142398 0.16294703 0.18067474]\n",
            " [0.16753705 0.20471584 0.19310045 0.20647127 0.22154824]\n",
            " [0.20559153 0.23387627 0.23391139 0.21530877 0.23182819]] -> [0.27738423]\n",
            "[[0.16726107 0.16255726 0.17269498 0.12874429 0.16335354]\n",
            " [0.14283416 0.13768053 0.14981937 0.12995429 0.13856876]\n",
            " [0.1219612  0.13887484 0.14566655 0.0893699  0.15328475]\n",
            " [0.14231682 0.17096182 0.17142398 0.16294703 0.18067474]\n",
            " [0.16753705 0.20471584 0.19310045 0.20647127 0.22154824]\n",
            " [0.20559153 0.23387627 0.23391139 0.21530877 0.23182819]\n",
            " [0.22729284 0.25919179 0.25485388 0.19016761 0.27738423]] -> [0.28548149]\n",
            "[[0.14283416 0.13768053 0.14981937 0.12995429 0.13856876]\n",
            " [0.1219612  0.13887484 0.14566655 0.0893699  0.15328475]\n",
            " [0.14231682 0.17096182 0.17142398 0.16294703 0.18067474]\n",
            " [0.16753705 0.20471584 0.19310045 0.20647127 0.22154824]\n",
            " [0.20559153 0.23387627 0.23391139 0.21530877 0.23182819]\n",
            " [0.22729284 0.25919179 0.25485388 0.19016761 0.27738423]\n",
            " [0.2615178  0.27021634 0.28266976 0.15208389 0.28548149]] -> [0.28453085]\n",
            "[[0.1219612  0.13887484 0.14566655 0.0893699  0.15328475]\n",
            " [0.14231682 0.17096182 0.17142398 0.16294703 0.18067474]\n",
            " [0.16753705 0.20471584 0.19310045 0.20647127 0.22154824]\n",
            " [0.20559153 0.23387627 0.23391139 0.21530877 0.23182819]\n",
            " [0.22729284 0.25919179 0.25485388 0.19016761 0.27738423]\n",
            " [0.2615178  0.27021634 0.28266976 0.15208389 0.28548149]\n",
            " [0.26641704 0.276069   0.28800376 0.16750919 0.28453085]] -> [0.29143119]\n",
            "[[0.14231682 0.17096182 0.17142398 0.16294703 0.18067474]\n",
            " [0.16753705 0.20471584 0.19310045 0.20647127 0.22154824]\n",
            " [0.20559153 0.23387627 0.23391139 0.21530877 0.23182819]\n",
            " [0.22729284 0.25919179 0.25485388 0.19016761 0.27738423]\n",
            " [0.2615178  0.27021634 0.28266976 0.15208389 0.28548149]\n",
            " [0.26641704 0.276069   0.28800376 0.16750919 0.28453085]\n",
            " [0.2772848  0.27879104 0.30733541 0.12384153 0.29143119]] -> [0.26451652]\n",
            "[[0.16753705 0.20471584 0.19310045 0.20647127 0.22154824]\n",
            " [0.20559153 0.23387627 0.23391139 0.21530877 0.23182819]\n",
            " [0.22729284 0.25919179 0.25485388 0.19016761 0.27738423]\n",
            " [0.2615178  0.27021634 0.28266976 0.15208389 0.28548149]\n",
            " [0.26641704 0.276069   0.28800376 0.16750919 0.28453085]\n",
            " [0.2772848  0.27879104 0.30733541 0.12384153 0.29143119]\n",
            " [0.27680167 0.27460584 0.28352885 0.14799677 0.26451652]] -> [0.26861794]\n",
            "[[0.20559153 0.23387627 0.23391139 0.21530877 0.23182819]\n",
            " [0.22729284 0.25919179 0.25485388 0.19016761 0.27738423]\n",
            " [0.2615178  0.27021634 0.28266976 0.15208389 0.28548149]\n",
            " [0.26641704 0.276069   0.28800376 0.16750919 0.28453085]\n",
            " [0.2772848  0.27879104 0.30733541 0.12384153 0.29143119]\n",
            " [0.27680167 0.27460584 0.28352885 0.14799677 0.26451652]\n",
            " [0.24913183 0.25279497 0.2719838  0.09448777 0.26861794]] -> [0.21989367]\n",
            "[[0.22729284 0.25919179 0.25485388 0.19016761 0.27738423]\n",
            " [0.2615178  0.27021634 0.28266976 0.15208389 0.28548149]\n",
            " [0.26641704 0.276069   0.28800376 0.16750919 0.28453085]\n",
            " [0.2772848  0.27879104 0.30733541 0.12384153 0.29143119]\n",
            " [0.27680167 0.27460584 0.28352885 0.14799677 0.26451652]\n",
            " [0.24913183 0.25279497 0.2719838  0.09448777 0.26861794]\n",
            " [0.2401271  0.23435275 0.24046271 0.15993547 0.21989367]] -> [0.20640994]\n",
            "[[0.2615178  0.27021634 0.28266976 0.15208389 0.28548149]\n",
            " [0.26641704 0.276069   0.28800376 0.16750919 0.28453085]\n",
            " [0.2772848  0.27879104 0.30733541 0.12384153 0.29143119]\n",
            " [0.27680167 0.27460584 0.28352885 0.14799677 0.26451652]\n",
            " [0.24913183 0.25279497 0.2719838  0.09448777 0.26861794]\n",
            " [0.2401271  0.23435275 0.24046271 0.15993547 0.21989367]\n",
            " [0.20869678 0.21152104 0.22596396 0.16248095 0.20640994]] -> [0.22165382]\n",
            "[[0.26641704 0.276069   0.28800376 0.16750919 0.28453085]\n",
            " [0.2772848  0.27879104 0.30733541 0.12384153 0.29143119]\n",
            " [0.27680167 0.27460584 0.28352885 0.14799677 0.26451652]\n",
            " [0.24913183 0.25279497 0.2719838  0.09448777 0.26861794]\n",
            " [0.2401271  0.23435275 0.24046271 0.15993547 0.21989367]\n",
            " [0.20869678 0.21152104 0.22596396 0.16248095 0.20640994]\n",
            " [0.20307307 0.20549844 0.2251763  0.12384153 0.22165382]] -> [0.19282063]\n",
            "[[0.2772848  0.27879104 0.30733541 0.12384153 0.29143119]\n",
            " [0.27680167 0.27460584 0.28352885 0.14799677 0.26451652]\n",
            " [0.24913183 0.25279497 0.2719838  0.09448777 0.26861794]\n",
            " [0.2401271  0.23435275 0.24046271 0.15993547 0.21989367]\n",
            " [0.20869678 0.21152104 0.22596396 0.16248095 0.20640994]\n",
            " [0.20307307 0.20549844 0.2251763  0.12384153 0.22165382]\n",
            " [0.20303844 0.21240568 0.20283771 0.15197634 0.19282063]] -> [0.2181333]\n",
            "[[0.27680167 0.27460584 0.28352885 0.14799677 0.26451652]\n",
            " [0.24913183 0.25279497 0.2719838  0.09448777 0.26861794]\n",
            " [0.2401271  0.23435275 0.24046271 0.15993547 0.21989367]\n",
            " [0.20869678 0.21152104 0.22596396 0.16248095 0.20640994]\n",
            " [0.20307307 0.20549844 0.2251763  0.12384153 0.22165382]\n",
            " [0.20303844 0.21240568 0.20283771 0.15197634 0.19282063]\n",
            " [0.19424085 0.20713175 0.20921002 0.14636551 0.2181333 ]] -> [0.2052128]\n",
            "[[0.24913183 0.25279497 0.2719838  0.09448777 0.26861794]\n",
            " [0.2401271  0.23435275 0.24046271 0.15993547 0.21989367]\n",
            " [0.20869678 0.21152104 0.22596396 0.16248095 0.20640994]\n",
            " [0.20307307 0.20549844 0.2251763  0.12384153 0.22165382]\n",
            " [0.20303844 0.21240568 0.20283771 0.15197634 0.19282063]\n",
            " [0.19424085 0.20713175 0.20921002 0.14636551 0.2181333 ]\n",
            " [0.19686278 0.1967536  0.21636978 0.16111858 0.2052128 ]] -> [0.23570078]\n",
            "[[0.2401271  0.23435275 0.24046271 0.15993547 0.21989367]\n",
            " [0.20869678 0.21152104 0.22596396 0.16248095 0.20640994]\n",
            " [0.20307307 0.20549844 0.2251763  0.12384153 0.22165382]\n",
            " [0.20303844 0.21240568 0.20283771 0.15197634 0.19282063]\n",
            " [0.19424085 0.20713175 0.20921002 0.14636551 0.2181333 ]\n",
            " [0.19686278 0.1967536  0.21636978 0.16111858 0.2052128 ]\n",
            " [0.19958837 0.21710131 0.2127899  0.19060679 0.23570078]] -> [0.23038477]\n",
            "[[0.20869678 0.21152104 0.22596396 0.16248095 0.20640994]\n",
            " [0.20307307 0.20549844 0.2251763  0.12384153 0.22165382]\n",
            " [0.20303844 0.21240568 0.20283771 0.15197634 0.19282063]\n",
            " [0.19424085 0.20713175 0.20921002 0.14636551 0.2181333 ]\n",
            " [0.19686278 0.1967536  0.21636978 0.16111858 0.2052128 ]\n",
            " [0.19958837 0.21710131 0.2127899  0.19060679 0.23570078]\n",
            " [0.22335965 0.22057201 0.24552831 0.10659676 0.23038477]] -> [0.23872853]\n",
            "[[0.20307307 0.20549844 0.2251763  0.12384153 0.22165382]\n",
            " [0.20303844 0.21240568 0.20283771 0.15197634 0.19282063]\n",
            " [0.19424085 0.20713175 0.20921002 0.14636551 0.2181333 ]\n",
            " [0.19686278 0.1967536  0.21636978 0.16111858 0.2052128 ]\n",
            " [0.19958837 0.21710131 0.2127899  0.19060679 0.23570078]\n",
            " [0.22335965 0.22057201 0.24552831 0.10659676 0.23038477]\n",
            " [0.23115678 0.22370242 0.25592791 0.23383526 0.23872853]] -> [0.2332716]\n",
            "[[0.20303844 0.21240568 0.20283771 0.15197634 0.19282063]\n",
            " [0.19424085 0.20713175 0.20921002 0.14636551 0.2181333 ]\n",
            " [0.19686278 0.1967536  0.21636978 0.16111858 0.2052128 ]\n",
            " [0.19958837 0.21710131 0.2127899  0.19060679 0.23570078]\n",
            " [0.22335965 0.22057201 0.24552831 0.10659676 0.23038477]\n",
            " [0.23115678 0.22370242 0.25592791 0.23383526 0.23872853]\n",
            " [0.22694782 0.22588009 0.2444006  0.14662544 0.2332716 ]] -> [0.2733356]\n",
            "[[0.19424085 0.20713175 0.20921002 0.14636551 0.2181333 ]\n",
            " [0.19686278 0.1967536  0.21636978 0.16111858 0.2052128 ]\n",
            " [0.19958837 0.21710131 0.2127899  0.19060679 0.23570078]\n",
            " [0.22335965 0.22057201 0.24552831 0.10659676 0.23038477]\n",
            " [0.23115678 0.22370242 0.25592791 0.23383526 0.23872853]\n",
            " [0.22694782 0.22588009 0.2444006  0.14662544 0.2332716 ]\n",
            " [0.23429646 0.2674943  0.2636604  0.2308237  0.2733356 ]] -> [0.23318369]\n",
            "[[0.19686278 0.1967536  0.21636978 0.16111858 0.2052128 ]\n",
            " [0.19958837 0.21710131 0.2127899  0.19060679 0.23570078]\n",
            " [0.22335965 0.22057201 0.24552831 0.10659676 0.23038477]\n",
            " [0.23115678 0.22370242 0.25592791 0.23383526 0.23872853]\n",
            " [0.22694782 0.22588009 0.2444006  0.14662544 0.2332716 ]\n",
            " [0.23429646 0.2674943  0.2636604  0.2308237  0.2733356 ]\n",
            " [0.26169032 0.25956622 0.25481816 0.19219324 0.23318369]] -> [0.2204569]\n",
            "[[0.19958837 0.21710131 0.2127899  0.19060679 0.23570078]\n",
            " [0.22335965 0.22057201 0.24552831 0.10659676 0.23038477]\n",
            " [0.23115678 0.22370242 0.25592791 0.23383526 0.23872853]\n",
            " [0.22694782 0.22588009 0.2444006  0.14662544 0.2332716 ]\n",
            " [0.23429646 0.2674943  0.2636604  0.2308237  0.2733356 ]\n",
            " [0.26169032 0.25956622 0.25481816 0.19219324 0.23318369]\n",
            " [0.21714956 0.21410709 0.22585661 0.14024379 0.2204569 ]] -> [0.19641161]\n",
            "[[0.22335965 0.22057201 0.24552831 0.10659676 0.23038477]\n",
            " [0.23115678 0.22370242 0.25592791 0.23383526 0.23872853]\n",
            " [0.22694782 0.22588009 0.2444006  0.14662544 0.2332716 ]\n",
            " [0.23429646 0.2674943  0.2636604  0.2308237  0.2733356 ]\n",
            " [0.26169032 0.25956622 0.25481816 0.19219324 0.23318369]\n",
            " [0.21714956 0.21410709 0.22585661 0.14024379 0.2204569 ]\n",
            " [0.20131351 0.20178951 0.21683515 0.16935556 0.19641161]] -> [0.20940235]\n",
            "[[0.23115678 0.22370242 0.25592791 0.23383526 0.23872853]\n",
            " [0.22694782 0.22588009 0.2444006  0.14662544 0.2332716 ]\n",
            " [0.23429646 0.2674943  0.2636604  0.2308237  0.2733356 ]\n",
            " [0.26169032 0.25956622 0.25481816 0.19219324 0.23318369]\n",
            " [0.21714956 0.21410709 0.22585661 0.14024379 0.2204569 ]\n",
            " [0.20131351 0.20178951 0.21683515 0.16935556 0.19641161]\n",
            " [0.19655237 0.19563072 0.21697844 0.11469033 0.20940235]] -> [0.19521447]\n",
            "[[0.22694782 0.22588009 0.2444006  0.14662544 0.2332716 ]\n",
            " [0.23429646 0.2674943  0.2636604  0.2308237  0.2733356 ]\n",
            " [0.26169032 0.25956622 0.25481816 0.19219324 0.23318369]\n",
            " [0.21714956 0.21410709 0.22585661 0.14024379 0.2204569 ]\n",
            " [0.20131351 0.20178951 0.21683515 0.16935556 0.19641161]\n",
            " [0.19655237 0.19563072 0.21697844 0.11469033 0.20940235]\n",
            " [0.19096308 0.19985007 0.21178752 0.14162409 0.19521447]] -> [0.17606266]\n",
            "[[0.23429646 0.2674943  0.2636604  0.2308237  0.2733356 ]\n",
            " [0.26169032 0.25956622 0.25481816 0.19219324 0.23318369]\n",
            " [0.21714956 0.21410709 0.22585661 0.14024379 0.2204569 ]\n",
            " [0.20131351 0.20178951 0.21683515 0.16935556 0.19641161]\n",
            " [0.19655237 0.19563072 0.21697844 0.11469033 0.20940235]\n",
            " [0.19096308 0.19985007 0.21178752 0.14162409 0.19521447]\n",
            " [0.18613311 0.18770263 0.18594069 0.17523528 0.17606266]] -> [0.15131322]\n",
            "[[0.26169032 0.25956622 0.25481816 0.19219324 0.23318369]\n",
            " [0.21714956 0.21410709 0.22585661 0.14024379 0.2204569 ]\n",
            " [0.20131351 0.20178951 0.21683515 0.16935556 0.19641161]\n",
            " [0.19655237 0.19563072 0.21697844 0.11469033 0.20940235]\n",
            " [0.19096308 0.19985007 0.21178752 0.14162409 0.19521447]\n",
            " [0.18613311 0.18770263 0.18594069 0.17523528 0.17606266]\n",
            " [0.15939468 0.15268954 0.16570327 0.1531236  0.15131322]] -> [0.15566113]\n",
            "[[0.21714956 0.21410709 0.22585661 0.14024379 0.2204569 ]\n",
            " [0.20131351 0.20178951 0.21683515 0.16935556 0.19641161]\n",
            " [0.19655237 0.19563072 0.21697844 0.11469033 0.20940235]\n",
            " [0.19096308 0.19985007 0.21178752 0.14162409 0.19521447]\n",
            " [0.18613311 0.18770263 0.18594069 0.17523528 0.17606266]\n",
            " [0.15939468 0.15268954 0.16570327 0.1531236  0.15131322]\n",
            " [0.12962046 0.14438723 0.1503921  0.11799767 0.15566113]] -> [0.15655877]\n",
            "[[0.20131351 0.20178951 0.21683515 0.16935556 0.19641161]\n",
            " [0.19655237 0.19563072 0.21697844 0.11469033 0.20940235]\n",
            " [0.19096308 0.19985007 0.21178752 0.14162409 0.19521447]\n",
            " [0.18613311 0.18770263 0.18594069 0.17523528 0.17606266]\n",
            " [0.15939468 0.15268954 0.16570327 0.1531236  0.15131322]\n",
            " [0.12962046 0.14438723 0.1503921  0.11799767 0.15566113]\n",
            " [0.14983798 0.15895037 0.173411   0.11606167 0.15655877]] -> [0.17271818]\n",
            "[[0.19655237 0.19563072 0.21697844 0.11469033 0.20940235]\n",
            " [0.19096308 0.19985007 0.21178752 0.14162409 0.19521447]\n",
            " [0.18613311 0.18770263 0.18594069 0.17523528 0.17606266]\n",
            " [0.15939468 0.15268954 0.16570327 0.1531236  0.15131322]\n",
            " [0.12962046 0.14438723 0.1503921  0.11799767 0.15566113]\n",
            " [0.14983798 0.15895037 0.173411   0.11606167 0.15655877]\n",
            " [0.15087306 0.16289754 0.18193115 0.10492068 0.17271818]] -> [0.16979622]\n",
            "[[0.19096308 0.19985007 0.21178752 0.14162409 0.19521447]\n",
            " [0.18613311 0.18770263 0.18594069 0.17523528 0.17606266]\n",
            " [0.15939468 0.15268954 0.16570327 0.1531236  0.15131322]\n",
            " [0.12962046 0.14438723 0.1503921  0.11799767 0.15566113]\n",
            " [0.14983798 0.15895037 0.173411   0.11606167 0.15655877]\n",
            " [0.15087306 0.16289754 0.18193115 0.10492068 0.17271818]\n",
            " [0.16001568 0.15643241 0.17158524 0.13891727 0.16979622]] -> [0.16708521]\n",
            "[[0.18613311 0.18770263 0.18594069 0.17523528 0.17606266]\n",
            " [0.15939468 0.15268954 0.16570327 0.1531236  0.15131322]\n",
            " [0.12962046 0.14438723 0.1503921  0.11799767 0.15566113]\n",
            " [0.14983798 0.15895037 0.173411   0.11606167 0.15655877]\n",
            " [0.15087306 0.16289754 0.18193115 0.10492068 0.17271818]\n",
            " [0.16001568 0.15643241 0.17158524 0.13891727 0.16979622]\n",
            " [0.16436296 0.15758945 0.17810062 0.12561621 0.16708521]] -> [0.16412814]\n",
            "[[0.15939468 0.15268954 0.16570327 0.1531236  0.15131322]\n",
            " [0.12962046 0.14438723 0.1503921  0.11799767 0.15566113]\n",
            " [0.14983798 0.15895037 0.173411   0.11606167 0.15655877]\n",
            " [0.15087306 0.16289754 0.18193115 0.10492068 0.17271818]\n",
            " [0.16001568 0.15643241 0.17158524 0.13891727 0.16979622]\n",
            " [0.16436296 0.15758945 0.17810062 0.12561621 0.16708521]\n",
            " [0.15097653 0.16361205 0.17810062 0.14675988 0.16412814]] -> [0.13321763]\n",
            "[[0.12962046 0.14438723 0.1503921  0.11799767 0.15566113]\n",
            " [0.14983798 0.15895037 0.173411   0.11606167 0.15655877]\n",
            " [0.15087306 0.16289754 0.18193115 0.10492068 0.17271818]\n",
            " [0.16001568 0.15643241 0.17158524 0.13891727 0.16979622]\n",
            " [0.16436296 0.15758945 0.17810062 0.12561621 0.16708521]\n",
            " [0.15097653 0.16361205 0.17810062 0.14675988 0.16412814]\n",
            " [0.14352421 0.14152899 0.14510105 0.23269696 0.13321763]] -> [0.14075166]\n",
            "[[0.14983798 0.15895037 0.173411   0.11606167 0.15655877]\n",
            " [0.15087306 0.16289754 0.18193115 0.10492068 0.17271818]\n",
            " [0.16001568 0.15643241 0.17158524 0.13891727 0.16979622]\n",
            " [0.16436296 0.15758945 0.17810062 0.12561621 0.16708521]\n",
            " [0.15097653 0.16361205 0.17810062 0.14675988 0.16412814]\n",
            " [0.14352421 0.14152899 0.14510105 0.23269696 0.13321763]\n",
            " [0.11747605 0.13186556 0.12765957 0.20712557 0.14075166]] -> [0.1452226]\n",
            "[[0.15087306 0.16289754 0.18193115 0.10492068 0.17271818]\n",
            " [0.16001568 0.15643241 0.17158524 0.13891727 0.16979622]\n",
            " [0.16436296 0.15758945 0.17810062 0.12561621 0.16708521]\n",
            " [0.15097653 0.16361205 0.17810062 0.14675988 0.16412814]\n",
            " [0.14352421 0.14152899 0.14510105 0.23269696 0.13321763]\n",
            " [0.11747605 0.13186556 0.12765957 0.20712557 0.14075166]\n",
            " [0.12161618 0.13479168 0.15053518 0.11579278 0.1452226 ]] -> [0.11089737]\n",
            "[[0.16001568 0.15643241 0.17158524 0.13891727 0.16979622]\n",
            " [0.16436296 0.15758945 0.17810062 0.12561621 0.16708521]\n",
            " [0.15097653 0.16361205 0.17810062 0.14675988 0.16412814]\n",
            " [0.14352421 0.14152899 0.14510105 0.23269696 0.13321763]\n",
            " [0.11747605 0.13186556 0.12765957 0.20712557 0.14075166]\n",
            " [0.12161618 0.13479168 0.15053518 0.11579278 0.1452226 ]\n",
            " [0.11733795 0.11522658 0.11974807 0.19215739 0.11089737]] -> [0.15078511]\n",
            "[[0.16436296 0.15758945 0.17810062 0.12561621 0.16708521]\n",
            " [0.15097653 0.16361205 0.17810062 0.14675988 0.16412814]\n",
            " [0.14352421 0.14152899 0.14510105 0.23269696 0.13321763]\n",
            " [0.11747605 0.13186556 0.12765957 0.20712557 0.14075166]\n",
            " [0.12161618 0.13479168 0.15053518 0.11579278 0.1452226 ]\n",
            " [0.11733795 0.11522658 0.11974807 0.19215739 0.11089737]\n",
            " [0.10678059 0.13649309 0.13224183 0.14979833 0.15078511]] -> [0.14582117]\n",
            "[[0.15097653 0.16361205 0.17810062 0.14675988 0.16412814]\n",
            " [0.14352421 0.14152899 0.14510105 0.23269696 0.13321763]\n",
            " [0.11747605 0.13186556 0.12765957 0.20712557 0.14075166]\n",
            " [0.12161618 0.13479168 0.15053518 0.11579278 0.1452226 ]\n",
            " [0.11733795 0.11522658 0.11974807 0.19215739 0.11089737]\n",
            " [0.10678059 0.13649309 0.13224183 0.14979833 0.15078511]\n",
            " [0.14787148 0.14772173 0.1650876  0.16463207 0.14582117]] -> [0.16481462]\n",
            "[[0.14352421 0.14152899 0.14510105 0.23269696 0.13321763]\n",
            " [0.11747605 0.13186556 0.12765957 0.20712557 0.14075166]\n",
            " [0.12161618 0.13479168 0.15053518 0.11579278 0.1452226 ]\n",
            " [0.11733795 0.11522658 0.11974807 0.19215739 0.11089737]\n",
            " [0.10678059 0.13649309 0.13224183 0.14979833 0.15078511]\n",
            " [0.14787148 0.14772173 0.1650876  0.16463207 0.14582117]\n",
            " [0.13714161 0.15347214 0.15819624 0.14211706 0.16481462]] -> [0.19169395]\n",
            "[[0.11747605 0.13186556 0.12765957 0.20712557 0.14075166]\n",
            " [0.12161618 0.13479168 0.15053518 0.11579278 0.1452226 ]\n",
            " [0.11733795 0.11522658 0.11974807 0.19215739 0.11089737]\n",
            " [0.10678059 0.13649309 0.13224183 0.14979833 0.15078511]\n",
            " [0.14787148 0.14772173 0.1650876  0.16463207 0.14582117]\n",
            " [0.13714161 0.15347214 0.15819624 0.14211706 0.16481462]\n",
            " [0.15991222 0.18709017 0.1885539  0.37437483 0.19169395]] -> [0.25527513]\n",
            "[[0.12161618 0.13479168 0.15053518 0.11579278 0.1452226 ]\n",
            " [0.11733795 0.11522658 0.11974807 0.19215739 0.11089737]\n",
            " [0.10678059 0.13649309 0.13224183 0.14979833 0.15078511]\n",
            " [0.14787148 0.14772173 0.1650876  0.16463207 0.14582117]\n",
            " [0.13714161 0.15347214 0.15819624 0.14211706 0.16481462]\n",
            " [0.15991222 0.18709017 0.1885539  0.37437483 0.19169395]\n",
            " [0.24650991 0.25575524 0.24948395 0.44139105 0.25527513]] -> [0.22115211]\n",
            "[[0.11733795 0.11522658 0.11974807 0.19215739 0.11089737]\n",
            " [0.10678059 0.13649309 0.13224183 0.14979833 0.15078511]\n",
            " [0.14787148 0.14772173 0.1650876  0.16463207 0.14582117]\n",
            " [0.13714161 0.15347214 0.15819624 0.14211706 0.16481462]\n",
            " [0.15991222 0.18709017 0.1885539  0.37437483 0.19169395]\n",
            " [0.24650991 0.25575524 0.24948395 0.44139105 0.25527513]\n",
            " [0.23715131 0.23808691 0.23497644 0.21422425 0.22115211]] -> [0.2152024]\n",
            "[[0.10678059 0.13649309 0.13224183 0.14979833 0.15078511]\n",
            " [0.14787148 0.14772173 0.1650876  0.16463207 0.14582117]\n",
            " [0.13714161 0.15347214 0.15819624 0.14211706 0.16481462]\n",
            " [0.15991222 0.18709017 0.1885539  0.37437483 0.19169395]\n",
            " [0.24650991 0.25575524 0.24948395 0.44139105 0.25527513]\n",
            " [0.23715131 0.23808691 0.23497644 0.21422425 0.22115211]\n",
            " [0.20696305 0.20429899 0.22483108 0.13293    0.2152024 ]] -> [0.19900802]\n",
            "[[0.14787148 0.14772173 0.1650876  0.16463207 0.14582117]\n",
            " [0.13714161 0.15347214 0.15819624 0.14211706 0.16481462]\n",
            " [0.15991222 0.18709017 0.1885539  0.37437483 0.19169395]\n",
            " [0.24650991 0.25575524 0.24948395 0.44139105 0.25527513]\n",
            " [0.23715131 0.23808691 0.23497644 0.21422425 0.22115211]\n",
            " [0.20696305 0.20429899 0.22483108 0.13293    0.2152024 ]\n",
            " [0.19257604 0.19633678 0.21244107 0.15155508 0.19900802]] -> [0.15767698]\n",
            "[[0.13714161 0.15347214 0.15819624 0.14211706 0.16481462]\n",
            " [0.15991222 0.18709017 0.1885539  0.37437483 0.19169395]\n",
            " [0.24650991 0.25575524 0.24948395 0.44139105 0.25527513]\n",
            " [0.23715131 0.23808691 0.23497644 0.21422425 0.22115211]\n",
            " [0.20696305 0.20429899 0.22483108 0.13293    0.2152024 ]\n",
            " [0.19257604 0.19633678 0.21244107 0.15155508 0.19900802]\n",
            " [0.1836059  0.1790176  0.17000127 0.18591915 0.15767698]] -> [0.15964846]\n",
            "[[0.15991222 0.18709017 0.1885539  0.37437483 0.19169395]\n",
            " [0.24650991 0.25575524 0.24948395 0.44139105 0.25527513]\n",
            " [0.23715131 0.23808691 0.23497644 0.21422425 0.22115211]\n",
            " [0.20696305 0.20429899 0.22483108 0.13293    0.2152024 ]\n",
            " [0.19257604 0.19633678 0.21244107 0.15155508 0.19900802]\n",
            " [0.1836059  0.1790176  0.17000127 0.18591915 0.15767698]\n",
            " [0.15103707 0.14822378 0.15944053 0.15777539 0.15964846]] -> [0.16978762]\n",
            "[[0.24650991 0.25575524 0.24948395 0.44139105 0.25527513]\n",
            " [0.23715131 0.23808691 0.23497644 0.21422425 0.22115211]\n",
            " [0.20696305 0.20429899 0.22483108 0.13293    0.2152024 ]\n",
            " [0.19257604 0.19633678 0.21244107 0.15155508 0.19900802]\n",
            " [0.1836059  0.1790176  0.17000127 0.18591915 0.15767698]\n",
            " [0.15103707 0.14822378 0.15944053 0.15777539 0.15964846]\n",
            " [0.1513822  0.16363772 0.1700371  0.11652774 0.16978762]] -> [0.13465254]\n",
            "[[0.23715131 0.23808691 0.23497644 0.21422425 0.22115211]\n",
            " [0.20696305 0.20429899 0.22483108 0.13293    0.2152024 ]\n",
            " [0.19257604 0.19633678 0.21244107 0.15155508 0.19900802]\n",
            " [0.1836059  0.1790176  0.17000127 0.18591915 0.15767698]\n",
            " [0.15103707 0.14822378 0.15944053 0.15777539 0.15964846]\n",
            " [0.1513822  0.16363772 0.1700371  0.11652774 0.16978762]\n",
            " [0.15027815 0.14890434 0.15332255 0.12325894 0.13465254]] -> [0.11148736]\n",
            "[[0.20696305 0.20429899 0.22483108 0.13293    0.2152024 ]\n",
            " [0.19257604 0.19633678 0.21244107 0.15155508 0.19900802]\n",
            " [0.1836059  0.1790176  0.17000127 0.18591915 0.15767698]\n",
            " [0.15103707 0.14822378 0.15944053 0.15777539 0.15964846]\n",
            " [0.1513822  0.16363772 0.1700371  0.11652774 0.16978762]\n",
            " [0.15027815 0.14890434 0.15332255 0.12325894 0.13465254]\n",
            " [0.12623093 0.12386116 0.12000816 0.13974187 0.11148736]] -> [0.13430057]\n",
            "[[0.19257604 0.19633678 0.21244107 0.15155508 0.19900802]\n",
            " [0.1836059  0.1790176  0.17000127 0.18591915 0.15767698]\n",
            " [0.15103707 0.14822378 0.15944053 0.15777539 0.15964846]\n",
            " [0.1513822  0.16363772 0.1700371  0.11652774 0.16978762]\n",
            " [0.15027815 0.14890434 0.15332255 0.12325894 0.13465254]\n",
            " [0.12623093 0.12386116 0.12000816 0.13974187 0.11148736]\n",
            " [0.1012178  0.12753604 0.12238871 0.13788653 0.13430057]] -> [0.16077485]\n",
            "[[0.1836059  0.1790176  0.17000127 0.18591915 0.15767698]\n",
            " [0.15103707 0.14822378 0.15944053 0.15777539 0.15964846]\n",
            " [0.1513822  0.16363772 0.1700371  0.11652774 0.16978762]\n",
            " [0.15027815 0.14890434 0.15332255 0.12325894 0.13465254]\n",
            " [0.12623093 0.12386116 0.12000816 0.13974187 0.11148736]\n",
            " [0.1012178  0.12753604 0.12238871 0.13788653 0.13430057]\n",
            " [0.14489602 0.15370214 0.13402337 0.13621045 0.16077485]] -> [0.15190324]\n",
            "[[0.15103707 0.14822378 0.15944053 0.15777539 0.15964846]\n",
            " [0.1513822  0.16363772 0.1700371  0.11652774 0.16978762]\n",
            " [0.15027815 0.14890434 0.15332255 0.12325894 0.13465254]\n",
            " [0.12623093 0.12386116 0.12000816 0.13974187 0.11148736]\n",
            " [0.1012178  0.12753604 0.12238871 0.13788653 0.13430057]\n",
            " [0.14489602 0.15370214 0.13402337 0.13621045 0.16077485]\n",
            " [0.15083007 0.15652616 0.17125436 0.08043381 0.15190324]] -> [0.12845636]\n",
            "[[0.1513822  0.16363772 0.1700371  0.11652774 0.16978762]\n",
            " [0.15027815 0.14890434 0.15332255 0.12325894 0.13465254]\n",
            " [0.12623093 0.12386116 0.12000816 0.13974187 0.11148736]\n",
            " [0.1012178  0.12753604 0.12238871 0.13788653 0.13430057]\n",
            " [0.14489602 0.15370214 0.13402337 0.13621045 0.16077485]\n",
            " [0.15083007 0.15652616 0.17125436 0.08043381 0.15190324]\n",
            " [0.12747291 0.12668186 0.13495418 0.14576499 0.12845636]] -> [0.13049833]\n",
            "[[0.15027815 0.14890434 0.15332255 0.12325894 0.13465254]\n",
            " [0.12623093 0.12386116 0.12000816 0.13974187 0.11148736]\n",
            " [0.1012178  0.12753604 0.12238871 0.13788653 0.13430057]\n",
            " [0.14489602 0.15370214 0.13402337 0.13621045 0.16077485]\n",
            " [0.15083007 0.15652616 0.17125436 0.08043381 0.15190324]\n",
            " [0.12747291 0.12668186 0.13495418 0.14576499 0.12845636]\n",
            " [0.1238849  0.13046909 0.14710799 0.11153536 0.13049833]] -> [0.16140873]\n",
            "[[0.12623093 0.12386116 0.12000816 0.13974187 0.11148736]\n",
            " [0.1012178  0.12753604 0.12238871 0.13788653 0.13430057]\n",
            " [0.14489602 0.15370214 0.13402337 0.13621045 0.16077485]\n",
            " [0.15083007 0.15652616 0.17125436 0.08043381 0.15190324]\n",
            " [0.12747291 0.12668186 0.13495418 0.14576499 0.12845636]\n",
            " [0.1238849  0.13046909 0.14710799 0.11153536 0.13049833]\n",
            " [0.13495975 0.14638644 0.16055029 0.12512324 0.16140873]] -> [0.14539013]\n",
            "[[0.1012178  0.12753604 0.12238871 0.13788653 0.13430057]\n",
            " [0.14489602 0.15370214 0.13402337 0.13621045 0.16077485]\n",
            " [0.15083007 0.15652616 0.17125436 0.08043381 0.15190324]\n",
            " [0.12747291 0.12668186 0.13495418 0.14576499 0.12845636]\n",
            " [0.1238849  0.13046909 0.14710799 0.11153536 0.13049833]\n",
            " [0.13495975 0.14638644 0.16055029 0.12512324 0.16140873]\n",
            " [0.15362463 0.14731871 0.15328322 0.17597921 0.14539013]] -> [0.13993334]\n",
            "[[0.14489602 0.15370214 0.13402337 0.13621045 0.16077485]\n",
            " [0.15083007 0.15652616 0.17125436 0.08043381 0.15190324]\n",
            " [0.12747291 0.12668186 0.13495418 0.14576499 0.12845636]\n",
            " [0.1238849  0.13046909 0.14710799 0.11153536 0.13049833]\n",
            " [0.13495975 0.14638644 0.16055029 0.12512324 0.16140873]\n",
            " [0.15362463 0.14731871 0.15328322 0.17597921 0.14539013]\n",
            " [0.12888757 0.13216354 0.14780588 0.17885632 0.13993334]] -> [0.15774724]\n",
            "[[0.15083007 0.15652616 0.17125436 0.08043381 0.15190324]\n",
            " [0.12747291 0.12668186 0.13495418 0.14576499 0.12845636]\n",
            " [0.1238849  0.13046909 0.14710799 0.11153536 0.13049833]\n",
            " [0.13495975 0.14638644 0.16055029 0.12512324 0.16140873]\n",
            " [0.15362463 0.14731871 0.15328322 0.17597921 0.14539013]\n",
            " [0.12888757 0.13216354 0.14780588 0.17885632 0.13993334]\n",
            " [0.13568413 0.15203469 0.16280564 0.17558483 0.15774724]] -> [0.16447158]\n",
            "[[0.12747291 0.12668186 0.13495418 0.14576499 0.12845636]\n",
            " [0.1238849  0.13046909 0.14710799 0.11153536 0.13049833]\n",
            " [0.13495975 0.14638644 0.16055029 0.12512324 0.16140873]\n",
            " [0.15362463 0.14731871 0.15328322 0.17597921 0.14539013]\n",
            " [0.12888757 0.13216354 0.14780588 0.17885632 0.13993334]\n",
            " [0.13568413 0.15203469 0.16280564 0.17558483 0.15774724]\n",
            " [0.15124406 0.15972463 0.16256223 0.12753428 0.16447158]] -> [0.17587808]\n",
            "[[0.1238849  0.13046909 0.14710799 0.11153536 0.13049833]\n",
            " [0.13495975 0.14638644 0.16055029 0.12512324 0.16140873]\n",
            " [0.15362463 0.14731871 0.15328322 0.17597921 0.14539013]\n",
            " [0.12888757 0.13216354 0.14780588 0.17885632 0.13993334]\n",
            " [0.13568413 0.15203469 0.16280564 0.17558483 0.15774724]\n",
            " [0.15124406 0.15972463 0.16256223 0.12753428 0.16447158]\n",
            " [0.14938109 0.16285518 0.17333054 0.13039347 0.17587808]] -> [0.16742871]\n",
            "[[0.13495975 0.14638644 0.16055029 0.12512324 0.16140873]\n",
            " [0.15362463 0.14731871 0.15328322 0.17597921 0.14539013]\n",
            " [0.12888757 0.13216354 0.14780588 0.17885632 0.13993334]\n",
            " [0.13568413 0.15203469 0.16280564 0.17558483 0.15774724]\n",
            " [0.15124406 0.15972463 0.16256223 0.12753428 0.16447158]\n",
            " [0.14938109 0.16285518 0.17333054 0.13039347 0.17587808]\n",
            " [0.15697132 0.16404601 0.18596768 0.10471453 0.16742871]] -> [0.14000382]\n",
            "[[0.15362463 0.14731871 0.15328322 0.17597921 0.14539013]\n",
            " [0.12888757 0.13216354 0.14780588 0.17885632 0.13993334]\n",
            " [0.13568413 0.15203469 0.16280564 0.17558483 0.15774724]\n",
            " [0.15124406 0.15972463 0.16256223 0.12753428 0.16447158]\n",
            " [0.14938109 0.16285518 0.17333054 0.13039347 0.17587808]\n",
            " [0.15697132 0.16404601 0.18596768 0.10471453 0.16742871]\n",
            " [0.14996755 0.14638644 0.15149327 0.21498611 0.14000382]] -> [0.16630211]\n",
            "[[0.12888757 0.13216354 0.14780588 0.17885632 0.13993334]\n",
            " [0.13568413 0.15203469 0.16280564 0.17558483 0.15774724]\n",
            " [0.15124406 0.15972463 0.16256223 0.12753428 0.16447158]\n",
            " [0.14938109 0.16285518 0.17333054 0.13039347 0.17587808]\n",
            " [0.15697132 0.16404601 0.18596768 0.10471453 0.16742871]\n",
            " [0.14996755 0.14638644 0.15149327 0.21498611 0.14000382]\n",
            " [0.13161306 0.15166045 0.15804454 0.13597741 0.16630211]] -> [0.16626709]\n",
            "[[0.13568413 0.15203469 0.16280564 0.17558483 0.15774724]\n",
            " [0.15124406 0.15972463 0.16256223 0.12753428 0.16447158]\n",
            " [0.14938109 0.16285518 0.17333054 0.13039347 0.17587808]\n",
            " [0.15697132 0.16404601 0.18596768 0.10471453 0.16742871]\n",
            " [0.14996755 0.14638644 0.15149327 0.21498611 0.14000382]\n",
            " [0.13161306 0.15166045 0.15804454 0.13597741 0.16630211]\n",
            " [0.14958809 0.1518646  0.17429718 0.09159272 0.16626709]] -> [0.13926443]\n",
            "[[0.15124406 0.15972463 0.16256223 0.12753428 0.16447158]\n",
            " [0.14938109 0.16285518 0.17333054 0.13039347 0.17587808]\n",
            " [0.15697132 0.16404601 0.18596768 0.10471453 0.16742871]\n",
            " [0.14996755 0.14638644 0.15149327 0.21498611 0.14000382]\n",
            " [0.13161306 0.15166045 0.15804454 0.13597741 0.16630211]\n",
            " [0.14958809 0.1518646  0.17429718 0.09159272 0.16626709]\n",
            " [0.14737998 0.14512749 0.15711373 0.23209644 0.13926443]] -> [0.14588305]\n",
            "[[0.14938109 0.16285518 0.17333054 0.13039347 0.17587808]\n",
            " [0.15697132 0.16404601 0.18596768 0.10471453 0.16742871]\n",
            " [0.14996755 0.14638644 0.15149327 0.21498611 0.14000382]\n",
            " [0.13161306 0.15166045 0.15804454 0.13597741 0.16630211]\n",
            " [0.14958809 0.1518646  0.17429718 0.09159272 0.16626709]\n",
            " [0.14737998 0.14512749 0.15711373 0.23209644 0.13926443]\n",
            " [0.14537887 0.13886659 0.1510637  0.16997401 0.14588305]] -> [0.16415464]\n",
            "[[0.15697132 0.16404601 0.18596768 0.10471453 0.16742871]\n",
            " [0.14996755 0.14638644 0.15149327 0.21498611 0.14000382]\n",
            " [0.13161306 0.15166045 0.15804454 0.13597741 0.16630211]\n",
            " [0.14958809 0.1518646  0.17429718 0.09159272 0.16626709]\n",
            " [0.14737998 0.14512749 0.15711373 0.23209644 0.13926443]\n",
            " [0.14537887 0.13886659 0.1510637  0.16997401 0.14588305]\n",
            " [0.13206159 0.1599969  0.15668416 0.17308416 0.16415464]] -> [0.16813286]\n",
            "[[0.14996755 0.14638644 0.15149327 0.21498611 0.14000382]\n",
            " [0.13161306 0.15166045 0.15804454 0.13597741 0.16630211]\n",
            " [0.14958809 0.1518646  0.17429718 0.09159272 0.16626709]\n",
            " [0.14737998 0.14512749 0.15711373 0.23209644 0.13926443]\n",
            " [0.14537887 0.13886659 0.1510637  0.16997401 0.14588305]\n",
            " [0.13206159 0.1599969  0.15668416 0.17308416 0.16415464]\n",
            " [0.15614313 0.1616982  0.17737584 0.15296227 0.16813286]] -> [0.15542378]\n",
            "[[0.13161306 0.15166045 0.15804454 0.13597741 0.16630211]\n",
            " [0.14958809 0.1518646  0.17429718 0.09159272 0.16626709]\n",
            " [0.14737998 0.14512749 0.15711373 0.23209644 0.13926443]\n",
            " [0.14537887 0.13886659 0.1510637  0.16997401 0.14588305]\n",
            " [0.13206159 0.1599969  0.15668416 0.17308416 0.16415464]\n",
            " [0.15614313 0.1616982  0.17737584 0.15296227 0.16813286]\n",
            " [0.14872557 0.15179669 0.16738801 0.12013982 0.15542378]] -> [0.14355959]\n",
            "[[0.14958809 0.1518646  0.17429718 0.09159272 0.16626709]\n",
            " [0.14737998 0.14512749 0.15711373 0.23209644 0.13926443]\n",
            " [0.14537887 0.13886659 0.1510637  0.16997401 0.14588305]\n",
            " [0.13206159 0.1599969  0.15668416 0.17308416 0.16415464]\n",
            " [0.15614313 0.1616982  0.17737584 0.15296227 0.16813286]\n",
            " [0.14872557 0.15179669 0.16738801 0.12013982 0.15542378]\n",
            " [0.14386083 0.14026178 0.16094425 0.12371605 0.14355959]] -> [0.12067612]\n",
            "[[0.14737998 0.14512749 0.15711373 0.23209644 0.13926443]\n",
            " [0.14537887 0.13886659 0.1510637  0.16997401 0.14588305]\n",
            " [0.13206159 0.1599969  0.15668416 0.17308416 0.16415464]\n",
            " [0.15614313 0.1616982  0.17737584 0.15296227 0.16813286]\n",
            " [0.14872557 0.15179669 0.16738801 0.12013982 0.15542378]\n",
            " [0.14386083 0.14026178 0.16094425 0.12371605 0.14355959]\n",
            " [0.13337264 0.12978167 0.1384624  0.13558304 0.12067612]] -> [0.12018319]\n",
            "[[0.14537887 0.13886659 0.1510637  0.16997401 0.14588305]\n",
            " [0.13206159 0.1599969  0.15668416 0.17308416 0.16415464]\n",
            " [0.15614313 0.1616982  0.17737584 0.15296227 0.16813286]\n",
            " [0.14872557 0.15179669 0.16738801 0.12013982 0.15542378]\n",
            " [0.14386083 0.14026178 0.16094425 0.12371605 0.14355959]\n",
            " [0.13337264 0.12978167 0.1384624  0.13558304 0.12067612]\n",
            " [0.11353464 0.11304087 0.12689941 0.12973021 0.12018319]] -> [0.15538854]\n",
            "[[0.13206159 0.1599969  0.15668416 0.17308416 0.16415464]\n",
            " [0.15614313 0.1616982  0.17737584 0.15296227 0.16813286]\n",
            " [0.14872557 0.15179669 0.16738801 0.12013982 0.15542378]\n",
            " [0.14386083 0.14026178 0.16094425 0.12371605 0.14355959]\n",
            " [0.13337264 0.12978167 0.1384624  0.13558304 0.12067612]\n",
            " [0.11353464 0.11304087 0.12689941 0.12973021 0.12018319]\n",
            " [0.11974475 0.14420872 0.14959582 0.16197006 0.15538854]] -> [0.14806577]\n",
            "[[0.15614313 0.1616982  0.17737584 0.15296227 0.16813286]\n",
            " [0.14872557 0.15179669 0.16738801 0.12013982 0.15542378]\n",
            " [0.14386083 0.14026178 0.16094425 0.12371605 0.14355959]\n",
            " [0.13337264 0.12978167 0.1384624  0.13558304 0.12067612]\n",
            " [0.11353464 0.11304087 0.12689941 0.12973021 0.12018319]\n",
            " [0.11974475 0.14420872 0.14959582 0.16197006 0.15538854]\n",
            " [0.1510198  0.14631832 0.16273419 0.10841624 0.14806577]] -> [0.14003906]\n",
            "[[0.14872557 0.15179669 0.16738801 0.12013982 0.15542378]\n",
            " [0.14386083 0.14026178 0.16094425 0.12371605 0.14355959]\n",
            " [0.13337264 0.12978167 0.1384624  0.13558304 0.12067612]\n",
            " [0.11353464 0.11304087 0.12689941 0.12973021 0.12018319]\n",
            " [0.11974475 0.14420872 0.14959582 0.16197006 0.15538854]\n",
            " [0.1510198  0.14631832 0.16273419 0.10841624 0.14806577]\n",
            " [0.12747291 0.12637906 0.15249553 0.08496011 0.14003906]] -> [0.1219787]\n",
            "[[0.14386083 0.14026178 0.16094425 0.12371605 0.14355959]\n",
            " [0.13337264 0.12978167 0.1384624  0.13558304 0.12067612]\n",
            " [0.11353464 0.11304087 0.12689941 0.12973021 0.12018319]\n",
            " [0.11974475 0.14420872 0.14959582 0.16197006 0.15538854]\n",
            " [0.1510198  0.14631832 0.16273419 0.10841624 0.14806577]\n",
            " [0.12747291 0.12637906 0.15249553 0.08496011 0.14003906]\n",
            " [0.11505268 0.10997844 0.13044347 0.14563055 0.1219787 ]] -> [0.12532325]\n",
            "[[0.13337264 0.12978167 0.1384624  0.13558304 0.12067612]\n",
            " [0.11353464 0.11304087 0.12689941 0.12973021 0.12018319]\n",
            " [0.11974475 0.14420872 0.14959582 0.16197006 0.15538854]\n",
            " [0.1510198  0.14631832 0.16273419 0.10841624 0.14806577]\n",
            " [0.12747291 0.12637906 0.15249553 0.08496011 0.14003906]\n",
            " [0.11505268 0.10997844 0.13044347 0.14563055 0.1219787 ]\n",
            " [0.1164328  0.11453803 0.1360281  0.09535717 0.12532325]] -> [0.12923099]\n",
            "[[0.11353464 0.11304087 0.12689941 0.12973021 0.12018319]\n",
            " [0.11974475 0.14420872 0.14959582 0.16197006 0.15538854]\n",
            " [0.1510198  0.14631832 0.16273419 0.10841624 0.14806577]\n",
            " [0.12747291 0.12637906 0.15249553 0.08496011 0.14003906]\n",
            " [0.11505268 0.10997844 0.13044347 0.14563055 0.1219787 ]\n",
            " [0.1164328  0.11453803 0.1360281  0.09535717 0.12532325]\n",
            " [0.11977929 0.11909742 0.13438127 0.11529085 0.12923099]] -> [0.15552928]\n",
            "[[0.11974475 0.14420872 0.14959582 0.16197006 0.15538854]\n",
            " [0.1510198  0.14631832 0.16273419 0.10841624 0.14806577]\n",
            " [0.12747291 0.12637906 0.15249553 0.08496011 0.14003906]\n",
            " [0.11505268 0.10997844 0.13044347 0.14563055 0.1219787 ]\n",
            " [0.1164328  0.11453803 0.1360281  0.09535717 0.12532325]\n",
            " [0.11977929 0.11909742 0.13438127 0.11529085 0.12923099]\n",
            " [0.12540294 0.1434943  0.15475088 0.16359236 0.15552928]] -> [0.15538854]\n",
            "[[0.1510198  0.14631832 0.16273419 0.10841624 0.14806577]\n",
            " [0.12747291 0.12637906 0.15249553 0.08496011 0.14003906]\n",
            " [0.11505268 0.10997844 0.13044347 0.14563055 0.1219787 ]\n",
            " [0.1164328  0.11453803 0.1360281  0.09535717 0.12532325]\n",
            " [0.11977929 0.11909742 0.13438127 0.11529085 0.12923099]\n",
            " [0.12540294 0.1434943  0.15475088 0.16359236 0.15552928]\n",
            " [0.14682806 0.14383448 0.16269836 0.16900601 0.15538854]] -> [0.16066934]\n",
            "[[0.12747291 0.12637906 0.15249553 0.08496011 0.14003906]\n",
            " [0.11505268 0.10997844 0.13044347 0.14563055 0.1219787 ]\n",
            " [0.1164328  0.11453803 0.1360281  0.09535717 0.12532325]\n",
            " [0.11977929 0.11909742 0.13438127 0.11529085 0.12923099]\n",
            " [0.12540294 0.1434943  0.15475088 0.16359236 0.15552928]\n",
            " [0.14682806 0.14383448 0.16269836 0.16900601 0.15538854]\n",
            " [0.15503929 0.1625148  0.17887955 0.1113561  0.16066934]] -> [0.16873129]\n",
            "[[0.11505268 0.10997844 0.13044347 0.14563055 0.1219787 ]\n",
            " [0.1164328  0.11453803 0.1360281  0.09535717 0.12532325]\n",
            " [0.11977929 0.11909742 0.13438127 0.11529085 0.12923099]\n",
            " [0.12540294 0.1434943  0.15475088 0.16359236 0.15552928]\n",
            " [0.14682806 0.14383448 0.16269836 0.16900601 0.15538854]\n",
            " [0.15503929 0.1625148  0.17887955 0.1113561  0.16066934]\n",
            " [0.15521175 0.15488965 0.17071729 0.10662364 0.16873129]] -> [0.15943724]\n",
            "[[0.1164328  0.11453803 0.1360281  0.09535717 0.12532325]\n",
            " [0.11977929 0.11909742 0.13438127 0.11529085 0.12923099]\n",
            " [0.12540294 0.1434943  0.15475088 0.16359236 0.15552928]\n",
            " [0.14682806 0.14383448 0.16269836 0.16900601 0.15538854]\n",
            " [0.15503929 0.1625148  0.17887955 0.1113561  0.16066934]\n",
            " [0.15521175 0.15488965 0.17071729 0.10662364 0.16873129]\n",
            " [0.15645373 0.14978906 0.17218495 0.11460966 0.15943724]] -> [0.15024848]\n",
            "[[0.11977929 0.11909742 0.13438127 0.11529085 0.12923099]\n",
            " [0.12540294 0.1434943  0.15475088 0.16359236 0.15552928]\n",
            " [0.14682806 0.14383448 0.16269836 0.16900601 0.15538854]\n",
            " [0.15503929 0.1625148  0.17887955 0.1113561  0.16066934]\n",
            " [0.15521175 0.15488965 0.17071729 0.10662364 0.16873129]\n",
            " [0.15645373 0.14978906 0.17218495 0.11460966 0.15943724]\n",
            " [0.15255511 0.15285149 0.17064562 0.11901049 0.15024848]] -> [0.13778586]\n",
            "[[0.12540294 0.1434943  0.15475088 0.16359236 0.15552928]\n",
            " [0.14682806 0.14383448 0.16269836 0.16900601 0.15538854]\n",
            " [0.15503929 0.1625148  0.17887955 0.1113561  0.16066934]\n",
            " [0.15521175 0.15488965 0.17071729 0.10662364 0.16873129]\n",
            " [0.15645373 0.14978906 0.17218495 0.11460966 0.15943724]\n",
            " [0.15255511 0.15285149 0.17064562 0.11901049 0.15024848]\n",
            " [0.14700052 0.14216724 0.15675561 0.1883302  0.13778586]] -> [0.10198209]\n",
            "[[0.14682806 0.14383448 0.16269836 0.16900601 0.15538854]\n",
            " [0.15503929 0.1625148  0.17887955 0.1113561  0.16066934]\n",
            " [0.15521175 0.15488965 0.17071729 0.10662364 0.16873129]\n",
            " [0.15645373 0.14978906 0.17218495 0.11460966 0.15943724]\n",
            " [0.15255511 0.15285149 0.17064562 0.11901049 0.15024848]\n",
            " [0.14700052 0.14216724 0.15675561 0.1883302  0.13778586]\n",
            " [0.10473696 0.11103324 0.11805696 0.17297661 0.10198209]] -> [0.09842631]\n",
            "[[0.15503929 0.1625148  0.17887955 0.1113561  0.16066934]\n",
            " [0.15521175 0.15488965 0.17071729 0.10662364 0.16873129]\n",
            " [0.15645373 0.14978906 0.17218495 0.11460966 0.15943724]\n",
            " [0.15255511 0.15285149 0.17064562 0.11901049 0.15024848]\n",
            " [0.14700052 0.14216724 0.15675561 0.1883302  0.13778586]\n",
            " [0.10473696 0.11103324 0.11805696 0.17297661 0.10198209]\n",
            " [0.10822158 0.10300312 0.11791384 0.19801918 0.09842631]] -> [0.10310868]\n",
            "[[0.15521175 0.15488965 0.17071729 0.10662364 0.16873129]\n",
            " [0.15645373 0.14978906 0.17218495 0.11460966 0.15943724]\n",
            " [0.15255511 0.15285149 0.17064562 0.11901049 0.15024848]\n",
            " [0.14700052 0.14216724 0.15675561 0.1883302  0.13778586]\n",
            " [0.10473696 0.11103324 0.11805696 0.17297661 0.10198209]\n",
            " [0.10822158 0.10300312 0.11791384 0.19801918 0.09842631]\n",
            " [0.10377083 0.10109767 0.10978741 0.17505602 0.10310868]] -> [0.10860071]\n",
            "[[0.15645373 0.14978906 0.17218495 0.11460966 0.15943724]\n",
            " [0.15255511 0.15285149 0.17064562 0.11901049 0.15024848]\n",
            " [0.14700052 0.14216724 0.15675561 0.1883302  0.13778586]\n",
            " [0.10473696 0.11103324 0.11805696 0.17297661 0.10198209]\n",
            " [0.10822158 0.10300312 0.11791384 0.19801918 0.09842631]\n",
            " [0.10377083 0.10109767 0.10978741 0.17505602 0.10310868]\n",
            " [0.09117814 0.09755902 0.11999024 0.11006543 0.10860071]] -> [0.10669948]\n",
            "[[0.15255511 0.15285149 0.17064562 0.11901049 0.15024848]\n",
            " [0.14700052 0.14216724 0.15675561 0.1883302  0.13778586]\n",
            " [0.10473696 0.11103324 0.11805696 0.17297661 0.10198209]\n",
            " [0.10822158 0.10300312 0.11791384 0.19801918 0.09842631]\n",
            " [0.10377083 0.10109767 0.10978741 0.17505602 0.10310868]\n",
            " [0.09117814 0.09755902 0.11999024 0.11006543 0.10860071]\n",
            " [0.08572694 0.09960051 0.112544   0.11406292 0.10669948]] -> [0.11430396]\n",
            "[[0.14700052 0.14216724 0.15675561 0.1883302  0.13778586]\n",
            " [0.10473696 0.11103324 0.11805696 0.17297661 0.10198209]\n",
            " [0.10822158 0.10300312 0.11791384 0.19801918 0.09842631]\n",
            " [0.10377083 0.10109767 0.10978741 0.17505602 0.10310868]\n",
            " [0.09117814 0.09755902 0.11999024 0.11006543 0.10860071]\n",
            " [0.08572694 0.09960051 0.112544   0.11406292 0.10669948]\n",
            " [0.09825078 0.10276492 0.09886878 0.14244869 0.11430396]] -> [0.08547077]\n",
            "[[0.10473696 0.11103324 0.11805696 0.17297661 0.10198209]\n",
            " [0.10822158 0.10300312 0.11791384 0.19801918 0.09842631]\n",
            " [0.10377083 0.10109767 0.10978741 0.17505602 0.10310868]\n",
            " [0.09117814 0.09755902 0.11999024 0.11006543 0.10860071]\n",
            " [0.08572694 0.09960051 0.112544   0.11406292 0.10669948]\n",
            " [0.09825078 0.10276492 0.09886878 0.14244869 0.11430396]\n",
            " [0.09107454 0.09103957 0.10219805 0.11551492 0.08547077]] -> [0.09902474]\n",
            "[[0.10822158 0.10300312 0.11791384 0.19801918 0.09842631]\n",
            " [0.10377083 0.10109767 0.10978741 0.17505602 0.10310868]\n",
            " [0.09117814 0.09755902 0.11999024 0.11006543 0.10860071]\n",
            " [0.08572694 0.09960051 0.112544   0.11406292 0.10669948]\n",
            " [0.09825078 0.10276492 0.09886878 0.14244869 0.11430396]\n",
            " [0.09107454 0.09103957 0.10219805 0.11551492 0.08547077]\n",
            " [0.09821624 0.09456471 0.11737677 0.16415703 0.09902474]] -> [0.13229384]\n",
            "[[0.10377083 0.10109767 0.10978741 0.17505602 0.10310868]\n",
            " [0.09117814 0.09755902 0.11999024 0.11006543 0.10860071]\n",
            " [0.08572694 0.09960051 0.112544   0.11406292 0.10669948]\n",
            " [0.09825078 0.10276492 0.09886878 0.14244869 0.11430396]\n",
            " [0.09107454 0.09103957 0.10219805 0.11551492 0.08547077]\n",
            " [0.09821624 0.09456471 0.11737677 0.16415703 0.09902474]\n",
            " [0.10915296 0.12447361 0.13599227 0.17467061 0.13229384]] -> [0.19010097]\n",
            "[[0.09117814 0.09755902 0.11999024 0.11006543 0.10860071]\n",
            " [0.08572694 0.09960051 0.112544   0.11406292 0.10669948]\n",
            " [0.09825078 0.10276492 0.09886878 0.14244869 0.11430396]\n",
            " [0.09107454 0.09103957 0.10219805 0.11551492 0.08547077]\n",
            " [0.09821624 0.09456471 0.11737677 0.16415703 0.09902474]\n",
            " [0.10915296 0.12447361 0.13599227 0.17467061 0.13229384]\n",
            " [0.13188913 0.17398158 0.16051467 0.19706014 0.19010097]] -> [0.24132471]\n",
            "[[0.08572694 0.09960051 0.112544   0.11406292 0.10669948]\n",
            " [0.09825078 0.10276492 0.09886878 0.14244869 0.11430396]\n",
            " [0.09107454 0.09103957 0.10219805 0.11551492 0.08547077]\n",
            " [0.09821624 0.09456471 0.11737677 0.16415703 0.09902474]\n",
            " [0.10915296 0.12447361 0.13599227 0.17467061 0.13229384]\n",
            " [0.13188913 0.17398158 0.16051467 0.19706014 0.19010097]\n",
            " [0.17977635 0.2377432  0.21174296 0.29006005 0.24132471]] -> [0.23822662]\n",
            "[[0.09825078 0.10276492 0.09886878 0.14244869 0.11430396]\n",
            " [0.09107454 0.09103957 0.10219805 0.11551492 0.08547077]\n",
            " [0.09821624 0.09456471 0.11737677 0.16415703 0.09902474]\n",
            " [0.10915296 0.12447361 0.13599227 0.17467061 0.13229384]\n",
            " [0.13188913 0.17398158 0.16051467 0.19706014 0.19010097]\n",
            " [0.17977635 0.2377432  0.21174296 0.29006005 0.24132471]\n",
            " [0.225904   0.23996848 0.24782814 0.15924532 0.23822662]] -> [0.30733474]\n",
            "[[0.09107454 0.09103957 0.10219805 0.11551492 0.08547077]\n",
            " [0.09821624 0.09456471 0.11737677 0.16415703 0.09902474]\n",
            " [0.10915296 0.12447361 0.13599227 0.17467061 0.13229384]\n",
            " [0.13188913 0.17398158 0.16051467 0.19706014 0.19010097]\n",
            " [0.17977635 0.2377432  0.21174296 0.29006005 0.24132471]\n",
            " [0.225904   0.23996848 0.24782814 0.15924532 0.23822662]\n",
            " [0.2431199  0.28820734 0.27721916 0.42667384 0.30733474]] -> [0.6350262]\n",
            "[[0.09821624 0.09456471 0.11737677 0.16415703 0.09902474]\n",
            " [0.10915296 0.12447361 0.13599227 0.17467061 0.13229384]\n",
            " [0.13188913 0.17398158 0.16051467 0.19706014 0.19010097]\n",
            " [0.17977635 0.2377432  0.21174296 0.29006005 0.24132471]\n",
            " [0.225904   0.23996848 0.24782814 0.15924532 0.23822662]\n",
            " [0.2431199  0.28820734 0.27721916 0.42667384 0.30733474]\n",
            " [0.53251321 0.60733171 0.56361074 1.         0.6350262 ]] -> [0.60013779]\n",
            "[[0.10915296 0.12447361 0.13599227 0.17467061 0.13229384]\n",
            " [0.13188913 0.17398158 0.16051467 0.19706014 0.19010097]\n",
            " [0.17977635 0.2377432  0.21174296 0.29006005 0.24132471]\n",
            " [0.225904   0.23996848 0.24782814 0.15924532 0.23822662]\n",
            " [0.2431199  0.28820734 0.27721916 0.42667384 0.30733474]\n",
            " [0.53251321 0.60733171 0.56361074 1.         0.6350262 ]\n",
            " [0.56784207 0.58831786 0.59228573 0.52460339 0.60013779]] -> [0.59760289]\n",
            "[[0.13188913 0.17398158 0.16051467 0.19706014 0.19010097]\n",
            " [0.17977635 0.2377432  0.21174296 0.29006005 0.24132471]\n",
            " [0.225904   0.23996848 0.24782814 0.15924532 0.23822662]\n",
            " [0.2431199  0.28820734 0.27721916 0.42667384 0.30733474]\n",
            " [0.53251321 0.60733171 0.56361074 1.         0.6350262 ]\n",
            " [0.56784207 0.58831786 0.59228573 0.52460339 0.60013779]\n",
            " [0.55393833 0.60233661 0.59690372 0.30198978 0.59760289]] -> [0.59689874]\n",
            "[[0.17977635 0.2377432  0.21174296 0.29006005 0.24132471]\n",
            " [0.225904   0.23996848 0.24782814 0.15924532 0.23822662]\n",
            " [0.2431199  0.28820734 0.27721916 0.42667384 0.30733474]\n",
            " [0.53251321 0.60733171 0.56361074 1.         0.6350262 ]\n",
            " [0.56784207 0.58831786 0.59228573 0.52460339 0.60013779]\n",
            " [0.55393833 0.60233661 0.59690372 0.30198978 0.59760289]\n",
            " [0.5735348  0.6215274  0.61372927 0.35147441 0.59689874]] -> [0.534163]\n",
            "[[0.225904   0.23996848 0.24782814 0.15924532 0.23822662]\n",
            " [0.2431199  0.28820734 0.27721916 0.42667384 0.30733474]\n",
            " [0.53251321 0.60733171 0.56361074 1.         0.6350262 ]\n",
            " [0.56784207 0.58831786 0.59228573 0.52460339 0.60013779]\n",
            " [0.55393833 0.60233661 0.59690372 0.30198978 0.59760289]\n",
            " [0.5735348  0.6215274  0.61372927 0.35147441 0.59689874]\n",
            " [0.57484585 0.57045414 0.54929116 0.27078964 0.534163  ]] -> [0.4612174]\n",
            "[[0.2431199  0.28820734 0.27721916 0.42667384 0.30733474]\n",
            " [0.53251321 0.60733171 0.56361074 1.         0.6350262 ]\n",
            " [0.56784207 0.58831786 0.59228573 0.52460339 0.60013779]\n",
            " [0.55393833 0.60233661 0.59690372 0.30198978 0.59760289]\n",
            " [0.5735348  0.6215274  0.61372927 0.35147441 0.59689874]\n",
            " [0.57484585 0.57045414 0.54929116 0.27078964 0.534163  ]\n",
            " [0.52561304 0.51784967 0.48313478 0.32426279 0.4612174 ]] -> [0.47424342]\n",
            "[[0.53251321 0.60733171 0.56361074 1.         0.6350262 ]\n",
            " [0.56784207 0.58831786 0.59228573 0.52460339 0.60013779]\n",
            " [0.55393833 0.60233661 0.59690372 0.30198978 0.59760289]\n",
            " [0.5735348  0.6215274  0.61372927 0.35147441 0.59689874]\n",
            " [0.57484585 0.57045414 0.54929116 0.27078964 0.534163  ]\n",
            " [0.52561304 0.51784967 0.48313478 0.32426279 0.4612174 ]\n",
            " [0.43591077 0.47065544 0.47590332 0.23908757 0.47424342]] -> [0.47684858]\n",
            "[[0.56784207 0.58831786 0.59228573 0.52460339 0.60013779]\n",
            " [0.55393833 0.60233661 0.59690372 0.30198978 0.59760289]\n",
            " [0.5735348  0.6215274  0.61372927 0.35147441 0.59689874]\n",
            " [0.57484585 0.57045414 0.54929116 0.27078964 0.534163  ]\n",
            " [0.52561304 0.51784967 0.48313478 0.32426279 0.4612174 ]\n",
            " [0.43591077 0.47065544 0.47590332 0.23908757 0.47424342]\n",
            " [0.47672536 0.4656537  0.48596282 0.15410953 0.47684858]] -> [0.49068426]\n",
            "[[0.55393833 0.60233661 0.59690372 0.30198978 0.59760289]\n",
            " [0.5735348  0.6215274  0.61372927 0.35147441 0.59689874]\n",
            " [0.57484585 0.57045414 0.54929116 0.27078964 0.534163  ]\n",
            " [0.52561304 0.51784967 0.48313478 0.32426279 0.4612174 ]\n",
            " [0.43591077 0.47065544 0.47590332 0.23908757 0.47424342]\n",
            " [0.47672536 0.4656537  0.48596282 0.15410953 0.47684858]\n",
            " [0.46282141 0.46745697 0.48360018 0.14046787 0.49068426]] -> [0.49300793]\n",
            "[[0.5735348  0.6215274  0.61372927 0.35147441 0.59689874]\n",
            " [0.57484585 0.57045414 0.54929116 0.27078964 0.534163  ]\n",
            " [0.52561304 0.51784967 0.48313478 0.32426279 0.4612174 ]\n",
            " [0.43591077 0.47065544 0.47590332 0.23908757 0.47424342]\n",
            " [0.47672536 0.4656537  0.48596282 0.15410953 0.47684858]\n",
            " [0.46282141 0.46745697 0.48360018 0.14046787 0.49068426]\n",
            " [0.46696156 0.47378579 0.48145211 0.13142422 0.49300793]] -> [0.46843445]\n",
            "[[0.57484585 0.57045414 0.54929116 0.27078964 0.534163  ]\n",
            " [0.52561304 0.51784967 0.48313478 0.32426279 0.4612174 ]\n",
            " [0.43591077 0.47065544 0.47590332 0.23908757 0.47424342]\n",
            " [0.47672536 0.4656537  0.48596282 0.15410953 0.47684858]\n",
            " [0.46282141 0.46745697 0.48360018 0.14046787 0.49068426]\n",
            " [0.46696156 0.47378579 0.48145211 0.13142422 0.49300793]\n",
            " [0.4717227  0.46592576 0.49380279 0.15220938 0.46843445]] -> [0.48814958]\n",
            "[[0.52561304 0.51784967 0.48313478 0.32426279 0.4612174 ]\n",
            " [0.43591077 0.47065544 0.47590332 0.23908757 0.47424342]\n",
            " [0.47672536 0.4656537  0.48596282 0.15410953 0.47684858]\n",
            " [0.46282141 0.46745697 0.48360018 0.14046787 0.49068426]\n",
            " [0.46696156 0.47378579 0.48145211 0.13142422 0.49300793]\n",
            " [0.4717227  0.46592576 0.49380279 0.15220938 0.46843445]\n",
            " [0.45088425 0.46642273 0.49323011 0.11621404 0.48814958]] -> [0.48124925]\n",
            "[[0.43591077 0.47065544 0.47590332 0.23908757 0.47424342]\n",
            " [0.47672536 0.4656537  0.48596282 0.15410953 0.47684858]\n",
            " [0.46282141 0.46745697 0.48360018 0.14046787 0.49068426]\n",
            " [0.46696156 0.47378579 0.48145211 0.13142422 0.49300793]\n",
            " [0.4717227  0.46592576 0.49380279 0.15220938 0.46843445]\n",
            " [0.45088425 0.46642273 0.49323011 0.11621404 0.48814958]\n",
            " [0.46151036 0.47239081 0.49974532 0.13292104 0.48124925]] -> [0.53240273]\n",
            "[[0.47672536 0.4656537  0.48596282 0.15410953 0.47684858]\n",
            " [0.46282141 0.46745697 0.48360018 0.14046787 0.49068426]\n",
            " [0.46696156 0.47378579 0.48145211 0.13142422 0.49300793]\n",
            " [0.4717227  0.46592576 0.49380279 0.15220938 0.46843445]\n",
            " [0.45088425 0.46642273 0.49323011 0.11621404 0.48814958]\n",
            " [0.46151036 0.47239081 0.49974532 0.13292104 0.48124925]\n",
            " [0.48190049 0.51679486 0.52122469 0.20851483 0.53240273]] -> [0.52853001]\n",
            "[[0.46282141 0.46745697 0.48360018 0.14046787 0.49068426]\n",
            " [0.46696156 0.47378579 0.48145211 0.13142422 0.49300793]\n",
            " [0.4717227  0.46592576 0.49380279 0.15220938 0.46843445]\n",
            " [0.45088425 0.46642273 0.49323011 0.11621404 0.48814958]\n",
            " [0.46151036 0.47239081 0.49974532 0.13292104 0.48124925]\n",
            " [0.48190049 0.51679486 0.52122469 0.20851483 0.53240273]\n",
            " [0.51871286 0.50835313 0.51796708 0.14024379 0.52853001]] -> [0.50254845]\n",
            "[[0.46696156 0.47378579 0.48145211 0.13142422 0.49300793]\n",
            " [0.4717227  0.46592576 0.49380279 0.15220938 0.46843445]\n",
            " [0.45088425 0.46642273 0.49323011 0.11621404 0.48814958]\n",
            " [0.46151036 0.47239081 0.49974532 0.13292104 0.48124925]\n",
            " [0.48190049 0.51679486 0.52122469 0.20851483 0.53240273]\n",
            " [0.51871286 0.50835313 0.51796708 0.14024379 0.52853001]\n",
            " [0.50225588 0.49916936 0.50887423 0.12512324 0.50254845]] -> [0.49702118]\n",
            "[[0.4717227  0.46592576 0.49380279 0.15220938 0.46843445]\n",
            " [0.45088425 0.46642273 0.49323011 0.11621404 0.48814958]\n",
            " [0.46151036 0.47239081 0.49974532 0.13292104 0.48124925]\n",
            " [0.48190049 0.51679486 0.52122469 0.20851483 0.53240273]\n",
            " [0.51871286 0.50835313 0.51796708 0.14024379 0.52853001]\n",
            " [0.50225588 0.49916936 0.50887423 0.12512324 0.50254845]\n",
            " [0.49966831 0.50175537 0.51438369 0.16145021 0.49702118]] -> [0.59225182]\n",
            "[[0.45088425 0.46642273 0.49323011 0.11621404 0.48814958]\n",
            " [0.46151036 0.47239081 0.49974532 0.13292104 0.48124925]\n",
            " [0.48190049 0.51679486 0.52122469 0.20851483 0.53240273]\n",
            " [0.51871286 0.50835313 0.51796708 0.14024379 0.52853001]\n",
            " [0.50225588 0.49916936 0.50887423 0.12512324 0.50254845]\n",
            " [0.49966831 0.50175537 0.51438369 0.16145021 0.49702118]\n",
            " [0.60220501 0.60880166 0.59679644 0.45005826 0.59225182]] -> [0.58795666]\n",
            "[[0.46151036 0.47239081 0.49974532 0.13292104 0.48124925]\n",
            " [0.48190049 0.51679486 0.52122469 0.20851483 0.53240273]\n",
            " [0.51871286 0.50835313 0.51796708 0.14024379 0.52853001]\n",
            " [0.50225588 0.49916936 0.50887423 0.12512324 0.50254845]\n",
            " [0.49966831 0.50175537 0.51438369 0.16145021 0.49702118]\n",
            " [0.60220501 0.60880166 0.59679644 0.45005826 0.59225182]\n",
            " [0.58109049 0.57511571 0.58970809 0.26250784 0.58795666]] -> [0.57700785]\n",
            "[[0.48190049 0.51679486 0.52122469 0.20851483 0.53240273]\n",
            " [0.51871286 0.50835313 0.51796708 0.14024379 0.52853001]\n",
            " [0.50225588 0.49916936 0.50887423 0.12512324 0.50254845]\n",
            " [0.49966831 0.50175537 0.51438369 0.16145021 0.49702118]\n",
            " [0.60220501 0.60880166 0.59679644 0.45005826 0.59225182]\n",
            " [0.58109049 0.57511571 0.58970809 0.26250784 0.58795666]\n",
            " [0.56812508 0.5734144  0.58745646 0.16158466 0.57700785]] -> [0.57936654]\n",
            "[[0.51871286 0.50835313 0.51796708 0.14024379 0.52853001]\n",
            " [0.50225588 0.49916936 0.50887423 0.12512324 0.50254845]\n",
            " [0.49966831 0.50175537 0.51438369 0.16145021 0.49702118]\n",
            " [0.60220501 0.60880166 0.59679644 0.45005826 0.59225182]\n",
            " [0.58109049 0.57511571 0.58970809 0.26250784 0.58795666]\n",
            " [0.56812508 0.5734144  0.58745646 0.16158466 0.57700785]\n",
            " [0.55324827 0.55760919 0.59103264 0.09538406 0.57936654]] -> [0.59256855]\n",
            "[[0.50225588 0.49916936 0.50887423 0.12512324 0.50254845]\n",
            " [0.49966831 0.50175537 0.51438369 0.16145021 0.49702118]\n",
            " [0.60220501 0.60880166 0.59679644 0.45005826 0.59225182]\n",
            " [0.58109049 0.57511571 0.58970809 0.26250784 0.58795666]\n",
            " [0.56812508 0.5734144  0.58745646 0.16158466 0.57700785]\n",
            " [0.55324827 0.55760919 0.59103264 0.09538406 0.57936654]\n",
            " [0.55942385 0.56279826 0.58594925 0.09355562 0.59256855]] -> [0.57588125]\n",
            "[[0.49966831 0.50175537 0.51438369 0.16145021 0.49702118]\n",
            " [0.60220501 0.60880166 0.59679644 0.45005826 0.59225182]\n",
            " [0.58109049 0.57511571 0.58970809 0.26250784 0.58795666]\n",
            " [0.56812508 0.5734144  0.58745646 0.16158466 0.57700785]\n",
            " [0.55324827 0.55760919 0.59103264 0.09538406 0.57936654]\n",
            " [0.55942385 0.56279826 0.58594925 0.09355562 0.59256855]\n",
            " [0.57701942 0.57171309 0.59389673 0.12980192 0.57588125]] -> [0.59267427]\n",
            "[[0.60220501 0.60880166 0.59679644 0.45005826 0.59225182]\n",
            " [0.58109049 0.57511571 0.58970809 0.26250784 0.58795666]\n",
            " [0.56812508 0.5734144  0.58745646 0.16158466 0.57700785]\n",
            " [0.55324827 0.55760919 0.59103264 0.09538406 0.57936654]\n",
            " [0.55942385 0.56279826 0.58594925 0.09355562 0.59256855]\n",
            " [0.57701942 0.57171309 0.59389673 0.12980192 0.57588125]\n",
            " [0.55873379 0.58192093 0.59650998 0.19034687 0.59267427]] -> [0.54314032]\n",
            "[[0.58109049 0.57511571 0.58970809 0.26250784 0.58795666]\n",
            " [0.56812508 0.5734144  0.58745646 0.16158466 0.57700785]\n",
            " [0.55324827 0.55760919 0.59103264 0.09538406 0.57936654]\n",
            " [0.55942385 0.56279826 0.58594925 0.09355562 0.59256855]\n",
            " [0.57701942 0.57171309 0.59389673 0.12980192 0.57588125]\n",
            " [0.55873379 0.58192093 0.59650998 0.19034687 0.59267427]\n",
            " [0.55480085 0.56827642 0.55609305 0.25521197 0.54314032]] -> [0.42220981]\n",
            "[[0.56812508 0.5734144  0.58745646 0.16158466 0.57700785]\n",
            " [0.55324827 0.55760919 0.59103264 0.09538406 0.57936654]\n",
            " [0.55942385 0.56279826 0.58594925 0.09355562 0.59256855]\n",
            " [0.57701942 0.57171309 0.59389673 0.12980192 0.57588125]\n",
            " [0.55873379 0.58192093 0.59650998 0.19034687 0.59267427]\n",
            " [0.55480085 0.56827642 0.55609305 0.25521197 0.54314032]\n",
            " [0.50070351 0.49022047 0.44665564 0.38158107 0.42220981]] -> [0.34169519]\n",
            "[[0.55324827 0.55760919 0.59103264 0.09538406 0.57936654]\n",
            " [0.55942385 0.56279826 0.58594925 0.09355562 0.59256855]\n",
            " [0.57701942 0.57171309 0.59389673 0.12980192 0.57588125]\n",
            " [0.55873379 0.58192093 0.59650998 0.19034687 0.59267427]\n",
            " [0.55480085 0.56827642 0.55609305 0.25521197 0.54314032]\n",
            " [0.50070351 0.49022047 0.44665564 0.38158107 0.42220981]\n",
            " [0.2703066  0.35166615 0.27739811 0.51648293 0.34169519]] -> [0.3151152]\n",
            "[[0.55942385 0.56279826 0.58594925 0.09355562 0.59256855]\n",
            " [0.57701942 0.57171309 0.59389673 0.12980192 0.57588125]\n",
            " [0.55873379 0.58192093 0.59650998 0.19034687 0.59267427]\n",
            " [0.55480085 0.56827642 0.55609305 0.25521197 0.54314032]\n",
            " [0.50070351 0.49022047 0.44665564 0.38158107 0.42220981]\n",
            " [0.2703066  0.35166615 0.27739811 0.51648293 0.34169519]\n",
            " [0.41489965 0.41332149 0.33489121 0.31640226 0.3151152 ]] -> [0.4790313]\n",
            "[[0.57701942 0.57171309 0.59389673 0.12980192 0.57588125]\n",
            " [0.55873379 0.58192093 0.59650998 0.19034687 0.59267427]\n",
            " [0.55480085 0.56827642 0.55609305 0.25521197 0.54314032]\n",
            " [0.50070351 0.49022047 0.44665564 0.38158107 0.42220981]\n",
            " [0.2703066  0.35166615 0.27739811 0.51648293 0.34169519]\n",
            " [0.41489965 0.41332149 0.33489121 0.31640226 0.3151152 ]\n",
            " [0.39916727 0.46184279 0.39911453 0.37895492 0.4790313 ]] -> [0.51068087]\n",
            "[[0.55873379 0.58192093 0.59650998 0.19034687 0.59267427]\n",
            " [0.55480085 0.56827642 0.55609305 0.25521197 0.54314032]\n",
            " [0.50070351 0.49022047 0.44665564 0.38158107 0.42220981]\n",
            " [0.2703066  0.35166615 0.27739811 0.51648293 0.34169519]\n",
            " [0.41489965 0.41332149 0.33489121 0.31640226 0.3151152 ]\n",
            " [0.39916727 0.46184279 0.39911453 0.37895492 0.4790313 ]\n",
            " [0.49939246 0.50226585 0.48127316 0.31221655 0.51068087]] -> [0.48522747]\n",
            "[[0.55480085 0.56827642 0.55609305 0.25521197 0.54314032]\n",
            " [0.50070351 0.49022047 0.44665564 0.38158107 0.42220981]\n",
            " [0.2703066  0.35166615 0.27739811 0.51648293 0.34169519]\n",
            " [0.41489965 0.41332149 0.33489121 0.31640226 0.3151152 ]\n",
            " [0.39916727 0.46184279 0.39911453 0.37895492 0.4790313 ]\n",
            " [0.49939246 0.50226585 0.48127316 0.31221655 0.51068087]\n",
            " [0.47669083 0.47943424 0.49043768 0.17664247 0.48522747]] -> [0.44252337]\n",
            "[[0.50070351 0.49022047 0.44665564 0.38158107 0.42220981]\n",
            " [0.2703066  0.35166615 0.27739811 0.51648293 0.34169519]\n",
            " [0.41489965 0.41332149 0.33489121 0.31640226 0.3151152 ]\n",
            " [0.39916727 0.46184279 0.39911453 0.37895492 0.4790313 ]\n",
            " [0.49939246 0.50226585 0.48127316 0.31221655 0.51068087]\n",
            " [0.47669083 0.47943424 0.49043768 0.17664247 0.48522747]\n",
            " [0.45847427 0.47575936 0.46580799 0.19438917 0.44252337]] -> [0.37049314]\n",
            "[[0.2703066  0.35166615 0.27739811 0.51648293 0.34169519]\n",
            " [0.41489965 0.41332149 0.33489121 0.31640226 0.3151152 ]\n",
            " [0.39916727 0.46184279 0.39911453 0.37895492 0.4790313 ]\n",
            " [0.49939246 0.50226585 0.48127316 0.31221655 0.51068087]\n",
            " [0.47669083 0.47943424 0.49043768 0.17664247 0.48522747]\n",
            " [0.45847427 0.47575936 0.46580799 0.19438917 0.44252337]\n",
            " [0.3716011  0.3977034  0.38139401 0.33111051 0.37049314]] -> [0.42875817]\n",
            "[[0.41489965 0.41332149 0.33489121 0.31640226 0.3151152 ]\n",
            " [0.39916727 0.46184279 0.39911453 0.37895492 0.4790313 ]\n",
            " [0.49939246 0.50226585 0.48127316 0.31221655 0.51068087]\n",
            " [0.47669083 0.47943424 0.49043768 0.17664247 0.48522747]\n",
            " [0.45847427 0.47575936 0.46580799 0.19438917 0.44252337]\n",
            " [0.3716011  0.3977034  0.38139401 0.33111051 0.37049314]\n",
            " [0.38274503 0.40273941 0.40147738 0.23014251 0.42875817]] -> [0.40027695]\n",
            "[[0.39916727 0.46184279 0.39911453 0.37895492 0.4790313 ]\n",
            " [0.49939246 0.50226585 0.48127316 0.31221655 0.51068087]\n",
            " [0.47669083 0.47943424 0.49043768 0.17664247 0.48522747]\n",
            " [0.45847427 0.47575936 0.46580799 0.19438917 0.44252337]\n",
            " [0.3716011  0.3977034  0.38139401 0.33111051 0.37049314]\n",
            " [0.38274503 0.40273941 0.40147738 0.23014251 0.42875817]\n",
            " [0.42211043 0.42101143 0.4126143  0.15700457 0.40027695]] -> [0.38073802]\n",
            "[[0.49939246 0.50226585 0.48127316 0.31221655 0.51068087]\n",
            " [0.47669083 0.47943424 0.49043768 0.17664247 0.48522747]\n",
            " [0.45847427 0.47575936 0.46580799 0.19438917 0.44252337]\n",
            " [0.3716011  0.3977034  0.38139401 0.33111051 0.37049314]\n",
            " [0.38274503 0.40273941 0.40147738 0.23014251 0.42875817]\n",
            " [0.42211043 0.42101143 0.4126143  0.15700457 0.40027695]\n",
            " [0.36345895 0.36575282 0.38551098 0.18652864 0.38073802]] -> [0.42988455]\n",
            "[[0.47669083 0.47943424 0.49043768 0.17664247 0.48522747]\n",
            " [0.45847427 0.47575936 0.46580799 0.19438917 0.44252337]\n",
            " [0.3716011  0.3977034  0.38139401 0.33111051 0.37049314]\n",
            " [0.38274503 0.40273941 0.40147738 0.23014251 0.42875817]\n",
            " [0.42211043 0.42101143 0.4126143  0.15700457 0.40027695]\n",
            " [0.36345895 0.36575282 0.38551098 0.18652864 0.38073802]\n",
            " [0.4065505  0.40944246 0.41726462 0.20360312 0.42988455]] -> [0.42305471]\n",
            "[[0.45847427 0.47575936 0.46580799 0.19438917 0.44252337]\n",
            " [0.3716011  0.3977034  0.38139401 0.33111051 0.37049314]\n",
            " [0.38274503 0.40273941 0.40147738 0.23014251 0.42875817]\n",
            " [0.42211043 0.42101143 0.4126143  0.15700457 0.40027695]\n",
            " [0.36345895 0.36575282 0.38551098 0.18652864 0.38073802]\n",
            " [0.4065505  0.40944246 0.41726462 0.20360312 0.42988455]\n",
            " [0.43666969 0.44418322 0.43688238 0.15185086 0.42305471]] -> [0.45343694]\n",
            "[[0.3716011  0.3977034  0.38139401 0.33111051 0.37049314]\n",
            " [0.38274503 0.40273941 0.40147738 0.23014251 0.42875817]\n",
            " [0.42211043 0.42101143 0.4126143  0.15700457 0.40027695]\n",
            " [0.36345895 0.36575282 0.38551098 0.18652864 0.38073802]\n",
            " [0.4065505  0.40944246 0.41726462 0.20360312 0.42988455]\n",
            " [0.43666969 0.44418322 0.43688238 0.15185086 0.42305471]\n",
            " [0.408655   0.43615289 0.44343365 0.17006364 0.45343694]] -> [0.46899786]\n",
            "[[0.38274503 0.40273941 0.40147738 0.23014251 0.42875817]\n",
            " [0.42211043 0.42101143 0.4126143  0.15700457 0.40027695]\n",
            " [0.36345895 0.36575282 0.38551098 0.18652864 0.38073802]\n",
            " [0.4065505  0.40944246 0.41726462 0.20360312 0.42988455]\n",
            " [0.43666969 0.44418322 0.43688238 0.15185086 0.42305471]\n",
            " [0.408655   0.43615289 0.44343365 0.17006364 0.45343694]\n",
            " [0.43159817 0.44166531 0.46487718 0.12239849 0.46899786]] -> [0.4600908]\n",
            "[[0.42211043 0.42101143 0.4126143  0.15700457 0.40027695]\n",
            " [0.36345895 0.36575282 0.38551098 0.18652864 0.38073802]\n",
            " [0.4065505  0.40944246 0.41726462 0.20360312 0.42988455]\n",
            " [0.43666969 0.44418322 0.43688238 0.15185086 0.42305471]\n",
            " [0.408655   0.43615289 0.44343365 0.17006364 0.45343694]\n",
            " [0.43159817 0.44166531 0.46487718 0.12239849 0.46899786]\n",
            " [0.45212623 0.44193737 0.47207281 0.15186878 0.4600908 ]] -> [0.50198526]\n",
            "[[0.36345895 0.36575282 0.38551098 0.18652864 0.38073802]\n",
            " [0.4065505  0.40944246 0.41726462 0.20360312 0.42988455]\n",
            " [0.43666969 0.44418322 0.43688238 0.15185086 0.42305471]\n",
            " [0.408655   0.43615289 0.44343365 0.17006364 0.45343694]\n",
            " [0.43159817 0.44166531 0.46487718 0.12239849 0.46899786]\n",
            " [0.45212623 0.44193737 0.47207281 0.15186878 0.4600908 ]\n",
            " [0.45557631 0.48562702 0.48764548 0.18611634 0.50198526]] -> [0.50494238]\n",
            "[[0.4065505  0.40944246 0.41726462 0.20360312 0.42988455]\n",
            " [0.43666969 0.44418322 0.43688238 0.15185086 0.42305471]\n",
            " [0.408655   0.43615289 0.44343365 0.17006364 0.45343694]\n",
            " [0.43159817 0.44166531 0.46487718 0.12239849 0.46899786]\n",
            " [0.45212623 0.44193737 0.47207281 0.15186878 0.4600908 ]\n",
            " [0.45557631 0.48562702 0.48764548 0.18611634 0.50198526]\n",
            " [0.48583343 0.48307506 0.5182177  0.1146007  0.50494238]] -> [0.52930464]\n",
            "[[0.43666969 0.44418322 0.43688238 0.15185086 0.42305471]\n",
            " [0.408655   0.43615289 0.44343365 0.17006364 0.45343694]\n",
            " [0.43159817 0.44166531 0.46487718 0.12239849 0.46899786]\n",
            " [0.45212623 0.44193737 0.47207281 0.15186878 0.4600908 ]\n",
            " [0.45557631 0.48562702 0.48764548 0.18611634 0.50198526]\n",
            " [0.48583343 0.48307506 0.5182177  0.1146007  0.50494238]\n",
            " [0.49383766 0.52713894 0.52788346 0.2031729  0.52930464]] -> [0.48124925]\n",
            "[[0.408655   0.43615289 0.44343365 0.17006364 0.45343694]\n",
            " [0.43159817 0.44166531 0.46487718 0.12239849 0.46899786]\n",
            " [0.45212623 0.44193737 0.47207281 0.15186878 0.4600908 ]\n",
            " [0.45557631 0.48562702 0.48764548 0.18611634 0.50198526]\n",
            " [0.48583343 0.48307506 0.5182177  0.1146007  0.50494238]\n",
            " [0.49383766 0.52713894 0.52788346 0.2031729  0.52930464]\n",
            " [0.49038757 0.49005038 0.4992443  0.45939769 0.48124925]] -> [0.50304137]\n",
            "[[0.43159817 0.44166531 0.46487718 0.12239849 0.46899786]\n",
            " [0.45212623 0.44193737 0.47207281 0.15186878 0.4600908 ]\n",
            " [0.45557631 0.48562702 0.48764548 0.18611634 0.50198526]\n",
            " [0.48583343 0.48307506 0.5182177  0.1146007  0.50494238]\n",
            " [0.49383766 0.52713894 0.52788346 0.2031729  0.52930464]\n",
            " [0.49038757 0.49005038 0.4992443  0.45939769 0.48124925]\n",
            " [0.48214202 0.47810717 0.49537795 0.15959487 0.50304137]] -> [0.45815455]\n",
            "[[0.45212623 0.44193737 0.47207281 0.15186878 0.4600908 ]\n",
            " [0.45557631 0.48562702 0.48764548 0.18611634 0.50198526]\n",
            " [0.48583343 0.48307506 0.5182177  0.1146007  0.50494238]\n",
            " [0.49383766 0.52713894 0.52788346 0.2031729  0.52930464]\n",
            " [0.49038757 0.49005038 0.4992443  0.45939769 0.48124925]\n",
            " [0.48214202 0.47810717 0.49537795 0.15959487 0.50304137]\n",
            " [0.4566113  0.4476878  0.45775323 0.22900421 0.45815455]] -> [0.45699271]\n",
            "[[0.45557631 0.48562702 0.48764548 0.18611634 0.50198526]\n",
            " [0.48583343 0.48307506 0.5182177  0.1146007  0.50494238]\n",
            " [0.49383766 0.52713894 0.52788346 0.2031729  0.52930464]\n",
            " [0.49038757 0.49005038 0.4992443  0.45939769 0.48124925]\n",
            " [0.48214202 0.47810717 0.49537795 0.15959487 0.50304137]\n",
            " [0.4566113  0.4476878  0.45775323 0.22900421 0.45815455]\n",
            " [0.43953332 0.45238342 0.47411337 0.13112844 0.45699271]] -> [0.46910336]\n",
            "[[0.48583343 0.48307506 0.5182177  0.1146007  0.50494238]\n",
            " [0.49383766 0.52713894 0.52788346 0.2031729  0.52930464]\n",
            " [0.49038757 0.49005038 0.4992443  0.45939769 0.48124925]\n",
            " [0.48214202 0.47810717 0.49537795 0.15959487 0.50304137]\n",
            " [0.4566113  0.4476878  0.45775323 0.22900421 0.45815455]\n",
            " [0.43953332 0.45238342 0.47411337 0.13112844 0.45699271]\n",
            " [0.42086845 0.44690526 0.44690626 0.2000717  0.46910336]] -> [0.42041431]\n",
            "[[0.49383766 0.52713894 0.52788346 0.2031729  0.52930464]\n",
            " [0.49038757 0.49005038 0.4992443  0.45939769 0.48124925]\n",
            " [0.48214202 0.47810717 0.49537795 0.15959487 0.50304137]\n",
            " [0.4566113  0.4476878  0.45775323 0.22900421 0.45815455]\n",
            " [0.43953332 0.45238342 0.47411337 0.13112844 0.45699271]\n",
            " [0.42086845 0.44690526 0.44690626 0.2000717  0.46910336]\n",
            " [0.46616811 0.45524171 0.44189432 0.19414717 0.42041431]] -> [0.36028372]\n",
            "[[0.49038757 0.49005038 0.4992443  0.45939769 0.48124925]\n",
            " [0.48214202 0.47810717 0.49537795 0.15959487 0.50304137]\n",
            " [0.4566113  0.4476878  0.45775323 0.22900421 0.45815455]\n",
            " [0.43953332 0.45238342 0.47411337 0.13112844 0.45699271]\n",
            " [0.42086845 0.44690526 0.44690626 0.2000717  0.46910336]\n",
            " [0.46616811 0.45524171 0.44189432 0.19414717 0.42041431]\n",
            " [0.39913294 0.40364095 0.36449701 0.27962714 0.36028372]] -> [0.36056521]\n",
            "[[0.48214202 0.47810717 0.49537795 0.15959487 0.50304137]\n",
            " [0.4566113  0.4476878  0.45775323 0.22900421 0.45815455]\n",
            " [0.43953332 0.45238342 0.47411337 0.13112844 0.45699271]\n",
            " [0.42086845 0.44690526 0.44690626 0.2000717  0.46910336]\n",
            " [0.46616811 0.45524171 0.44189432 0.19414717 0.42041431]\n",
            " [0.39913294 0.40364095 0.36449701 0.27962714 0.36028372]\n",
            " [0.35407481 0.37095891 0.367504   0.20629201 0.36056521]] -> [0.40791645]\n",
            "[[0.4566113  0.4476878  0.45775323 0.22900421 0.45815455]\n",
            " [0.43953332 0.45238342 0.47411337 0.13112844 0.45699271]\n",
            " [0.42086845 0.44690526 0.44690626 0.2000717  0.46910336]\n",
            " [0.46616811 0.45524171 0.44189432 0.19414717 0.42041431]\n",
            " [0.39913294 0.40364095 0.36449701 0.27962714 0.36028372]\n",
            " [0.35407481 0.37095891 0.367504   0.20629201 0.36056521]\n",
            " [0.37477533 0.38375277 0.40512873 0.21560455 0.40791645]] -> [0.41802037]\n",
            "[[0.43953332 0.45238342 0.47411337 0.13112844 0.45699271]\n",
            " [0.42086845 0.44690526 0.44690626 0.2000717  0.46910336]\n",
            " [0.46616811 0.45524171 0.44189432 0.19414717 0.42041431]\n",
            " [0.39913294 0.40364095 0.36449701 0.27962714 0.36028372]\n",
            " [0.35407481 0.37095891 0.367504   0.20629201 0.36056521]\n",
            " [0.37477533 0.38375277 0.40512873 0.21560455 0.40791645]\n",
            " [0.39233616 0.39508353 0.40197841 0.16668459 0.41802037]] -> [0.47301111]\n",
            "[[0.42086845 0.44690526 0.44690626 0.2000717  0.46910336]\n",
            " [0.46616811 0.45524171 0.44189432 0.19414717 0.42041431]\n",
            " [0.39913294 0.40364095 0.36449701 0.27962714 0.36028372]\n",
            " [0.35407481 0.37095891 0.367504   0.20629201 0.36056521]\n",
            " [0.37477533 0.38375277 0.40512873 0.21560455 0.40791645]\n",
            " [0.39233616 0.39508353 0.40197841 0.16668459 0.41802037]\n",
            " [0.38829962 0.44697338 0.41372056 0.23993009 0.47301111]] -> [0.52427009]\n",
            "[[0.46616811 0.45524171 0.44189432 0.19414717 0.42041431]\n",
            " [0.39913294 0.40364095 0.36449701 0.27962714 0.36028372]\n",
            " [0.35407481 0.37095891 0.367504   0.20629201 0.36056521]\n",
            " [0.37477533 0.38375277 0.40512873 0.21560455 0.40791645]\n",
            " [0.39233616 0.39508353 0.40197841 0.16668459 0.41802037]\n",
            " [0.38829962 0.44697338 0.41372056 0.23993009 0.47301111]\n",
            " [0.47386173 0.50029228 0.49917263 0.16094828 0.52427009]] -> [0.53824672]\n",
            "[[0.39913294 0.40364095 0.36449701 0.27962714 0.36028372]\n",
            " [0.35407481 0.37095891 0.367504   0.20629201 0.36056521]\n",
            " [0.37477533 0.38375277 0.40512873 0.21560455 0.40791645]\n",
            " [0.39233616 0.39508353 0.40197841 0.16668459 0.41802037]\n",
            " [0.38829962 0.44697338 0.41372056 0.23993009 0.47301111]\n",
            " [0.47386173 0.50029228 0.49917263 0.16094828 0.52427009]\n",
            " [0.49746042 0.52152455 0.53328914 0.19345702 0.53824672]] -> [0.52740341]\n",
            "[[0.35407481 0.37095891 0.367504   0.20629201 0.36056521]\n",
            " [0.37477533 0.38375277 0.40512873 0.21560455 0.40791645]\n",
            " [0.39233616 0.39508353 0.40197841 0.16668459 0.41802037]\n",
            " [0.38829962 0.44697338 0.41372056 0.23993009 0.47301111]\n",
            " [0.47386173 0.50029228 0.49917263 0.16094828 0.52427009]\n",
            " [0.49746042 0.52152455 0.53328914 0.19345702 0.53824672]\n",
            " [0.5333412  0.52614873 0.51760918 0.18686027 0.52740341]] -> [0.51613766]\n",
            "[[0.37477533 0.38375277 0.40512873 0.21560455 0.40791645]\n",
            " [0.39233616 0.39508353 0.40197841 0.16668459 0.41802037]\n",
            " [0.38829962 0.44697338 0.41372056 0.23993009 0.47301111]\n",
            " [0.47386173 0.50029228 0.49917263 0.16094828 0.52427009]\n",
            " [0.49746042 0.52152455 0.53328914 0.19345702 0.53824672]\n",
            " [0.5333412  0.52614873 0.51760918 0.18686027 0.52740341]\n",
            " [0.50615449 0.50519205 0.49401758 0.19487317 0.51613766]] -> [0.53180408]\n",
            "[[0.39233616 0.39508353 0.40197841 0.16668459 0.41802037]\n",
            " [0.38829962 0.44697338 0.41372056 0.23993009 0.47301111]\n",
            " [0.47386173 0.50029228 0.49917263 0.16094828 0.52427009]\n",
            " [0.49746042 0.52152455 0.53328914 0.19345702 0.53824672]\n",
            " [0.5333412  0.52614873 0.51760918 0.18686027 0.52740341]\n",
            " [0.50615449 0.50519205 0.49401758 0.19487317 0.51613766]\n",
            " [0.50146243 0.510432   0.52895017 0.14706462 0.53180408]] -> [0.54257691]\n",
            "[[0.38829962 0.44697338 0.41372056 0.23993009 0.47301111]\n",
            " [0.47386173 0.50029228 0.49917263 0.16094828 0.52427009]\n",
            " [0.49746042 0.52152455 0.53328914 0.19345702 0.53824672]\n",
            " [0.5333412  0.52614873 0.51760918 0.18686027 0.52740341]\n",
            " [0.50615449 0.50519205 0.49401758 0.19487317 0.51613766]\n",
            " [0.50146243 0.510432   0.52895017 0.14706462 0.53180408]\n",
            " [0.5086732  0.51897259 0.54216721 0.11358788 0.54257691]] -> [0.56239754]\n",
            "[[0.47386173 0.50029228 0.49917263 0.16094828 0.52427009]\n",
            " [0.49746042 0.52152455 0.53328914 0.19345702 0.53824672]\n",
            " [0.5333412  0.52614873 0.51760918 0.18686027 0.52740341]\n",
            " [0.50615449 0.50519205 0.49401758 0.19487317 0.51613766]\n",
            " [0.50146243 0.510432   0.52895017 0.14706462 0.53180408]\n",
            " [0.5086732  0.51897259 0.54216721 0.11358788 0.54257691]\n",
            " [0.51233028 0.55065776 0.55698802 0.16131577 0.56239754]] -> [0.55838408]\n",
            "[[0.49746042 0.52152455 0.53328914 0.19345702 0.53824672]\n",
            " [0.5333412  0.52614873 0.51760918 0.18686027 0.52740341]\n",
            " [0.50615449 0.50519205 0.49401758 0.19487317 0.51613766]\n",
            " [0.50146243 0.510432   0.52895017 0.14706462 0.53180408]\n",
            " [0.5086732  0.51897259 0.54216721 0.11358788 0.54257691]\n",
            " [0.51233028 0.55065776 0.55698802 0.16131577 0.56239754]\n",
            " [0.54703815 0.5560271  0.57739325 0.12616295 0.55838408]] -> [0.5956314]\n",
            "[[0.5333412  0.52614873 0.51760918 0.18686027 0.52740341]\n",
            " [0.50615449 0.50519205 0.49401758 0.19487317 0.51613766]\n",
            " [0.50146243 0.510432   0.52895017 0.14706462 0.53180408]\n",
            " [0.5086732  0.51897259 0.54216721 0.11358788 0.54257691]\n",
            " [0.51233028 0.55065776 0.55698802 0.16131577 0.56239754]\n",
            " [0.54703815 0.5560271  0.57739325 0.12616295 0.55838408]\n",
            " [0.55204061 0.56875284 0.59747662 0.16830689 0.5956314 ]] -> [0.59725092]\n",
            "[[0.50615449 0.50519205 0.49401758 0.19487317 0.51613766]\n",
            " [0.50146243 0.510432   0.52895017 0.14706462 0.53180408]\n",
            " [0.5086732  0.51897259 0.54216721 0.11358788 0.54257691]\n",
            " [0.51233028 0.55065776 0.55698802 0.16131577 0.56239754]\n",
            " [0.54703815 0.5560271  0.57739325 0.12616295 0.55838408]\n",
            " [0.55204061 0.56875284 0.59747662 0.16830689 0.5956314 ]\n",
            " [0.58464397 0.57501353 0.6072855  0.14369454 0.59725092]] -> [0.61098088]\n",
            "[[0.50146243 0.510432   0.52895017 0.14706462 0.53180408]\n",
            " [0.5086732  0.51897259 0.54216721 0.11358788 0.54257691]\n",
            " [0.51233028 0.55065776 0.55698802 0.16131577 0.56239754]\n",
            " [0.54703815 0.5560271  0.57739325 0.12616295 0.55838408]\n",
            " [0.55204061 0.56875284 0.59747662 0.16830689 0.5956314 ]\n",
            " [0.58464397 0.57501353 0.6072855  0.14369454 0.59725092]\n",
            " [0.57453525 0.58130849 0.61580567 0.13170207 0.61098088]] -> [0.55528621]\n",
            "[[0.5086732  0.51897259 0.54216721 0.11358788 0.54257691]\n",
            " [0.51233028 0.55065776 0.55698802 0.16131577 0.56239754]\n",
            " [0.54703815 0.5560271  0.57739325 0.12616295 0.55838408]\n",
            " [0.55204061 0.56875284 0.59747662 0.16830689 0.5956314 ]\n",
            " [0.58464397 0.57501353 0.6072855  0.14369454 0.59725092]\n",
            " [0.57453525 0.58130849 0.61580567 0.13170207 0.61098088]\n",
            " [0.58440244 0.57416288 0.56072895 0.2224702  0.55528621]] -> [0.52828355]\n",
            "[[0.51233028 0.55065776 0.55698802 0.16131577 0.56239754]\n",
            " [0.54703815 0.5560271  0.57739325 0.12616295 0.55838408]\n",
            " [0.55204061 0.56875284 0.59747662 0.16830689 0.5956314 ]\n",
            " [0.58464397 0.57501353 0.6072855  0.14369454 0.59725092]\n",
            " [0.57453525 0.58130849 0.61580567 0.13170207 0.61098088]\n",
            " [0.58440244 0.57416288 0.56072895 0.2224702  0.55528621]\n",
            " [0.55028124 0.54404983 0.55190441 0.15982791 0.52828355]] -> [0.56060203]\n",
            "[[0.54703815 0.5560271  0.57739325 0.12616295 0.55838408]\n",
            " [0.55204061 0.56875284 0.59747662 0.16830689 0.5956314 ]\n",
            " [0.58464397 0.57501353 0.6072855  0.14369454 0.59725092]\n",
            " [0.57453525 0.58130849 0.61580567 0.13170207 0.61098088]\n",
            " [0.58440244 0.57416288 0.56072895 0.2224702  0.55528621]\n",
            " [0.55028124 0.54404983 0.55190441 0.15982791 0.52828355]\n",
            " [0.52457805 0.55061685 0.56006668 0.36417496 0.56060203]] -> [0.73736817]\n",
            "[[0.55204061 0.56875284 0.59747662 0.16830689 0.5956314 ]\n",
            " [0.58464397 0.57501353 0.6072855  0.14369454 0.59725092]\n",
            " [0.57453525 0.58130849 0.61580567 0.13170207 0.61098088]\n",
            " [0.58440244 0.57416288 0.56072895 0.2224702  0.55528621]\n",
            " [0.55028124 0.54404983 0.55190441 0.15982791 0.52828355]\n",
            " [0.52457805 0.55061685 0.56006668 0.36417496 0.56060203]\n",
            " [0.80334504 0.79628556 0.7658748  0.59567984 0.73736817]] -> [0.77531964]\n",
            "[[0.58464397 0.57501353 0.6072855  0.14369454 0.59725092]\n",
            " [0.57453525 0.58130849 0.61580567 0.13170207 0.61098088]\n",
            " [0.58440244 0.57416288 0.56072895 0.2224702  0.55528621]\n",
            " [0.55028124 0.54404983 0.55190441 0.15982791 0.52828355]\n",
            " [0.52457805 0.55061685 0.56006668 0.36417496 0.56060203]\n",
            " [0.80334504 0.79628556 0.7658748  0.59567984 0.73736817]\n",
            " [0.71381524 0.75936729 0.76501566 0.24278032 0.77531964]] -> [0.76021641]\n",
            "[[0.57453525 0.58130849 0.61580567 0.13170207 0.61098088]\n",
            " [0.58440244 0.57416288 0.56072895 0.2224702  0.55528621]\n",
            " [0.55028124 0.54404983 0.55190441 0.15982791 0.52828355]\n",
            " [0.52457805 0.55061685 0.56006668 0.36417496 0.56060203]\n",
            " [0.80334504 0.79628556 0.7658748  0.59567984 0.73736817]\n",
            " [0.71381524 0.75936729 0.76501566 0.24278032 0.77531964]\n",
            " [0.73392931 0.74055074 0.77679343 0.20058259 0.76021641]] -> [0.77591807]\n",
            "[[0.58440244 0.57416288 0.56072895 0.2224702  0.55528621]\n",
            " [0.55028124 0.54404983 0.55190441 0.15982791 0.52828355]\n",
            " [0.52457805 0.55061685 0.56006668 0.36417496 0.56060203]\n",
            " [0.80334504 0.79628556 0.7658748  0.59567984 0.73736817]\n",
            " [0.71381524 0.75936729 0.76501566 0.24278032 0.77531964]\n",
            " [0.73392931 0.74055074 0.77679343 0.20058259 0.76021641]\n",
            " [0.73375684 0.73837302 0.77153109 0.19458636 0.77591807]] -> [0.7898945]\n",
            "[[0.55028124 0.54404983 0.55190441 0.15982791 0.52828355]\n",
            " [0.52457805 0.55061685 0.56006668 0.36417496 0.56060203]\n",
            " [0.80334504 0.79628556 0.7658748  0.59567984 0.73736817]\n",
            " [0.71381524 0.75936729 0.76501566 0.24278032 0.77531964]\n",
            " [0.73392931 0.74055074 0.77679343 0.20058259 0.76021641]\n",
            " [0.73375684 0.73837302 0.77153109 0.19458636 0.77591807]\n",
            " [0.74469356 0.75633892 0.79633974 0.12979296 0.7898945 ]] -> [0.76838408]\n",
            "[[0.52457805 0.55061685 0.56006668 0.36417496 0.56060203]\n",
            " [0.80334504 0.79628556 0.7658748  0.59567984 0.73736817]\n",
            " [0.71381524 0.75936729 0.76501566 0.24278032 0.77531964]\n",
            " [0.73392931 0.74055074 0.77679343 0.20058259 0.76021641]\n",
            " [0.73375684 0.73837302 0.77153109 0.19458636 0.77591807]\n",
            " [0.74469356 0.75633892 0.79633974 0.12979296 0.7898945 ]\n",
            " [0.76273745 0.75545421 0.79648285 0.17037734 0.76838408]] -> [0.80464554]\n",
            "[[0.80334504 0.79628556 0.7658748  0.59567984 0.73736817]\n",
            " [0.71381524 0.75936729 0.76501566 0.24278032 0.77531964]\n",
            " [0.73392931 0.74055074 0.77679343 0.20058259 0.76021641]\n",
            " [0.73375684 0.73837302 0.77153109 0.19458636 0.77591807]\n",
            " [0.74469356 0.75633892 0.79633974 0.12979296 0.7898945 ]\n",
            " [0.76273745 0.75545421 0.79648285 0.17037734 0.76838408]\n",
            " [0.7466256  0.76777165 0.78144725 0.16836067 0.80464554]] -> [0.80834206]\n",
            "[[0.71381524 0.75936729 0.76501566 0.24278032 0.77531964]\n",
            " [0.73392931 0.74055074 0.77679343 0.20058259 0.76021641]\n",
            " [0.73375684 0.73837302 0.77153109 0.19458636 0.77591807]\n",
            " [0.74469356 0.75633892 0.79633974 0.12979296 0.7898945 ]\n",
            " [0.76273745 0.75545421 0.79648285 0.17037734 0.76838408]\n",
            " [0.7466256  0.76777165 0.78144725 0.16836067 0.80464554]\n",
            " [0.77353624 0.77808167 0.8132009  0.13959846 0.80834206]] -> [0.82928929]\n",
            "[[0.73392931 0.74055074 0.77679343 0.20058259 0.76021641]\n",
            " [0.73375684 0.73837302 0.77153109 0.19458636 0.77591807]\n",
            " [0.74469356 0.75633892 0.79633974 0.12979296 0.7898945 ]\n",
            " [0.76273745 0.75545421 0.79648285 0.17037734 0.76838408]\n",
            " [0.7466256  0.76777165 0.78144725 0.16836067 0.80464554]\n",
            " [0.77353624 0.77808167 0.8132009  0.13959846 0.80834206]\n",
            " [0.78436956 0.80683358 0.83890473 0.15226315 0.82928929]] -> [0.84034382]\n",
            "[[0.73375684 0.73837302 0.77153109 0.19458636 0.77591807]\n",
            " [0.74469356 0.75633892 0.79633974 0.12979296 0.7898945 ]\n",
            " [0.76273745 0.75545421 0.79648285 0.17037734 0.76838408]\n",
            " [0.7466256  0.76777165 0.78144725 0.16836067 0.80464554]\n",
            " [0.77353624 0.77808167 0.8132009  0.13959846 0.80834206]\n",
            " [0.78436956 0.80683358 0.83890473 0.15226315 0.82928929]\n",
            " [0.81014161 0.82854227 0.86600435 0.16614681 0.84034382]] -> [0.8491804]\n",
            "[[0.74469356 0.75633892 0.79633974 0.12979296 0.7898945 ]\n",
            " [0.76273745 0.75545421 0.79648285 0.17037734 0.76838408]\n",
            " [0.7466256  0.76777165 0.78144725 0.16836067 0.80464554]\n",
            " [0.77353624 0.77808167 0.8132009  0.13959846 0.80834206]\n",
            " [0.78436956 0.80683358 0.83890473 0.15226315 0.82928929]\n",
            " [0.81014161 0.82854227 0.86600435 0.16614681 0.84034382]\n",
            " [0.81714539 0.81469361 0.85719795 0.13477637 0.8491804 ]] -> [0.81795327]\n",
            "[[0.76273745 0.75545421 0.79648285 0.17037734 0.76838408]\n",
            " [0.7466256  0.76777165 0.78144725 0.16836067 0.80464554]\n",
            " [0.77353624 0.77808167 0.8132009  0.13959846 0.80834206]\n",
            " [0.78436956 0.80683358 0.83890473 0.15226315 0.82928929]\n",
            " [0.81014161 0.82854227 0.86600435 0.16614681 0.84034382]\n",
            " [0.81714539 0.81469361 0.85719795 0.13477637 0.8491804 ]\n",
            " [0.81266032 0.81231195 0.83006228 0.18480774 0.81795327]] -> [0.83002868]\n",
            "[[0.7466256  0.76777165 0.78144725 0.16836067 0.80464554]\n",
            " [0.77353624 0.77808167 0.8132009  0.13959846 0.80834206]\n",
            " [0.78436956 0.80683358 0.83890473 0.15226315 0.82928929]\n",
            " [0.81014161 0.82854227 0.86600435 0.16614681 0.84034382]\n",
            " [0.81714539 0.81469361 0.85719795 0.13477637 0.8491804 ]\n",
            " [0.81266032 0.81231195 0.83006228 0.18480774 0.81795327]\n",
            " [0.79264985 0.7982932  0.82673301 0.14341669 0.83002868]] -> [0.85495413]\n",
            "[[0.77353624 0.77808167 0.8132009  0.13959846 0.80834206]\n",
            " [0.78436956 0.80683358 0.83890473 0.15226315 0.82928929]\n",
            " [0.81014161 0.82854227 0.86600435 0.16614681 0.84034382]\n",
            " [0.81714539 0.81469361 0.85719795 0.13477637 0.8491804 ]\n",
            " [0.81266032 0.81231195 0.83006228 0.18480774 0.81795327]\n",
            " [0.79264985 0.7982932  0.82673301 0.14341669 0.83002868]\n",
            " [0.82045755 0.83371431 0.8687251  0.12176212 0.85495413]] -> [0.84027334]\n",
            "[[0.78436956 0.80683358 0.83890473 0.15226315 0.82928929]\n",
            " [0.81014161 0.82854227 0.86600435 0.16614681 0.84034382]\n",
            " [0.81714539 0.81469361 0.85719795 0.13477637 0.8491804 ]\n",
            " [0.81266032 0.81231195 0.83006228 0.18480774 0.81795327]\n",
            " [0.79264985 0.7982932  0.82673301 0.14341669 0.83002868]\n",
            " [0.82045755 0.83371431 0.8687251  0.12176212 0.85495413]\n",
            " [0.81542035 0.82282591 0.86305111 0.16395985 0.84027334]] -> [0.7901762]\n",
            "[[0.81014161 0.82854227 0.86600435 0.16614681 0.84034382]\n",
            " [0.81714539 0.81469361 0.85719795 0.13477637 0.8491804 ]\n",
            " [0.81266032 0.81231195 0.83006228 0.18480774 0.81795327]\n",
            " [0.79264985 0.7982932  0.82673301 0.14341669 0.83002868]\n",
            " [0.82045755 0.83371431 0.8687251  0.12176212 0.85495413]\n",
            " [0.81542035 0.82282591 0.86305111 0.16395985 0.84027334]\n",
            " [0.80910663 0.80019865 0.82039652 0.18531863 0.7901762 ]] -> [0.83228187]\n",
            "[[0.81714539 0.81469361 0.85719795 0.13477637 0.8491804 ]\n",
            " [0.81266032 0.81231195 0.83006228 0.18480774 0.81795327]\n",
            " [0.79264985 0.7982932  0.82673301 0.14341669 0.83002868]\n",
            " [0.82045755 0.83371431 0.8687251  0.12176212 0.85495413]\n",
            " [0.81542035 0.82282591 0.86305111 0.16395985 0.84027334]\n",
            " [0.80910663 0.80019865 0.82039652 0.18531863 0.7901762 ]\n",
            " [0.76228892 0.7945502  0.80106522 0.17011742 0.83228187]] -> [0.8193966]\n",
            "[[0.81266032 0.81231195 0.83006228 0.18480774 0.81795327]\n",
            " [0.79264985 0.7982932  0.82673301 0.14341669 0.83002868]\n",
            " [0.82045755 0.83371431 0.8687251  0.12176212 0.85495413]\n",
            " [0.81542035 0.82282591 0.86305111 0.16395985 0.84027334]\n",
            " [0.80910663 0.80019865 0.82039652 0.18531863 0.7901762 ]\n",
            " [0.76228892 0.7945502  0.80106522 0.17011742 0.83228187]\n",
            " [0.80952062 0.80256329 0.84293911 0.13471363 0.8193966 ]] -> [0.8711485]\n",
            "[[0.79264985 0.7982932  0.82673301 0.14341669 0.83002868]\n",
            " [0.82045755 0.83371431 0.8687251  0.12176212 0.85495413]\n",
            " [0.81542035 0.82282591 0.86305111 0.16395985 0.84027334]\n",
            " [0.80910663 0.80019865 0.82039652 0.18531863 0.7901762 ]\n",
            " [0.76228892 0.7945502  0.80106522 0.17011742 0.83228187]\n",
            " [0.80952062 0.80256329 0.84293911 0.13471363 0.8193966 ]\n",
            " [0.80362111 0.83510929 0.85716211 0.15025545 0.8711485 ]] -> [0.86555076]\n",
            "[[0.82045755 0.83371431 0.8687251  0.12176212 0.85495413]\n",
            " [0.81542035 0.82282591 0.86305111 0.16395985 0.84027334]\n",
            " [0.80910663 0.80019865 0.82039652 0.18531863 0.7901762 ]\n",
            " [0.76228892 0.7945502  0.80106522 0.17011742 0.83228187]\n",
            " [0.80952062 0.80256329 0.84293911 0.13471363 0.8193966 ]\n",
            " [0.80362111 0.83510929 0.85716211 0.15025545 0.8711485 ]\n",
            " [0.84212398 0.83711692 0.89450039 0.11823967 0.86555076]] -> [0.9295893]\n",
            "[[0.81542035 0.82282591 0.86305111 0.16395985 0.84027334]\n",
            " [0.80910663 0.80019865 0.82039652 0.18531863 0.7901762 ]\n",
            " [0.76228892 0.7945502  0.80106522 0.17011742 0.83228187]\n",
            " [0.80952062 0.80256329 0.84293911 0.13471363 0.8193966 ]\n",
            " [0.80362111 0.83510929 0.85716211 0.15025545 0.8711485 ]\n",
            " [0.84212398 0.83711692 0.89450039 0.11823967 0.86555076]\n",
            " [0.8690003  0.89128647 0.91444043 0.19757999 0.9295893 ]] -> [0.92740658]\n",
            "[[0.80910663 0.80019865 0.82039652 0.18531863 0.7901762 ]\n",
            " [0.76228892 0.7945502  0.80106522 0.17011742 0.83228187]\n",
            " [0.80952062 0.80256329 0.84293911 0.13471363 0.8193966 ]\n",
            " [0.80362111 0.83510929 0.85716211 0.15025545 0.8711485 ]\n",
            " [0.84212398 0.83711692 0.89450039 0.11823967 0.86555076]\n",
            " [0.8690003  0.89128647 0.91444043 0.19757999 0.9295893 ]\n",
            " [0.90667519 0.90757826 0.94601513 0.12607332 0.92740658]] -> [0.90029864]\n",
            "[[0.76228892 0.7945502  0.80106522 0.17011742 0.83228187]\n",
            " [0.80952062 0.80256329 0.84293911 0.13471363 0.8193966 ]\n",
            " [0.80362111 0.83510929 0.85716211 0.15025545 0.8711485 ]\n",
            " [0.84212398 0.83711692 0.89450039 0.11823967 0.86555076]\n",
            " [0.8690003  0.89128647 0.91444043 0.19757999 0.9295893 ]\n",
            " [0.90667519 0.90757826 0.94601513 0.12607332 0.92740658]\n",
            " [0.88787217 0.8823002  0.89521641 0.20840728 0.90029864]] -> [0.89984095]\n",
            "[[0.80952062 0.80256329 0.84293911 0.13471363 0.8193966 ]\n",
            " [0.80362111 0.83510929 0.85716211 0.15025545 0.8711485 ]\n",
            " [0.84212398 0.83711692 0.89450039 0.11823967 0.86555076]\n",
            " [0.8690003  0.89128647 0.91444043 0.19757999 0.9295893 ]\n",
            " [0.90667519 0.90757826 0.94601513 0.12607332 0.92740658]\n",
            " [0.88787217 0.8823002  0.89521641 0.20840728 0.90029864]\n",
            " [0.87455489 0.87114305 0.9253949  0.09986556 0.89984095]] -> [0.90726923]\n",
            "[[0.80362111 0.83510929 0.85716211 0.15025545 0.8711485 ]\n",
            " [0.84212398 0.83711692 0.89450039 0.11823967 0.86555076]\n",
            " [0.8690003  0.89128647 0.91444043 0.19757999 0.9295893 ]\n",
            " [0.90667519 0.90757826 0.94601513 0.12607332 0.92740658]\n",
            " [0.88787217 0.8823002  0.89521641 0.20840728 0.90029864]\n",
            " [0.87455489 0.87114305 0.9253949  0.09986556 0.89984095]\n",
            " [0.87565894 0.87594065 0.93051412 0.07444654 0.90726923]] -> [0.88030181]\n",
            "[[0.84212398 0.83711692 0.89450039 0.11823967 0.86555076]\n",
            " [0.8690003  0.89128647 0.91444043 0.19757999 0.9295893 ]\n",
            " [0.90667519 0.90757826 0.94601513 0.12607332 0.92740658]\n",
            " [0.88787217 0.8823002  0.89521641 0.20840728 0.90029864]\n",
            " [0.87455489 0.87114305 0.9253949  0.09986556 0.89984095]\n",
            " [0.87565894 0.87594065 0.93051412 0.07444654 0.90726923]\n",
            " [0.87686639 0.88111269 0.90824728 0.18729945 0.88030181]] -> [0.96634369]\n",
            "[[0.8690003  0.89128647 0.91444043 0.19757999 0.9295893 ]\n",
            " [0.90667519 0.90757826 0.94601513 0.12607332 0.92740658]\n",
            " [0.88787217 0.8823002  0.89521641 0.20840728 0.90029864]\n",
            " [0.87455489 0.87114305 0.9253949  0.09986556 0.89984095]\n",
            " [0.87565894 0.87594065 0.93051412 0.07444654 0.90726923]\n",
            " [0.87686639 0.88111269 0.90824728 0.18729945 0.88030181]\n",
            " [0.87100119 0.92881739 0.92768608 0.19061576 0.96634369]] -> [0.94993809]\n",
            "[[0.90667519 0.90757826 0.94601513 0.12607332 0.92740658]\n",
            " [0.88787217 0.8823002  0.89521641 0.20840728 0.90029864]\n",
            " [0.87455489 0.87114305 0.9253949  0.09986556 0.89984095]\n",
            " [0.87565894 0.87594065 0.93051412 0.07444654 0.90726923]\n",
            " [0.87686639 0.88111269 0.90824728 0.18729945 0.88030181]\n",
            " [0.87100119 0.92881739 0.92768608 0.19061576 0.96634369]\n",
            " [0.94617873 0.95265271 0.97157563 0.19920229 0.94993809]] -> [0.91529593]\n",
            "[[0.88787217 0.8823002  0.89521641 0.20840728 0.90029864]\n",
            " [0.87455489 0.87114305 0.9253949  0.09986556 0.89984095]\n",
            " [0.87565894 0.87594065 0.93051412 0.07444654 0.90726923]\n",
            " [0.87686639 0.88111269 0.90824728 0.18729945 0.88030181]\n",
            " [0.87100119 0.92881739 0.92768608 0.19061576 0.96634369]\n",
            " [0.94617873 0.95265271 0.97157563 0.19920229 0.94993809]\n",
            " [0.93620793 0.92897045 0.92385557 0.23148696 0.91529593]] -> [0.96553404]\n",
            "[[0.87455489 0.87114305 0.9253949  0.09986556 0.89984095]\n",
            " [0.87565894 0.87594065 0.93051412 0.07444654 0.90726923]\n",
            " [0.87686639 0.88111269 0.90824728 0.18729945 0.88030181]\n",
            " [0.87100119 0.92881739 0.92768608 0.19061576 0.96634369]\n",
            " [0.94617873 0.95265271 0.97157563 0.19920229 0.94993809]\n",
            " [0.93620793 0.92897045 0.92385557 0.23148696 0.91529593]\n",
            " [0.89166718 0.92725211 0.93949969 0.24642825 0.96553404]] -> [0.95300094]\n",
            "[[0.87565894 0.87594065 0.93051412 0.07444654 0.90726923]\n",
            " [0.87686639 0.88111269 0.90824728 0.18729945 0.88030181]\n",
            " [0.87100119 0.92881739 0.92768608 0.19061576 0.96634369]\n",
            " [0.94617873 0.95265271 0.97157563 0.19920229 0.94993809]\n",
            " [0.93620793 0.92897045 0.92385557 0.23148696 0.91529593]\n",
            " [0.89166718 0.92725211 0.93949969 0.24642825 0.96553404]\n",
            " [0.94228011 0.92806871 0.95772145 0.16172806 0.95300094]] -> [0.94990285]\n",
            "[[0.87686639 0.88111269 0.90824728 0.18729945 0.88030181]\n",
            " [0.87100119 0.92881739 0.92768608 0.19061576 0.96634369]\n",
            " [0.94617873 0.95265271 0.97157563 0.19920229 0.94993809]\n",
            " [0.93620793 0.92897045 0.92385557 0.23148696 0.91529593]\n",
            " [0.89166718 0.92725211 0.93949969 0.24642825 0.96553404]\n",
            " [0.94228011 0.92806871 0.95772145 0.16172806 0.95300094]\n",
            " [0.90819324 0.91469646 0.95453529 0.1632697  0.94990285]] -> [0.91202186]\n",
            "[[0.87100119 0.92881739 0.92768608 0.19061576 0.96634369]\n",
            " [0.94617873 0.95265271 0.97157563 0.19920229 0.94993809]\n",
            " [0.93620793 0.92897045 0.92385557 0.23148696 0.91529593]\n",
            " [0.89166718 0.92725211 0.93949969 0.24642825 0.96553404]\n",
            " [0.94228011 0.92806871 0.95772145 0.16172806 0.95300094]\n",
            " [0.90819324 0.91469646 0.95453529 0.1632697  0.94990285]\n",
            " [0.91260924 0.91275695 0.89296456 0.24129246 0.91202186]] -> [0.90445284]\n",
            "[[0.94617873 0.95265271 0.97157563 0.19920229 0.94993809]\n",
            " [0.93620793 0.92897045 0.92385557 0.23148696 0.91529593]\n",
            " [0.89166718 0.92725211 0.93949969 0.24642825 0.96553404]\n",
            " [0.94228011 0.92806871 0.95772145 0.16172806 0.95300094]\n",
            " [0.90819324 0.91469646 0.95453529 0.1632697  0.94990285]\n",
            " [0.91260924 0.91275695 0.89296456 0.24129246 0.91202186]\n",
            " [0.89080466 0.88424303 0.9174118  0.17719817 0.90445284]] -> [0.86717028]\n",
            "[[0.93620793 0.92897045 0.92385557 0.23148696 0.91529593]\n",
            " [0.89166718 0.92725211 0.93949969 0.24642825 0.96553404]\n",
            " [0.94228011 0.92806871 0.95772145 0.16172806 0.95300094]\n",
            " [0.90819324 0.91469646 0.95453529 0.1632697  0.94990285]\n",
            " [0.91260924 0.91275695 0.89296456 0.24129246 0.91202186]\n",
            " [0.89080466 0.88424303 0.9174118  0.17719817 0.90445284]\n",
            " [0.85047314 0.84974069 0.89206609 0.19866452 0.86717028]] -> [0.89850313]\n",
            "[[0.89166718 0.92725211 0.93949969 0.24642825 0.96553404]\n",
            " [0.94228011 0.92806871 0.95772145 0.16172806 0.95300094]\n",
            " [0.90819324 0.91469646 0.95453529 0.1632697  0.94990285]\n",
            " [0.91260924 0.91275695 0.89296456 0.24129246 0.91202186]\n",
            " [0.89080466 0.88424303 0.9174118  0.17719817 0.90445284]\n",
            " [0.85047314 0.84974069 0.89206609 0.19866452 0.86717028]\n",
            " [0.85264671 0.86001644 0.84703095 0.21552389 0.89850313]] -> [0.88311841]\n",
            "[[0.94228011 0.92806871 0.95772145 0.16172806 0.95300094]\n",
            " [0.90819324 0.91469646 0.95453529 0.1632697  0.94990285]\n",
            " [0.91260924 0.91275695 0.89296456 0.24129246 0.91202186]\n",
            " [0.89080466 0.88424303 0.9174118  0.17719817 0.90445284]\n",
            " [0.85047314 0.84974069 0.89206609 0.19866452 0.86717028]\n",
            " [0.85264671 0.86001644 0.84703095 0.21552389 0.89850313]\n",
            " [0.89132226 0.891831   0.91447626 0.23826297 0.88311841]] -> [0.93483508]\n",
            "[[0.90819324 0.91469646 0.95453529 0.1632697  0.94990285]\n",
            " [0.91260924 0.91275695 0.89296456 0.24129246 0.91202186]\n",
            " [0.89080466 0.88424303 0.9174118  0.17719817 0.90445284]\n",
            " [0.85047314 0.84974069 0.89206609 0.19866452 0.86717028]\n",
            " [0.85264671 0.86001644 0.84703095 0.21552389 0.89850313]\n",
            " [0.89132226 0.891831   0.91447626 0.23826297 0.88311841]\n",
            " [0.880972   0.90037159 0.9016781  0.17795106 0.93483508]] -> [0.90434712]\n",
            "[[0.91260924 0.91275695 0.89296456 0.24129246 0.91202186]\n",
            " [0.89080466 0.88424303 0.9174118  0.17719817 0.90445284]\n",
            " [0.85047314 0.84974069 0.89206609 0.19866452 0.86717028]\n",
            " [0.85264671 0.86001644 0.84703095 0.21552389 0.89850313]\n",
            " [0.89132226 0.891831   0.91447626 0.23826297 0.88311841]\n",
            " [0.880972   0.90037159 0.9016781  0.17795106 0.93483508]\n",
            " [0.92382202 0.90748294 0.9359198  0.1385229  0.90434712]] -> [0.86871933]\n",
            "[[0.89080466 0.88424303 0.9174118  0.17719817 0.90445284]\n",
            " [0.85047314 0.84974069 0.89206609 0.19866452 0.86717028]\n",
            " [0.85264671 0.86001644 0.84703095 0.21552389 0.89850313]\n",
            " [0.89132226 0.891831   0.91447626 0.23826297 0.88311841]\n",
            " [0.880972   0.90037159 0.9016781  0.17795106 0.93483508]\n",
            " [0.92382202 0.90748294 0.9359198  0.1385229  0.90434712]\n",
            " [0.86893123 0.87839064 0.89707802 0.28150937 0.86871933]] -> [0.89850313]\n",
            "[[0.85047314 0.84974069 0.89206609 0.19866452 0.86717028]\n",
            " [0.85264671 0.86001644 0.84703095 0.21552389 0.89850313]\n",
            " [0.89132226 0.891831   0.91447626 0.23826297 0.88311841]\n",
            " [0.880972   0.90037159 0.9016781  0.17795106 0.93483508]\n",
            " [0.92382202 0.90748294 0.9359198  0.1385229  0.90434712]\n",
            " [0.86893123 0.87839064 0.89707802 0.28150937 0.86871933]\n",
            " [0.86762018 0.86433783 0.90370074 0.13604015 0.89850313]] -> [0.90635385]\n",
            "[[0.85264671 0.86001644 0.84703095 0.21552389 0.89850313]\n",
            " [0.89132226 0.891831   0.91447626 0.23826297 0.88311841]\n",
            " [0.880972   0.90037159 0.9016781  0.17795106 0.93483508]\n",
            " [0.92382202 0.90748294 0.9359198  0.1385229  0.90434712]\n",
            " [0.86893123 0.87839064 0.89707802 0.28150937 0.86871933]\n",
            " [0.86762018 0.86433783 0.90370074 0.13604015 0.89850313]\n",
            " [0.88666472 0.88084042 0.92349767 0.12167249 0.90635385]] -> [0.90744521]\n",
            "[[0.89132226 0.891831   0.91447626 0.23826297 0.88311841]\n",
            " [0.880972   0.90037159 0.9016781  0.17795106 0.93483508]\n",
            " [0.92382202 0.90748294 0.9359198  0.1385229  0.90434712]\n",
            " [0.86893123 0.87839064 0.89707802 0.28150937 0.86871933]\n",
            " [0.86762018 0.86433783 0.90370074 0.13604015 0.89850313]\n",
            " [0.88666472 0.88084042 0.92349767 0.12167249 0.90635385]\n",
            " [0.8929437  0.8786629  0.91802032 0.13964327 0.90744521]] -> [0.90072108]\n",
            "[[0.880972   0.90037159 0.9016781  0.17795106 0.93483508]\n",
            " [0.92382202 0.90748294 0.9359198  0.1385229  0.90434712]\n",
            " [0.86893123 0.87839064 0.89707802 0.28150937 0.86871933]\n",
            " [0.86762018 0.86433783 0.90370074 0.13604015 0.89850313]\n",
            " [0.88666472 0.88084042 0.92349767 0.12167249 0.90635385]\n",
            " [0.8929437  0.8786629  0.91802032 0.13964327 0.90744521]\n",
            " [0.87941942 0.86893127 0.92739963 0.04654477 0.90072108]] -> [0.95039578]\n",
            "[[0.92382202 0.90748294 0.9359198  0.1385229  0.90434712]\n",
            " [0.86893123 0.87839064 0.89707802 0.28150937 0.86871933]\n",
            " [0.86762018 0.86433783 0.90370074 0.13604015 0.89850313]\n",
            " [0.88666472 0.88084042 0.92349767 0.12167249 0.90635385]\n",
            " [0.8929437  0.8786629  0.91802032 0.13964327 0.90744521]\n",
            " [0.87941942 0.86893127 0.92739963 0.04654477 0.90072108]\n",
            " [0.89104619 0.90853774 0.93778141 0.135108   0.95039578]] -> [1.]\n",
            "[[0.86893123 0.87839064 0.89707802 0.28150937 0.86871933]\n",
            " [0.86762018 0.86433783 0.90370074 0.13604015 0.89850313]\n",
            " [0.88666472 0.88084042 0.92349767 0.12167249 0.90635385]\n",
            " [0.8929437  0.8786629  0.91802032 0.13964327 0.90744521]\n",
            " [0.87941942 0.86893127 0.92739963 0.04654477 0.90072108]\n",
            " [0.89104619 0.90853774 0.93778141 0.135108   0.95039578]\n",
            " [0.93855396 0.9663481  0.99831734 0.15748857 1.        ]] -> [0.98028509]\n",
            "[[0.86762018 0.86433783 0.90370074 0.13604015 0.89850313]\n",
            " [0.88666472 0.88084042 0.92349767 0.12167249 0.90635385]\n",
            " [0.8929437  0.8786629  0.91802032 0.13964327 0.90744521]\n",
            " [0.87941942 0.86893127 0.92739963 0.04654477 0.90072108]\n",
            " [0.89104619 0.90853774 0.93778141 0.135108   0.95039578]\n",
            " [0.93855396 0.9663481  0.99831734 0.15748857 1.        ]\n",
            " [0.97274423 0.95824987 1.         0.11521018 0.98028509]] -> [0.93761622]\n",
            "[[0.88666472 0.88084042 0.92349767 0.12167249 0.90635385]\n",
            " [0.8929437  0.8786629  0.91802032 0.13964327 0.90744521]\n",
            " [0.87941942 0.86893127 0.92739963 0.04654477 0.90072108]\n",
            " [0.89104619 0.90853774 0.93778141 0.135108   0.95039578]\n",
            " [0.93855396 0.9663481  0.99831734 0.15748857 1.        ]\n",
            " [0.97274423 0.95824987 1.         0.11521018 0.98028509]\n",
            " [0.94824869 0.93068879 0.96935611 0.13280452 0.93761622]] -> [0.87762638]\n",
            "[[0.8929437  0.8786629  0.91802032 0.13964327 0.90744521]\n",
            " [0.87941942 0.86893127 0.92739963 0.04654477 0.90072108]\n",
            " [0.89104619 0.90853774 0.93778141 0.135108   0.95039578]\n",
            " [0.93855396 0.9663481  0.99831734 0.15748857 1.        ]\n",
            " [0.97274423 0.95824987 1.         0.11521018 0.98028509]\n",
            " [0.94824869 0.93068879 0.96935611 0.13280452 0.93761622]\n",
            " [0.85682139 0.8441263  0.87240529 0.29263243 0.87762638]] -> [0.88023154]\n",
            "[[0.87941942 0.86893127 0.92739963 0.04654477 0.90072108]\n",
            " [0.89104619 0.90853774 0.93778141 0.135108   0.95039578]\n",
            " [0.93855396 0.9663481  0.99831734 0.15748857 1.        ]\n",
            " [0.97274423 0.95824987 1.         0.11521018 0.98028509]\n",
            " [0.94824869 0.93068879 0.96935611 0.13280452 0.93761622]\n",
            " [0.85682139 0.8441263  0.87240529 0.29263243 0.87762638]\n",
            " [0.86872423 0.87114305 0.89883214 0.17413283 0.88023154]] -> [0.88389282]\n",
            "[[0.89104619 0.90853774 0.93778141 0.135108   0.95039578]\n",
            " [0.93855396 0.9663481  0.99831734 0.15748857 1.        ]\n",
            " [0.97274423 0.95824987 1.         0.11521018 0.98028509]\n",
            " [0.94824869 0.93068879 0.96935611 0.13280452 0.93761622]\n",
            " [0.85682139 0.8441263  0.87240529 0.29263243 0.87762638]\n",
            " [0.86872423 0.87114305 0.89883214 0.17413283 0.88023154]\n",
            " [0.81197026 0.85474244 0.86403545 0.1738012  0.88389282]] -> [0.82323407]\n",
            "[[0.93855396 0.9663481  0.99831734 0.15748857 1.        ]\n",
            " [0.97274423 0.95824987 1.         0.11521018 0.98028509]\n",
            " [0.94824869 0.93068879 0.96935611 0.13280452 0.93761622]\n",
            " [0.85682139 0.8441263  0.87240529 0.29263243 0.87762638]\n",
            " [0.86872423 0.87114305 0.89883214 0.17413283 0.88023154]\n",
            " [0.81197026 0.85474244 0.86403545 0.1738012  0.88389282]\n",
            " [0.81303978 0.82520778 0.82873774 0.26492785 0.82323407]] -> [0.78126914]\n",
            "[[0.97274423 0.95824987 1.         0.11521018 0.98028509]\n",
            " [0.94824869 0.93068879 0.96935611 0.13280452 0.93761622]\n",
            " [0.85682139 0.8441263  0.87240529 0.29263243 0.87762638]\n",
            " [0.86872423 0.87114305 0.89883214 0.17413283 0.88023154]\n",
            " [0.81197026 0.85474244 0.86403545 0.1738012  0.88389282]\n",
            " [0.81303978 0.82520778 0.82873774 0.26492785 0.82323407]\n",
            " [0.81697293 0.80727594 0.80704359 0.21896567 0.78126914]] -> [0.78676138]\n",
            "[[0.94824869 0.93068879 0.96935611 0.13280452 0.93761622]\n",
            " [0.85682139 0.8441263  0.87240529 0.29263243 0.87762638]\n",
            " [0.86872423 0.87114305 0.89883214 0.17413283 0.88023154]\n",
            " [0.81197026 0.85474244 0.86403545 0.1738012  0.88389282]\n",
            " [0.81303978 0.82520778 0.82873774 0.26492785 0.82323407]\n",
            " [0.81697293 0.80727594 0.80704359 0.21896567 0.78126914]\n",
            " [0.76577354 0.75836337 0.7731777  0.18667204 0.78676138]] -> [0.82210748]\n",
            "[[0.85682139 0.8441263  0.87240529 0.29263243 0.87762638]\n",
            " [0.86872423 0.87114305 0.89883214 0.17413283 0.88023154]\n",
            " [0.81197026 0.85474244 0.86403545 0.1738012  0.88389282]\n",
            " [0.81303978 0.82520778 0.82873774 0.26492785 0.82323407]\n",
            " [0.81697293 0.80727594 0.80704359 0.21896567 0.78126914]\n",
            " [0.76577354 0.75836337 0.7731777  0.18667204 0.78676138]\n",
            " [0.78326551 0.7920323  0.82249805 0.18074751 0.82210748]] -> [0.7322986]\n",
            "[[0.86872423 0.87114305 0.89883214 0.17413283 0.88023154]\n",
            " [0.81197026 0.85474244 0.86403545 0.1738012  0.88389282]\n",
            " [0.81303978 0.82520778 0.82873774 0.26492785 0.82323407]\n",
            " [0.81697293 0.80727594 0.80704359 0.21896567 0.78126914]\n",
            " [0.76577354 0.75836337 0.7731777  0.18667204 0.78676138]\n",
            " [0.78326551 0.7920323  0.82249805 0.18074751 0.82210748]\n",
            " [0.81490275 0.81241392 0.75552885 0.22351887 0.7322986 ]] -> [0.78214928]\n",
            "[[0.81197026 0.85474244 0.86403545 0.1738012  0.88389282]\n",
            " [0.81303978 0.82520778 0.82873774 0.26492785 0.82323407]\n",
            " [0.81697293 0.80727594 0.80704359 0.21896567 0.78126914]\n",
            " [0.76577354 0.75836337 0.7731777  0.18667204 0.78676138]\n",
            " [0.78326551 0.7920323  0.82249805 0.18074751 0.82210748]\n",
            " [0.81490275 0.81241392 0.75552885 0.22351887 0.7322986 ]\n",
            " [0.72702913 0.76880942 0.72148401 0.19879    0.78214928]] -> [0.71078818]\n",
            "[[0.81303978 0.82520778 0.82873774 0.26492785 0.82323407]\n",
            " [0.81697293 0.80727594 0.80704359 0.21896567 0.78126914]\n",
            " [0.76577354 0.75836337 0.7731777  0.18667204 0.78676138]\n",
            " [0.78326551 0.7920323  0.82249805 0.18074751 0.82210748]\n",
            " [0.81490275 0.81241392 0.75552885 0.22351887 0.7322986 ]\n",
            " [0.72702913 0.76880942 0.72148401 0.19879    0.78214928]\n",
            " [0.6818674  0.71714075 0.70813107 0.32127812 0.71078818]] -> [0.73662878]\n",
            "[[0.81697293 0.80727594 0.80704359 0.21896567 0.78126914]\n",
            " [0.76577354 0.75836337 0.7731777  0.18667204 0.78676138]\n",
            " [0.78326551 0.7920323  0.82249805 0.18074751 0.82210748]\n",
            " [0.81490275 0.81241392 0.75552885 0.22351887 0.7322986 ]\n",
            " [0.72702913 0.76880942 0.72148401 0.19879    0.78214928]\n",
            " [0.6818674  0.71714075 0.70813107 0.32127812 0.71078818]\n",
            " [0.71985289 0.72816518 0.73691335 0.20258134 0.73662878]] -> [0.72487032]\n",
            "[[0.76577354 0.75836337 0.7731777  0.18667204 0.78676138]\n",
            " [0.78326551 0.7920323  0.82249805 0.18074751 0.82210748]\n",
            " [0.81490275 0.81241392 0.75552885 0.22351887 0.7322986 ]\n",
            " [0.72702913 0.76880942 0.72148401 0.19879    0.78214928]\n",
            " [0.6818674  0.71714075 0.70813107 0.32127812 0.71078818]\n",
            " [0.71985289 0.72816518 0.73691335 0.20258134 0.73662878]\n",
            " [0.66917111 0.71751499 0.6647786  0.30806668 0.72487032]] -> [0.75352752]\n",
            "[[0.78326551 0.7920323  0.82249805 0.18074751 0.82210748]\n",
            " [0.81490275 0.81241392 0.75552885 0.22351887 0.7322986 ]\n",
            " [0.72702913 0.76880942 0.72148401 0.19879    0.78214928]\n",
            " [0.6818674  0.71714075 0.70813107 0.32127812 0.71078818]\n",
            " [0.71985289 0.72816518 0.73691335 0.20258134 0.73662878]\n",
            " [0.66917111 0.71751499 0.6647786  0.30806668 0.72487032]\n",
            " [0.71598881 0.75950332 0.74067241 0.215497   0.75352752]] -> [0.81922061]\n",
            "[[0.81490275 0.81241392 0.75552885 0.22351887 0.7322986 ]\n",
            " [0.72702913 0.76880942 0.72148401 0.19879    0.78214928]\n",
            " [0.6818674  0.71714075 0.70813107 0.32127812 0.71078818]\n",
            " [0.71985289 0.72816518 0.73691335 0.20258134 0.73662878]\n",
            " [0.66917111 0.71751499 0.6647786  0.30806668 0.72487032]\n",
            " [0.71598881 0.75950332 0.74067241 0.215497   0.75352752]\n",
            " [0.78988962 0.78992269 0.83253592 0.17960921 0.81922061]] -> [0.77141169]\n",
            "[[0.72702913 0.76880942 0.72148401 0.19879    0.78214928]\n",
            " [0.6818674  0.71714075 0.70813107 0.32127812 0.71078818]\n",
            " [0.71985289 0.72816518 0.73691335 0.20258134 0.73662878]\n",
            " [0.66917111 0.71751499 0.6647786  0.30806668 0.72487032]\n",
            " [0.71598881 0.75950332 0.74067241 0.215497   0.75352752]\n",
            " [0.78988962 0.78992269 0.83253592 0.17960921 0.81922061]\n",
            " [0.78982076 0.7951967  0.79633974 0.1527113  0.77141169]] -> [0.7762348]\n",
            "[[0.6818674  0.71714075 0.70813107 0.32127812 0.71078818]\n",
            " [0.71985289 0.72816518 0.73691335 0.20258134 0.73662878]\n",
            " [0.66917111 0.71751499 0.6647786  0.30806668 0.72487032]\n",
            " [0.71598881 0.75950332 0.74067241 0.215497   0.75352752]\n",
            " [0.78988962 0.78992269 0.83253592 0.17960921 0.81922061]\n",
            " [0.78982076 0.7951967  0.79633974 0.1527113  0.77141169]\n",
            " [0.75625127 0.75640704 0.7837026  0.11865197 0.7762348 ]] -> [0.73029186]\n",
            "[[0.71985289 0.72816518 0.73691335 0.20258134 0.73662878]\n",
            " [0.66917111 0.71751499 0.6647786  0.30806668 0.72487032]\n",
            " [0.71598881 0.75950332 0.74067241 0.215497   0.75352752]\n",
            " [0.78988962 0.78992269 0.83253592 0.17960921 0.81922061]\n",
            " [0.78982076 0.7951967  0.79633974 0.1527113  0.77141169]\n",
            " [0.75625127 0.75640704 0.7837026  0.11865197 0.7762348 ]\n",
            " [0.75563028 0.75625377 0.7404218  0.19595769 0.73029186]] -> [0.83932294]\n",
            "[[0.66917111 0.71751499 0.6647786  0.30806668 0.72487032]\n",
            " [0.71598881 0.75950332 0.74067241 0.215497   0.75352752]\n",
            " [0.78988962 0.78992269 0.83253592 0.17960921 0.81922061]\n",
            " [0.78982076 0.7951967  0.79633974 0.1527113  0.77141169]\n",
            " [0.75625127 0.75640704 0.7837026  0.11865197 0.7762348 ]\n",
            " [0.75563028 0.75625377 0.7404218  0.19595769 0.73029186]\n",
            " [0.78512848 0.80884121 0.80471657 0.2391772  0.83932294]] -> [0.88153412]\n",
            "[[0.71598881 0.75950332 0.74067241 0.215497   0.75352752]\n",
            " [0.78988962 0.78992269 0.83253592 0.17960921 0.81922061]\n",
            " [0.78982076 0.7951967  0.79633974 0.1527113  0.77141169]\n",
            " [0.75625127 0.75640704 0.7837026  0.11865197 0.7762348 ]\n",
            " [0.75563028 0.75625377 0.7404218  0.19595769 0.73029186]\n",
            " [0.78512848 0.80884121 0.80471657 0.2391772  0.83932294]\n",
            " [0.81724899 0.8472907  0.85644609 0.31069284 0.88153412]] -> [0.91339492]\n",
            "[[0.78988962 0.78992269 0.83253592 0.17960921 0.81922061]\n",
            " [0.78982076 0.7951967  0.79633974 0.1527113  0.77141169]\n",
            " [0.75625127 0.75640704 0.7837026  0.11865197 0.7762348 ]\n",
            " [0.75563028 0.75625377 0.7404218  0.19595769 0.73029186]\n",
            " [0.78512848 0.80884121 0.80471657 0.2391772  0.83932294]\n",
            " [0.81724899 0.8472907  0.85644609 0.31069284 0.88153412]\n",
            " [0.88255911 0.89108232 0.91540707 0.45991754 0.91339492]] -> [0.95792978]\n",
            "[[0.78982076 0.7951967  0.79633974 0.1527113  0.77141169]\n",
            " [0.75625127 0.75640704 0.7837026  0.11865197 0.7762348 ]\n",
            " [0.75563028 0.75625377 0.7404218  0.19595769 0.73029186]\n",
            " [0.78512848 0.80884121 0.80471657 0.2391772  0.83932294]\n",
            " [0.81724899 0.8472907  0.85644609 0.31069284 0.88153412]\n",
            " [0.88255911 0.89108232 0.91540707 0.45991754 0.91339492]\n",
            " [1.         1.         0.99194524 0.56827104 0.95792978]] -> [0.82520556]\n",
            "[[0.75625127 0.75640704 0.7837026  0.11865197 0.7762348 ]\n",
            " [0.75563028 0.75625377 0.7404218  0.19595769 0.73029186]\n",
            " [0.78512848 0.80884121 0.80471657 0.2391772  0.83932294]\n",
            " [0.81724899 0.8472907  0.85644609 0.31069284 0.88153412]\n",
            " [0.88255911 0.89108232 0.91540707 0.45991754 0.91339492]\n",
            " [1.         1.         0.99194524 0.56827104 0.95792978]\n",
            " [0.95073266 0.94770185 0.8338928  0.5523976  0.82520556]] -> [0.75852662]\n",
            "[[0.75563028 0.75625377 0.7404218  0.19595769 0.73029186]\n",
            " [0.78512848 0.80884121 0.80471657 0.2391772  0.83932294]\n",
            " [0.81724899 0.8472907  0.85644609 0.31069284 0.88153412]\n",
            " [0.88255911 0.89108232 0.91540707 0.45991754 0.91339492]\n",
            " [1.         1.         0.99194524 0.56827104 0.95792978]\n",
            " [0.95073266 0.94770185 0.8338928  0.5523976  0.82520556]\n",
            " [0.78716413 0.78607772 0.7671635  0.46256162 0.75852662]] -> [0.67248474]\n",
            "[[0.78512848 0.80884121 0.80471657 0.2391772  0.83932294]\n",
            " [0.81724899 0.8472907  0.85644609 0.31069284 0.88153412]\n",
            " [0.88255911 0.89108232 0.91540707 0.45991754 0.91339492]\n",
            " [1.         1.         0.99194524 0.56827104 0.95792978]\n",
            " [0.95073266 0.94770185 0.8338928  0.5523976  0.82520556]\n",
            " [0.78716413 0.78607772 0.7671635  0.46256162 0.75852662]\n",
            " [0.72181947 0.70778356 0.68944413 0.45691494 0.67248474]] -> [0.66956263]\n",
            "[[0.81724899 0.8472907  0.85644609 0.31069284 0.88153412]\n",
            " [0.88255911 0.89108232 0.91540707 0.45991754 0.91339492]\n",
            " [1.         1.         0.99194524 0.56827104 0.95792978]\n",
            " [0.95073266 0.94770185 0.8338928  0.5523976  0.82520556]\n",
            " [0.78716413 0.78607772 0.7671635  0.46256162 0.75852662]\n",
            " [0.72181947 0.70778356 0.68944413 0.45691494 0.67248474]\n",
            " [0.59754727 0.63986753 0.62826363 0.37998566 0.66956263]] -> [0.65326254]\n",
            "[[0.88255911 0.89108232 0.91540707 0.45991754 0.91339492]\n",
            " [1.         1.         0.99194524 0.56827104 0.95792978]\n",
            " [0.95073266 0.94770185 0.8338928  0.5523976  0.82520556]\n",
            " [0.78716413 0.78607772 0.7671635  0.46256162 0.75852662]\n",
            " [0.72181947 0.70778356 0.68944413 0.45691494 0.67248474]\n",
            " [0.59754727 0.63986753 0.62826363 0.37998566 0.66956263]\n",
            " [0.61296926 0.69386699 0.64870491 0.32275701 0.65326254]] -> [0.67442099]\n",
            "[[1.         1.         0.99194524 0.56827104 0.95792978]\n",
            " [0.95073266 0.94770185 0.8338928  0.5523976  0.82520556]\n",
            " [0.78716413 0.78607772 0.7671635  0.46256162 0.75852662]\n",
            " [0.72181947 0.70778356 0.68944413 0.45691494 0.67248474]\n",
            " [0.59754727 0.63986753 0.62826363 0.37998566 0.66956263]\n",
            " [0.61296926 0.69386699 0.64870491 0.32275701 0.65326254]\n",
            " [0.66313345 0.69866458 0.69653225 0.23573541 0.67442099]] -> [0.67086521]\n",
            "[[0.95073266 0.94770185 0.8338928  0.5523976  0.82520556]\n",
            " [0.78716413 0.78607772 0.7671635  0.46256162 0.75852662]\n",
            " [0.72181947 0.70778356 0.68944413 0.45691494 0.67248474]\n",
            " [0.59754727 0.63986753 0.62826363 0.37998566 0.66956263]\n",
            " [0.61296926 0.69386699 0.64870491 0.32275701 0.65326254]\n",
            " [0.66313345 0.69866458 0.69653225 0.23573541 0.67442099]\n",
            " [0.62221547 0.65796926 0.6490556  0.27033253 0.67086521]] -> [0.66836577]\n",
            "[[0.78716413 0.78607772 0.7671635  0.46256162 0.75852662]\n",
            " [0.72181947 0.70778356 0.68944413 0.45691494 0.67248474]\n",
            " [0.59754727 0.63986753 0.62826363 0.37998566 0.66956263]\n",
            " [0.61296926 0.69386699 0.64870491 0.32275701 0.65326254]\n",
            " [0.66313345 0.69866458 0.69653225 0.23573541 0.67442099]\n",
            " [0.62221547 0.65796926 0.6490556  0.27033253 0.67086521]\n",
            " [0.67486384 0.67294084 0.68389512 0.19122524 0.66836577]] -> [0.69864229]\n",
            "[[0.72181947 0.70778356 0.68944413 0.45691494 0.67248474]\n",
            " [0.59754727 0.63986753 0.62826363 0.37998566 0.66956263]\n",
            " [0.61296926 0.69386699 0.64870491 0.32275701 0.65326254]\n",
            " [0.66313345 0.69866458 0.69653225 0.23573541 0.67442099]\n",
            " [0.62221547 0.65796926 0.6490556  0.27033253 0.67086521]\n",
            " [0.67486384 0.67294084 0.68389512 0.19122524 0.66836577]\n",
            " [0.68424797 0.68740194 0.70698548 0.22515909 0.69864229]] -> [0.75989968]\n",
            "[[0.59754727 0.63986753 0.62826363 0.37998566 0.66956263]\n",
            " [0.61296926 0.69386699 0.64870491 0.32275701 0.65326254]\n",
            " [0.66313345 0.69866458 0.69653225 0.23573541 0.67442099]\n",
            " [0.62221547 0.65796926 0.6490556  0.27033253 0.67086521]\n",
            " [0.67486384 0.67294084 0.68389512 0.19122524 0.66836577]\n",
            " [0.68424797 0.68740194 0.70698548 0.22515909 0.69864229]\n",
            " [0.70187807 0.72738265 0.72964628 0.22270324 0.75989968]] -> [0.7209976]\n",
            "[[0.61296926 0.69386699 0.64870491 0.32275701 0.65326254]\n",
            " [0.66313345 0.69866458 0.69653225 0.23573541 0.67442099]\n",
            " [0.62221547 0.65796926 0.6490556  0.27033253 0.67086521]\n",
            " [0.67486384 0.67294084 0.68389512 0.19122524 0.66836577]\n",
            " [0.68424797 0.68740194 0.70698548 0.22515909 0.69864229]\n",
            " [0.70187807 0.72738265 0.72964628 0.22270324 0.75989968]\n",
            " [0.74296852 0.73622936 0.74629287 0.16808282 0.7209976 ]] -> [0.73353069]\n",
            "[[0.66313345 0.69866458 0.69653225 0.23573541 0.67442099]\n",
            " [0.62221547 0.65796926 0.6490556  0.27033253 0.67086521]\n",
            " [0.67486384 0.67294084 0.68389512 0.19122524 0.66836577]\n",
            " [0.68424797 0.68740194 0.70698548 0.22515909 0.69864229]\n",
            " [0.70187807 0.72738265 0.72964628 0.22270324 0.75989968]\n",
            " [0.74296852 0.73622936 0.74629287 0.16808282 0.7209976 ]\n",
            " [0.69132082 0.7046906  0.73920454 0.14174061 0.73353069]] -> [0.75306984]\n",
            "[[0.62221547 0.65796926 0.6490556  0.27033253 0.67086521]\n",
            " [0.67486384 0.67294084 0.68389512 0.19122524 0.66836577]\n",
            " [0.68424797 0.68740194 0.70698548 0.22515909 0.69864229]\n",
            " [0.70187807 0.72738265 0.72964628 0.22270324 0.75989968]\n",
            " [0.74296852 0.73622936 0.74629287 0.16808282 0.7209976 ]\n",
            " [0.69132082 0.7046906  0.73920454 0.14174061 0.73353069]\n",
            " [0.73417084 0.73925773 0.76949052 0.17405216 0.75306984]] -> [0.7157168]\n",
            "[[0.67486384 0.67294084 0.68389512 0.19122524 0.66836577]\n",
            " [0.68424797 0.68740194 0.70698548 0.22515909 0.69864229]\n",
            " [0.70187807 0.72738265 0.72964628 0.22270324 0.75989968]\n",
            " [0.74296852 0.73622936 0.74629287 0.16808282 0.7209976 ]\n",
            " [0.69132082 0.7046906  0.73920454 0.14174061 0.73353069]\n",
            " [0.73417084 0.73925773 0.76949052 0.17405216 0.75306984]\n",
            " [0.71347032 0.7227892  0.73752209 0.17938514 0.7157168 ]] -> [0.72877806]\n",
            "[[0.68424797 0.68740194 0.70698548 0.22515909 0.69864229]\n",
            " [0.70187807 0.72738265 0.72964628 0.22270324 0.75989968]\n",
            " [0.74296852 0.73622936 0.74629287 0.16808282 0.7209976 ]\n",
            " [0.69132082 0.7046906  0.73920454 0.14174061 0.73353069]\n",
            " [0.73417084 0.73925773 0.76949052 0.17405216 0.75306984]\n",
            " [0.71347032 0.7227892  0.73752209 0.17938514 0.7157168 ]\n",
            " [0.67024063 0.69420717 0.69169948 0.17528906 0.72877806]] -> [0.75057018]\n",
            "[[0.70187807 0.72738265 0.72964628 0.22270324 0.75989968]\n",
            " [0.74296852 0.73622936 0.74629287 0.16808282 0.7209976 ]\n",
            " [0.69132082 0.7046906  0.73920454 0.14174061 0.73353069]\n",
            " [0.73417084 0.73925773 0.76949052 0.17405216 0.75306984]\n",
            " [0.71347032 0.7227892  0.73752209 0.17938514 0.7157168 ]\n",
            " [0.67024063 0.69420717 0.69169948 0.17528906 0.72877806]\n",
            " [0.70850218 0.71455473 0.72680032 0.14648203 0.75057018]] -> [0.74817624]\n",
            "[[0.74296852 0.73622936 0.74629287 0.16808282 0.7209976 ]\n",
            " [0.69132082 0.7046906  0.73920454 0.14174061 0.73353069]\n",
            " [0.73417084 0.73925773 0.76949052 0.17405216 0.75306984]\n",
            " [0.71347032 0.7227892  0.73752209 0.17938514 0.7157168 ]\n",
            " [0.67024063 0.69420717 0.69169948 0.17528906 0.72877806]\n",
            " [0.70850218 0.71455473 0.72680032 0.14648203 0.75057018]\n",
            " [0.73806945 0.73990424 0.76358361 0.20037645 0.74817624]] -> [0.72247638]\n",
            "[[0.69132082 0.7046906  0.73920454 0.14174061 0.73353069]\n",
            " [0.73417084 0.73925773 0.76949052 0.17405216 0.75306984]\n",
            " [0.71347032 0.7227892  0.73752209 0.17938514 0.7157168 ]\n",
            " [0.67024063 0.69420717 0.69169948 0.17528906 0.72877806]\n",
            " [0.70850218 0.71455473 0.72680032 0.14648203 0.75057018]\n",
            " [0.73806945 0.73990424 0.76358361 0.20037645 0.74817624]\n",
            " [0.7095717  0.73126168 0.75219957 0.22167249 0.72247638]] -> [0.79654836]\n",
            "[[0.73417084 0.73925773 0.76949052 0.17405216 0.75306984]\n",
            " [0.71347032 0.7227892  0.73752209 0.17938514 0.7157168 ]\n",
            " [0.67024063 0.69420717 0.69169948 0.17528906 0.72877806]\n",
            " [0.70850218 0.71455473 0.72680032 0.14648203 0.75057018]\n",
            " [0.73806945 0.73990424 0.76358361 0.20037645 0.74817624]\n",
            " [0.7095717  0.73126168 0.75219957 0.22167249 0.72247638]\n",
            " [0.72095695 0.75821032 0.75968165 0.19212154 0.79654836]] -> [0.7966891]\n",
            "[[0.71347032 0.7227892  0.73752209 0.17938514 0.7157168 ]\n",
            " [0.67024063 0.69420717 0.69169948 0.17528906 0.72877806]\n",
            " [0.70850218 0.71455473 0.72680032 0.14648203 0.75057018]\n",
            " [0.73806945 0.73990424 0.76358361 0.20037645 0.74817624]\n",
            " [0.7095717  0.73126168 0.75219957 0.22167249 0.72247638]\n",
            " [0.72095695 0.75821032 0.75968165 0.19212154 0.79654836]\n",
            " [0.7740193  0.76225943 0.80346369 0.14529892 0.7966891 ]] -> [0.77405209]\n",
            "[[0.67024063 0.69420717 0.69169948 0.17528906 0.72877806]\n",
            " [0.70850218 0.71455473 0.72680032 0.14648203 0.75057018]\n",
            " [0.73806945 0.73990424 0.76358361 0.20037645 0.74817624]\n",
            " [0.7095717  0.73126168 0.75219957 0.22167249 0.72247638]\n",
            " [0.72095695 0.75821032 0.75968165 0.19212154 0.79654836]\n",
            " [0.7740193  0.76225943 0.80346369 0.14529892 0.7966891 ]\n",
            " [0.77291525 0.76038804 0.78205599 0.17478713 0.77405209]] -> [0.76866578]\n",
            "[[0.70850218 0.71455473 0.72680032 0.14648203 0.75057018]\n",
            " [0.73806945 0.73990424 0.76358361 0.20037645 0.74817624]\n",
            " [0.7095717  0.73126168 0.75219957 0.22167249 0.72247638]\n",
            " [0.72095695 0.75821032 0.75968165 0.19212154 0.79654836]\n",
            " [0.7740193  0.76225943 0.80346369 0.14529892 0.7966891 ]\n",
            " [0.77291525 0.76038804 0.78205599 0.17478713 0.77405209]\n",
            " [0.76018442 0.75031623 0.78205599 0.17605091 0.76866578]] -> [0.71328762]\n",
            "[[0.73806945 0.73990424 0.76358361 0.20037645 0.74817624]\n",
            " [0.7095717  0.73126168 0.75219957 0.22167249 0.72247638]\n",
            " [0.72095695 0.75821032 0.75968165 0.19212154 0.79654836]\n",
            " [0.7740193  0.76225943 0.80346369 0.14529892 0.7966891 ]\n",
            " [0.77291525 0.76038804 0.78205599 0.17478713 0.77405209]\n",
            " [0.76018442 0.75031623 0.78205599 0.17605091 0.76866578]\n",
            " [0.73227333 0.72173772 0.71360842 0.26684593 0.71328762]] -> [0.70909818]\n",
            "[[0.7095717  0.73126168 0.75219957 0.22167249 0.72247638]\n",
            " [0.72095695 0.75821032 0.75968165 0.19212154 0.79654836]\n",
            " [0.7740193  0.76225943 0.80346369 0.14529892 0.7966891 ]\n",
            " [0.77291525 0.76038804 0.78205599 0.17478713 0.77405209]\n",
            " [0.76018442 0.75031623 0.78205599 0.17605091 0.76866578]\n",
            " [0.73227333 0.72173772 0.71360842 0.26684593 0.71328762]\n",
            " [0.66910225 0.707103   0.70802379 0.18539034 0.70909818]] -> [0.74877467]\n",
            "[[0.72095695 0.75821032 0.75968165 0.19212154 0.79654836]\n",
            " [0.7740193  0.76225943 0.80346369 0.14529892 0.7966891 ]\n",
            " [0.77291525 0.76038804 0.78205599 0.17478713 0.77405209]\n",
            " [0.76018442 0.75031623 0.78205599 0.17605091 0.76866578]\n",
            " [0.73227333 0.72173772 0.71360842 0.26684593 0.71328762]\n",
            " [0.66910225 0.707103   0.70802379 0.18539034 0.70909818]\n",
            " [0.70318891 0.71353399 0.73902558 0.12670073 0.74877467]] -> [0.77546039]\n",
            "[[0.7740193  0.76225943 0.80346369 0.14529892 0.7966891 ]\n",
            " [0.77291525 0.76038804 0.78205599 0.17478713 0.77405209]\n",
            " [0.76018442 0.75031623 0.78205599 0.17605091 0.76866578]\n",
            " [0.73227333 0.72173772 0.71360842 0.26684593 0.71328762]\n",
            " [0.66910225 0.707103   0.70802379 0.18539034 0.70909818]\n",
            " [0.70318891 0.71353399 0.73902558 0.12670073 0.74877467]\n",
            " [0.73648234 0.75014614 0.77253335 0.25325804 0.77546039]] -> [0.82474788]\n",
            "[[0.77291525 0.76038804 0.78205599 0.17478713 0.77405209]\n",
            " [0.76018442 0.75031623 0.78205599 0.17605091 0.76866578]\n",
            " [0.73227333 0.72173772 0.71360842 0.26684593 0.71328762]\n",
            " [0.66910225 0.707103   0.70802379 0.18539034 0.70909818]\n",
            " [0.70318891 0.71353399 0.73902558 0.12670073 0.74877467]\n",
            " [0.73648234 0.75014614 0.77253335 0.25325804 0.77546039]\n",
            " [0.77746939 0.78580546 0.82181065 0.17593439 0.82474788]] -> [0.83766818]\n",
            "[[0.76018442 0.75031623 0.78205599 0.17605091 0.76866578]\n",
            " [0.73227333 0.72173772 0.71360842 0.26684593 0.71328762]\n",
            " [0.66910225 0.707103   0.70802379 0.18539034 0.70909818]\n",
            " [0.70318891 0.71353399 0.73902558 0.12670073 0.74877467]\n",
            " [0.73648234 0.75014614 0.77253335 0.25325804 0.77546039]\n",
            " [0.77746939 0.78580546 0.82181065 0.17593439 0.82474788]\n",
            " [0.80096447 0.81499994 0.85053939 0.15330286 0.83766818]] -> [0.83006392]\n",
            "[[0.73227333 0.72173772 0.71360842 0.26684593 0.71328762]\n",
            " [0.66910225 0.707103   0.70802379 0.18539034 0.70909818]\n",
            " [0.70318891 0.71353399 0.73902558 0.12670073 0.74877467]\n",
            " [0.73648234 0.75014614 0.77253335 0.25325804 0.77546039]\n",
            " [0.77746939 0.78580546 0.82181065 0.17593439 0.82474788]\n",
            " [0.80096447 0.81499994 0.85053939 0.15330286 0.83766818]\n",
            " [0.80134393 0.80407747 0.84917902 0.15354486 0.83006392]] -> [0.85738331]\n",
            "[[0.66910225 0.707103   0.70802379 0.18539034 0.70909818]\n",
            " [0.70318891 0.71353399 0.73902558 0.12670073 0.74877467]\n",
            " [0.73648234 0.75014614 0.77253335 0.25325804 0.77546039]\n",
            " [0.77746939 0.78580546 0.82181065 0.17593439 0.82474788]\n",
            " [0.80096447 0.81499994 0.85053939 0.15330286 0.83766818]\n",
            " [0.80134393 0.80407747 0.84917902 0.15354486 0.83006392]\n",
            " [0.79944642 0.82170299 0.84824821 0.14488662 0.85738331]] -> [0.86333302]\n",
            "[[0.70318891 0.71353399 0.73902558 0.12670073 0.74877467]\n",
            " [0.73648234 0.75014614 0.77253335 0.25325804 0.77546039]\n",
            " [0.77746939 0.78580546 0.82181065 0.17593439 0.82474788]\n",
            " [0.80096447 0.81499994 0.85053939 0.15330286 0.83766818]\n",
            " [0.80134393 0.80407747 0.84917902 0.15354486 0.83006392]\n",
            " [0.79944642 0.82170299 0.84824821 0.14488662 0.85738331]\n",
            " [0.83422336 0.84075774 0.88938117 0.16607511 0.86333302]] -> [0.86269913]\n",
            "[[0.73648234 0.75014614 0.77253335 0.25325804 0.77546039]\n",
            " [0.77746939 0.78580546 0.82181065 0.17593439 0.82474788]\n",
            " [0.80096447 0.81499994 0.85053939 0.15330286 0.83766818]\n",
            " [0.80134393 0.80407747 0.84917902 0.15354486 0.83006392]\n",
            " [0.79944642 0.82170299 0.84824821 0.14488662 0.85738331]\n",
            " [0.83422336 0.84075774 0.88938117 0.16607511 0.86333302]\n",
            " [0.85288824 0.83711692 0.87445306 0.26645156 0.86269913]] -> [0.87850652]\n",
            "[[0.77746939 0.78580546 0.82181065 0.17593439 0.82474788]\n",
            " [0.80096447 0.81499994 0.85053939 0.15330286 0.83766818]\n",
            " [0.80134393 0.80407747 0.84917902 0.15354486 0.83006392]\n",
            " [0.79944642 0.82170299 0.84824821 0.14488662 0.85738331]\n",
            " [0.83422336 0.84075774 0.88938117 0.16607511 0.86333302]\n",
            " [0.85288824 0.83711692 0.87445306 0.26645156 0.86269913]\n",
            " [0.83439582 0.83881823 0.88048867 0.1638971  0.87850652]] -> [0.8737889]\n",
            "[[0.80096447 0.81499994 0.85053939 0.15330286 0.83766818]\n",
            " [0.80134393 0.80407747 0.84917902 0.15354486 0.83006392]\n",
            " [0.79944642 0.82170299 0.84824821 0.14488662 0.85738331]\n",
            " [0.83422336 0.84075774 0.88938117 0.16607511 0.86333302]\n",
            " [0.85288824 0.83711692 0.87445306 0.26645156 0.86269913]\n",
            " [0.83439582 0.83881823 0.88048867 0.1638971  0.87850652]\n",
            " [0.83770798 0.84732476 0.89460789 0.11309492 0.8737889 ]] -> [0.86431866]\n",
            "[[0.80134393 0.80407747 0.84917902 0.15354486 0.83006392]\n",
            " [0.79944642 0.82170299 0.84824821 0.14488662 0.85738331]\n",
            " [0.83422336 0.84075774 0.88938117 0.16607511 0.86333302]\n",
            " [0.85288824 0.83711692 0.87445306 0.26645156 0.86269913]\n",
            " [0.83439582 0.83881823 0.88048867 0.1638971  0.87850652]\n",
            " [0.83770798 0.84732476 0.89460789 0.11309492 0.8737889 ]\n",
            " [0.85461328 0.84977455 0.88991824 0.1276508  0.86431866]] -> [0.85460195]\n",
            "[[0.79944642 0.82170299 0.84824821 0.14488662 0.85738331]\n",
            " [0.83422336 0.84075774 0.88938117 0.16607511 0.86333302]\n",
            " [0.85288824 0.83711692 0.87445306 0.26645156 0.86269913]\n",
            " [0.83439582 0.83881823 0.88048867 0.1638971  0.87850652]\n",
            " [0.83770798 0.84732476 0.89460789 0.11309492 0.8737889 ]\n",
            " [0.85461328 0.84977455 0.88991824 0.1276508  0.86431866]\n",
            " [0.81890497 0.82264564 0.87148169 0.14224254 0.85460195]] -> [0.84837074]\n",
            "[[0.83422336 0.84075774 0.88938117 0.16607511 0.86333302]\n",
            " [0.85288824 0.83711692 0.87445306 0.26645156 0.86269913]\n",
            " [0.83439582 0.83881823 0.88048867 0.1638971  0.87850652]\n",
            " [0.83770798 0.84732476 0.89460789 0.11309492 0.8737889 ]\n",
            " [0.85461328 0.84977455 0.88991824 0.1276508  0.86431866]\n",
            " [0.81890497 0.82264564 0.87148169 0.14224254 0.85460195]\n",
            " [0.83539627 0.82687502 0.87685153 0.11592722 0.84837074]] -> [0.88794153]\n",
            "[[0.85288824 0.83711692 0.87445306 0.26645156 0.86269913]\n",
            " [0.83439582 0.83881823 0.88048867 0.1638971  0.87850652]\n",
            " [0.83770798 0.84732476 0.89460789 0.11309492 0.8737889 ]\n",
            " [0.85461328 0.84977455 0.88991824 0.1276508  0.86431866]\n",
            " [0.81890497 0.82264564 0.87148169 0.14224254 0.85460195]\n",
            " [0.83539627 0.82687502 0.87685153 0.11592722 0.84837074]\n",
            " [0.82780625 0.85498064 0.86346276 0.16992919 0.88794153]] -> [0.90821984]\n",
            "[[0.83439582 0.83881823 0.88048867 0.1638971  0.87850652]\n",
            " [0.83770798 0.84732476 0.89460789 0.11309492 0.8737889 ]\n",
            " [0.85461328 0.84977455 0.88991824 0.1276508  0.86431866]\n",
            " [0.81890497 0.82264564 0.87148169 0.14224254 0.85460195]\n",
            " [0.83539627 0.82687502 0.87685153 0.11592722 0.84837074]\n",
            " [0.82780625 0.85498064 0.86346276 0.16992919 0.88794153]\n",
            " [0.88131692 0.89115044 0.93498899 0.15904813 0.90821984]] -> [0.88857519]\n",
            "[[0.83770798 0.84732476 0.89460789 0.11309492 0.8737889 ]\n",
            " [0.85461328 0.84977455 0.88991824 0.1276508  0.86431866]\n",
            " [0.81890497 0.82264564 0.87148169 0.14224254 0.85460195]\n",
            " [0.83539627 0.82687502 0.87685153 0.11592722 0.84837074]\n",
            " [0.82780625 0.85498064 0.86346276 0.16992919 0.88794153]\n",
            " [0.88131692 0.89115044 0.93498899 0.15904813 0.90821984]\n",
            " [0.87838443 0.86722997 0.90706585 0.15334767 0.88857519]] -> [0.90603691]\n",
            "[[0.85461328 0.84977455 0.88991824 0.1276508  0.86431866]\n",
            " [0.81890497 0.82264564 0.87148169 0.14224254 0.85460195]\n",
            " [0.83539627 0.82687502 0.87685153 0.11592722 0.84837074]\n",
            " [0.82780625 0.85498064 0.86346276 0.16992919 0.88794153]\n",
            " [0.88131692 0.89115044 0.93498899 0.15904813 0.90821984]\n",
            " [0.87838443 0.86722997 0.90706585 0.15334767 0.88857519]\n",
            " [0.84164092 0.86549481 0.89296106 0.14061128 0.90603691]] -> [0.88977206]\n",
            "[[0.81890497 0.82264564 0.87148169 0.14224254 0.85460195]\n",
            " [0.83539627 0.82687502 0.87685153 0.11592722 0.84837074]\n",
            " [0.82780625 0.85498064 0.86346276 0.16992919 0.88794153]\n",
            " [0.88131692 0.89115044 0.93498899 0.15904813 0.90821984]\n",
            " [0.87838443 0.86722997 0.90706585 0.15334767 0.88857519]\n",
            " [0.84164092 0.86549481 0.89296106 0.14061128 0.90603691]\n",
            " [0.881179   0.8738651  0.91239986 0.10095008 0.88977206]] -> [0.86340328]\n",
            "[[0.83539627 0.82687502 0.87685153 0.11592722 0.84837074]\n",
            " [0.82780625 0.85498064 0.86346276 0.16992919 0.88794153]\n",
            " [0.88131692 0.89115044 0.93498899 0.15904813 0.90821984]\n",
            " [0.87838443 0.86722997 0.90706585 0.15334767 0.88857519]\n",
            " [0.84164092 0.86549481 0.89296106 0.14061128 0.90603691]\n",
            " [0.881179   0.8738651  0.91239986 0.10095008 0.88977206]\n",
            " [0.83957095 0.83983897 0.88712581 0.10077978 0.86340328]] -> [0.89118035]\n",
            "[[0.82780625 0.85498064 0.86346276 0.16992919 0.88794153]\n",
            " [0.88131692 0.89115044 0.93498899 0.15904813 0.90821984]\n",
            " [0.87838443 0.86722997 0.90706585 0.15334767 0.88857519]\n",
            " [0.84164092 0.86549481 0.89296106 0.14061128 0.90603691]\n",
            " [0.881179   0.8738651  0.91239986 0.10095008 0.88977206]\n",
            " [0.83957095 0.83983897 0.88712581 0.10077978 0.86340328]\n",
            " [0.83187733 0.85154397 0.887806   0.09373487 0.89118035]] -> [0.87213436]\n",
            "[[0.47419206 0.50445037 0.51770809 0.22296353 0.49142842]\n",
            " [0.46909658 0.46301195 0.47045433 0.2111109  0.45809697]\n",
            " [0.40525134 0.46000606 0.44658508 0.18462886 0.4877246 ]\n",
            " [0.47395239 0.47450627 0.47948097 0.10216366 0.46012771]\n",
            " [0.40165458 0.41556139 0.43671021 0.10180958 0.41538731]\n",
            " [0.3882862  0.43583844 0.43786128 0.08716152 0.4625172 ]\n",
            " [0.44583656 0.44031831 0.44222336 0.16161315 0.43020148]] -> [0.42345156]\n",
            "[[0.46909658 0.46301195 0.47045433 0.2111109  0.45809697]\n",
            " [0.40525134 0.46000606 0.44658508 0.18462886 0.4877246 ]\n",
            " [0.47395239 0.47450627 0.47948097 0.10216366 0.46012771]\n",
            " [0.40165458 0.41556139 0.43671021 0.10180958 0.41538731]\n",
            " [0.3882862  0.43583844 0.43786128 0.08716152 0.4625172 ]\n",
            " [0.44583656 0.44031831 0.44222336 0.16161315 0.43020148]\n",
            " [0.43744363 0.43118193 0.43780064 0.13134796 0.42345156]] -> [0.40523252]\n",
            "[[0.40525134 0.46000606 0.44658508 0.18462886 0.4877246 ]\n",
            " [0.47395239 0.47450627 0.47948097 0.10216366 0.46012771]\n",
            " [0.40165458 0.41556139 0.43671021 0.10180958 0.41538731]\n",
            " [0.3882862  0.43583844 0.43786128 0.08716152 0.4625172 ]\n",
            " [0.44583656 0.44031831 0.44222336 0.16161315 0.43020148]\n",
            " [0.43744363 0.43118193 0.43780064 0.13134796 0.42345156]\n",
            " [0.43174882 0.42852933 0.44082973 0.11817216 0.40523252]] -> [0.44698661]\n",
            "[[0.47395239 0.47450627 0.47948097 0.10216366 0.46012771]\n",
            " [0.40165458 0.41556139 0.43671021 0.10180958 0.41538731]\n",
            " [0.3882862  0.43583844 0.43786128 0.08716152 0.4625172 ]\n",
            " [0.44583656 0.44031831 0.44222336 0.16161315 0.43020148]\n",
            " [0.43744363 0.43118193 0.43780064 0.13134796 0.42345156]\n",
            " [0.43174882 0.42852933 0.44082973 0.11817216 0.40523252]\n",
            " [0.40165458 0.42163288 0.41029662 0.14293967 0.44698661]] -> [0.49853637]\n",
            "[[0.40165458 0.41556139 0.43671021 0.10180958 0.41538731]\n",
            " [0.3882862  0.43583844 0.43786128 0.08716152 0.4625172 ]\n",
            " [0.44583656 0.44031831 0.44222336 0.16161315 0.43020148]\n",
            " [0.43744363 0.43118193 0.43780064 0.13134796 0.42345156]\n",
            " [0.43174882 0.42852933 0.44082973 0.11817216 0.40523252]\n",
            " [0.40165458 0.42163288 0.41029662 0.14293967 0.44698661]\n",
            " [0.46855688 0.48381966 0.49057346 0.20893047 0.49853637]] -> [0.50737719]\n",
            "[[0.3882862  0.43583844 0.43786128 0.08716152 0.4625172 ]\n",
            " [0.44583656 0.44031831 0.44222336 0.16161315 0.43020148]\n",
            " [0.43744363 0.43118193 0.43780064 0.13134796 0.42345156]\n",
            " [0.43174882 0.42852933 0.44082973 0.11817216 0.40523252]\n",
            " [0.40165458 0.42163288 0.41029662 0.14293967 0.44698661]\n",
            " [0.46855688 0.48381966 0.49057346 0.20893047 0.49853637]\n",
            " [0.4976321  0.50109047 0.54172895 0.10236866 0.50737719]] -> [0.54202261]\n",
            "[[0.44583656 0.44031831 0.44222336 0.16161315 0.43020148]\n",
            " [0.43744363 0.43118193 0.43780064 0.13134796 0.42345156]\n",
            " [0.43174882 0.42852933 0.44082973 0.11817216 0.40523252]\n",
            " [0.40165458 0.42163288 0.41029662 0.14293967 0.44698661]\n",
            " [0.46855688 0.48381966 0.49057346 0.20893047 0.49853637]\n",
            " [0.4976321  0.50109047 0.54172895 0.10236866 0.50737719]\n",
            " [0.49745208 0.52284114 0.54166203 0.22797667 0.54202261]] -> [0.58747978]\n",
            "[[0.43744363 0.43118193 0.43780064 0.13134796 0.42345156]\n",
            " [0.43174882 0.42852933 0.44082973 0.11817216 0.40523252]\n",
            " [0.40165458 0.42163288 0.41029662 0.14293967 0.44698661]\n",
            " [0.46855688 0.48381966 0.49057346 0.20893047 0.49853637]\n",
            " [0.4976321  0.50109047 0.54172895 0.10236866 0.50737719]\n",
            " [0.49745208 0.52284114 0.54166203 0.22797667 0.54202261]\n",
            " [0.53629892 0.56439721 0.56956613 0.18077117 0.58747978]] -> [0.51173763]\n",
            "[[0.43174882 0.42852933 0.44082973 0.11817216 0.40523252]\n",
            " [0.40165458 0.42163288 0.41029662 0.14293967 0.44698661]\n",
            " [0.46855688 0.48381966 0.49057346 0.20893047 0.49853637]\n",
            " [0.4976321  0.50109047 0.54172895 0.10236866 0.50737719]\n",
            " [0.49745208 0.52284114 0.54166203 0.22797667 0.54202261]\n",
            " [0.53629892 0.56439721 0.56956613 0.18077117 0.58747978]\n",
            " [0.59055219 0.57530222 0.52128258 0.26919995 0.51173763]] -> [0.50421114]\n",
            "[[0.40165458 0.42163288 0.41029662 0.14293967 0.44698661]\n",
            " [0.46855688 0.48381966 0.49057346 0.20893047 0.49853637]\n",
            " [0.4976321  0.50109047 0.54172895 0.10236866 0.50737719]\n",
            " [0.49745208 0.52284114 0.54166203 0.22797667 0.54202261]\n",
            " [0.53629892 0.56439721 0.56956613 0.18077117 0.58747978]\n",
            " [0.59055219 0.57530222 0.52128258 0.26919995 0.51173763]\n",
            " [0.52155147 0.50593586 0.52540211 0.17577666 0.50421114]] -> [0.54285897]\n",
            "[[0.46855688 0.48381966 0.49057346 0.20893047 0.49853637]\n",
            " [0.4976321  0.50109047 0.54172895 0.10236866 0.50737719]\n",
            " [0.49745208 0.52284114 0.54166203 0.22797667 0.54202261]\n",
            " [0.53629892 0.56439721 0.56956613 0.18077117 0.58747978]\n",
            " [0.59055219 0.57530222 0.52128258 0.26919995 0.51173763]\n",
            " [0.52155147 0.50593586 0.52540211 0.17577666 0.50421114]\n",
            " [0.50584501 0.51959924 0.52261521 0.46115284 0.54285897]] -> [0.30171446]\n",
            "[[0.4976321  0.50109047 0.54172895 0.10236866 0.50737719]\n",
            " [0.49745208 0.52284114 0.54166203 0.22797667 0.54202261]\n",
            " [0.53629892 0.56439721 0.56956613 0.18077117 0.58747978]\n",
            " [0.59055219 0.57530222 0.52128258 0.26919995 0.51173763]\n",
            " [0.52155147 0.50593586 0.52540211 0.17577666 0.50421114]\n",
            " [0.50584501 0.51959924 0.52261521 0.46115284 0.54285897]\n",
            " [0.33151483 0.37618625 0.30488407 1.         0.30171446]] -> [0.3278778]\n",
            "[[0.49745208 0.52284114 0.54166203 0.22797667 0.54202261]\n",
            " [0.53629892 0.56439721 0.56956613 0.18077117 0.58747978]\n",
            " [0.59055219 0.57530222 0.52128258 0.26919995 0.51173763]\n",
            " [0.52155147 0.50593586 0.52540211 0.17577666 0.50421114]\n",
            " [0.50584501 0.51959924 0.52261521 0.46115284 0.54285897]\n",
            " [0.33151483 0.37618625 0.30488407 1.         0.30171446]\n",
            " [0.27036734 0.30433244 0.31687953 0.25591234 0.3278778 ]] -> [0.23821762]\n",
            "[[0.53629892 0.56439721 0.56956613 0.18077117 0.58747978]\n",
            " [0.59055219 0.57530222 0.52128258 0.26919995 0.51173763]\n",
            " [0.52155147 0.50593586 0.52540211 0.17577666 0.50421114]\n",
            " [0.50584501 0.51959924 0.52261521 0.46115284 0.54285897]\n",
            " [0.33151483 0.37618625 0.30488407 1.         0.30171446]\n",
            " [0.27036734 0.30433244 0.31687953 0.25591234 0.3278778 ]\n",
            " [0.32623934 0.3151547  0.24076436 0.40228107 0.23821762]] -> [0.22447896]\n",
            "[[0.59055219 0.57530222 0.52128258 0.26919995 0.51173763]\n",
            " [0.52155147 0.50593586 0.52540211 0.17577666 0.50421114]\n",
            " [0.50584501 0.51959924 0.52261521 0.46115284 0.54285897]\n",
            " [0.33151483 0.37618625 0.30488407 1.         0.30171446]\n",
            " [0.27036734 0.30433244 0.31687953 0.25591234 0.3278778 ]\n",
            " [0.32623934 0.3151547  0.24076436 0.40228107 0.23821762]\n",
            " [0.21755278 0.21620977 0.17617799 0.46825323 0.22447896]] -> [0.13595373]\n",
            "[[0.52155147 0.50593586 0.52540211 0.17577666 0.50421114]\n",
            " [0.50584501 0.51959924 0.52261521 0.46115284 0.54285897]\n",
            " [0.33151483 0.37618625 0.30488407 1.         0.30171446]\n",
            " [0.27036734 0.30433244 0.31687953 0.25591234 0.3278778 ]\n",
            " [0.32623934 0.3151547  0.24076436 0.40228107 0.23821762]\n",
            " [0.21755278 0.21620977 0.17617799 0.46825323 0.22447896]\n",
            " [0.22336797 0.24680218 0.15912419 0.42514769 0.13595373]] -> [0.14784065]\n",
            "[[0.50584501 0.51959924 0.52261521 0.46115284 0.54285897]\n",
            " [0.33151483 0.37618625 0.30488407 1.         0.30171446]\n",
            " [0.27036734 0.30433244 0.31687953 0.25591234 0.3278778 ]\n",
            " [0.32623934 0.3151547  0.24076436 0.40228107 0.23821762]\n",
            " [0.21755278 0.21620977 0.17617799 0.46825323 0.22447896]\n",
            " [0.22336797 0.24680218 0.15912419 0.42514769 0.13595373]\n",
            " [0.11809851 0.14924847 0.15579227 0.35440467 0.14784065]] -> [0.1789022]\n",
            "[[0.33151483 0.37618625 0.30488407 1.         0.30171446]\n",
            " [0.27036734 0.30433244 0.31687953 0.25591234 0.3278778 ]\n",
            " [0.32623934 0.3151547  0.24076436 0.40228107 0.23821762]\n",
            " [0.21755278 0.21620977 0.17617799 0.46825323 0.22447896]\n",
            " [0.22336797 0.24680218 0.15912419 0.42514769 0.13595373]\n",
            " [0.11809851 0.14924847 0.15579227 0.35440467 0.14784065]\n",
            " [0.15964274 0.16704994 0.16790863 0.1974133  0.1789022 ]] -> [0.14395782]\n",
            "[[0.27036734 0.30433244 0.31687953 0.25591234 0.3278778 ]\n",
            " [0.32623934 0.3151547  0.24076436 0.40228107 0.23821762]\n",
            " [0.21755278 0.21620977 0.17617799 0.46825323 0.22447896]\n",
            " [0.22336797 0.24680218 0.15912419 0.42514769 0.13595373]\n",
            " [0.11809851 0.14924847 0.15579227 0.35440467 0.14784065]\n",
            " [0.15964274 0.16704994 0.16790863 0.1974133  0.1789022 ]\n",
            " [0.1550866  0.15054545 0.17396682 0.17849755 0.14395782]] -> [0.163909]\n",
            "[[0.32623934 0.3151547  0.24076436 0.40228107 0.23821762]\n",
            " [0.21755278 0.21620977 0.17617799 0.46825323 0.22447896]\n",
            " [0.22336797 0.24680218 0.15912419 0.42514769 0.13595373]\n",
            " [0.11809851 0.14924847 0.15579227 0.35440467 0.14784065]\n",
            " [0.15964274 0.16704994 0.16790863 0.1974133  0.1789022 ]\n",
            " [0.1550866  0.15054545 0.17396682 0.17849755 0.14395782]\n",
            " [0.11683946 0.16180376 0.15585291 0.20639594 0.163909  ]] -> [0.19813624]\n",
            "[[0.21755278 0.21620977 0.17617799 0.46825323 0.22447896]\n",
            " [0.22336797 0.24680218 0.15912419 0.42514769 0.13595373]\n",
            " [0.11809851 0.14924847 0.15579227 0.35440467 0.14784065]\n",
            " [0.15964274 0.16704994 0.16790863 0.1974133  0.1789022 ]\n",
            " [0.1550866  0.15054545 0.17396682 0.17849755 0.14395782]\n",
            " [0.11683946 0.16180376 0.15585291 0.20639594 0.163909  ]\n",
            " [0.16006242 0.17695264 0.19650308 0.20453232 0.19813624]] -> [0.25601811]\n",
            "[[0.22336797 0.24680218 0.15912419 0.42514769 0.13595373]\n",
            " [0.11809851 0.14924847 0.15579227 0.35440467 0.14784065]\n",
            " [0.15964274 0.16704994 0.16790863 0.1974133  0.1789022 ]\n",
            " [0.1550866  0.15054545 0.17396682 0.17849755 0.14395782]\n",
            " [0.11683946 0.16180376 0.15585291 0.20639594 0.163909  ]\n",
            " [0.16006242 0.17695264 0.19650308 0.20453232 0.19813624]\n",
            " [0.16413887 0.23318592 0.21096411 0.23110755 0.25601811]] -> [0.26665086]\n",
            "[[0.11809851 0.14924847 0.15579227 0.35440467 0.14784065]\n",
            " [0.15964274 0.16704994 0.16790863 0.1974133  0.1789022 ]\n",
            " [0.1550866  0.15054545 0.17396682 0.17849755 0.14395782]\n",
            " [0.11683946 0.16180376 0.15585291 0.20639594 0.163909  ]\n",
            " [0.16006242 0.17695264 0.19650308 0.20453232 0.19813624]\n",
            " [0.16413887 0.23318592 0.21096411 0.23110755 0.25601811]\n",
            " [0.24578862 0.27356338 0.28301412 0.17225442 0.26665086]] -> [0.32805682]\n",
            "[[0.15964274 0.16704994 0.16790863 0.1974133  0.1789022 ]\n",
            " [0.1550866  0.15054545 0.17396682 0.17849755 0.14395782]\n",
            " [0.11683946 0.16180376 0.15585291 0.20639594 0.163909  ]\n",
            " [0.16006242 0.17695264 0.19650308 0.20453232 0.19813624]\n",
            " [0.16413887 0.23318592 0.21096411 0.23110755 0.25601811]\n",
            " [0.24578862 0.27356338 0.28301412 0.17225442 0.26665086]\n",
            " [0.27426414 0.30179785 0.31766676 0.18330569 0.32805682]] -> [0.28092692]\n",
            "[[0.1550866  0.15054545 0.17396682 0.17849755 0.14395782]\n",
            " [0.11683946 0.16180376 0.15585291 0.20639594 0.163909  ]\n",
            " [0.16006242 0.17695264 0.19650308 0.20453232 0.19813624]\n",
            " [0.16413887 0.23318592 0.21096411 0.23110755 0.25601811]\n",
            " [0.24578862 0.27356338 0.28301412 0.17225442 0.26665086]\n",
            " [0.27426414 0.30179785 0.31766676 0.18330569 0.32805682]\n",
            " [0.31418964 0.30757433 0.29997696 0.20613504 0.28092692]] -> [0.26909979]\n",
            "[[0.11683946 0.16180376 0.15585291 0.20639594 0.163909  ]\n",
            " [0.16006242 0.17695264 0.19650308 0.20453232 0.19813624]\n",
            " [0.16413887 0.23318592 0.21096411 0.23110755 0.25601811]\n",
            " [0.24578862 0.27356338 0.28301412 0.17225442 0.26665086]\n",
            " [0.27426414 0.30179785 0.31766676 0.18330569 0.32805682]\n",
            " [0.31418964 0.30757433 0.29997696 0.20613504 0.28092692]\n",
            " [0.27612253 0.27674627 0.27695594 0.14437466 0.26909979]] -> [0.25428597]\n",
            "[[0.16006242 0.17695264 0.19650308 0.20453232 0.19813624]\n",
            " [0.16413887 0.23318592 0.21096411 0.23110755 0.25601811]\n",
            " [0.24578862 0.27356338 0.28301412 0.17225442 0.26665086]\n",
            " [0.27426414 0.30179785 0.31766676 0.18330569 0.32805682]\n",
            " [0.31418964 0.30757433 0.29997696 0.20613504 0.28092692]\n",
            " [0.27612253 0.27674627 0.27695594 0.14437466 0.26909979]\n",
            " [0.24536894 0.26149126 0.27853113 0.13576474 0.25428597]] -> [0.28809502]\n",
            "[[0.16413887 0.23318592 0.21096411 0.23110755 0.25601811]\n",
            " [0.24578862 0.27356338 0.28301412 0.17225442 0.26665086]\n",
            " [0.27426414 0.30179785 0.31766676 0.18330569 0.32805682]\n",
            " [0.31418964 0.30757433 0.29997696 0.20613504 0.28092692]\n",
            " [0.27612253 0.27674627 0.27695594 0.14437466 0.26909979]\n",
            " [0.24536894 0.26149126 0.27853113 0.13576474 0.25428597]\n",
            " [0.22858345 0.2722074  0.25666117 0.13624928 0.28809502]] -> [0.22680829]\n",
            "[[0.24578862 0.27356338 0.28301412 0.17225442 0.26665086]\n",
            " [0.27426414 0.30179785 0.31766676 0.18330569 0.32805682]\n",
            " [0.31418964 0.30757433 0.29997696 0.20613504 0.28092692]\n",
            " [0.27612253 0.27674627 0.27695594 0.14437466 0.26909979]\n",
            " [0.24536894 0.26149126 0.27853113 0.13576474 0.25428597]\n",
            " [0.22858345 0.2722074  0.25666117 0.13624928 0.28809502]\n",
            " [0.26970799 0.29012688 0.24733133 0.26373954 0.22680829]] -> [0.22919778]\n",
            "[[0.27426414 0.30179785 0.31766676 0.18330569 0.32805682]\n",
            " [0.31418964 0.30757433 0.29997696 0.20613504 0.28092692]\n",
            " [0.27612253 0.27674627 0.27695594 0.14437466 0.26909979]\n",
            " [0.24536894 0.26149126 0.27853113 0.13576474 0.25428597]\n",
            " [0.22858345 0.2722074  0.25666117 0.13624928 0.28809502]\n",
            " [0.26970799 0.29012688 0.24733133 0.26373954 0.22680829]\n",
            " [0.19585147 0.2316533  0.22624897 0.22005628 0.22919778]] -> [0.1915059]\n",
            "[[0.31418964 0.30757433 0.29997696 0.20613504 0.28092692]\n",
            " [0.27612253 0.27674627 0.27695594 0.14437466 0.26909979]\n",
            " [0.24536894 0.26149126 0.27853113 0.13576474 0.25428597]\n",
            " [0.22858345 0.2722074  0.25666117 0.13624928 0.28809502]\n",
            " [0.26970799 0.29012688 0.24733133 0.26373954 0.22680829]\n",
            " [0.19585147 0.2316533  0.22624897 0.22005628 0.22919778]\n",
            " [0.18799823 0.19864431 0.20304603 0.20205371 0.1915059 ]] -> [0.24777484]\n",
            "[[0.27612253 0.27674627 0.27695594 0.14437466 0.26909979]\n",
            " [0.24536894 0.26149126 0.27853113 0.13576474 0.25428597]\n",
            " [0.22858345 0.2722074  0.25666117 0.13624928 0.28809502]\n",
            " [0.26970799 0.29012688 0.24733133 0.26373954 0.22680829]\n",
            " [0.19585147 0.2316533  0.22624897 0.22005628 0.22919778]\n",
            " [0.18799823 0.19864431 0.20304603 0.20205371 0.1915059 ]\n",
            " [0.18356211 0.24921912 0.22558266 0.23153618 0.24777484]] -> [0.21492137]\n",
            "[[0.24536894 0.26149126 0.27853113 0.13576474 0.25428597]\n",
            " [0.22858345 0.2722074  0.25666117 0.13624928 0.28809502]\n",
            " [0.26970799 0.29012688 0.24733133 0.26373954 0.22680829]\n",
            " [0.19585147 0.2316533  0.22624897 0.22005628 0.22919778]\n",
            " [0.18799823 0.19864431 0.20304603 0.20205371 0.1915059 ]\n",
            " [0.18356211 0.24921912 0.22558266 0.23153618 0.24777484]\n",
            " [0.212997   0.23093447 0.24775545 0.1387838  0.21492137]] -> [0.30959934]\n",
            "[[0.22858345 0.2722074  0.25666117 0.13624928 0.28809502]\n",
            " [0.26970799 0.29012688 0.24733133 0.26373954 0.22680829]\n",
            " [0.19585147 0.2316533  0.22624897 0.22005628 0.22919778]\n",
            " [0.18799823 0.19864431 0.20304603 0.20205371 0.1915059 ]\n",
            " [0.18356211 0.24921912 0.22558266 0.23153618 0.24777484]\n",
            " [0.212997   0.23093447 0.24775545 0.1387838  0.21492137]\n",
            " [0.21497503 0.28688462 0.26399133 0.25037738 0.30959934]] -> [0.3405413]\n",
            "[[0.26970799 0.29012688 0.24733133 0.26373954 0.22680829]\n",
            " [0.19585147 0.2316533  0.22624897 0.22005628 0.22919778]\n",
            " [0.18799823 0.19864431 0.20304603 0.20205371 0.1915059 ]\n",
            " [0.18356211 0.24921912 0.22558266 0.23153618 0.24777484]\n",
            " [0.212997   0.23093447 0.24775545 0.1387838  0.21492137]\n",
            " [0.21497503 0.28688462 0.26399133 0.25037738 0.30959934]\n",
            " [0.29830352 0.3254348  0.3418089  0.19441287 0.3405413 ]] -> [0.33367179]\n",
            "[[0.19585147 0.2316533  0.22624897 0.22005628 0.22919778]\n",
            " [0.18799823 0.19864431 0.20304603 0.20205371 0.1915059 ]\n",
            " [0.18356211 0.24921912 0.22558266 0.23153618 0.24777484]\n",
            " [0.212997   0.23093447 0.24775545 0.1387838  0.21492137]\n",
            " [0.21497503 0.28688462 0.26399133 0.25037738 0.30959934]\n",
            " [0.29830352 0.3254348  0.3418089  0.19441287 0.3405413 ]\n",
            " [0.31095255 0.33026832 0.34529242 0.18455432 0.33367179]] -> [0.38468415]\n",
            "[[0.18799823 0.19864431 0.20304603 0.20205371 0.1915059 ]\n",
            " [0.18356211 0.24921912 0.22558266 0.23153618 0.24777484]\n",
            " [0.212997   0.23093447 0.24775545 0.1387838  0.21492137]\n",
            " [0.21497503 0.28688462 0.26399133 0.25037738 0.30959934]\n",
            " [0.29830352 0.3254348  0.3418089  0.19441287 0.3405413 ]\n",
            " [0.31095255 0.33026832 0.34529242 0.18455432 0.33367179]\n",
            " [0.31778677 0.36331257 0.36782869 0.25867049 0.38468415]] -> [0.40296262]\n",
            "[[0.18356211 0.24921912 0.22558266 0.23153618 0.24777484]\n",
            " [0.212997   0.23093447 0.24775545 0.1387838  0.21492137]\n",
            " [0.21497503 0.28688462 0.26399133 0.25037738 0.30959934]\n",
            " [0.29830352 0.3254348  0.3418089  0.19441287 0.3405413 ]\n",
            " [0.31095255 0.33026832 0.34529242 0.18455432 0.33367179]\n",
            " [0.31778677 0.36331257 0.36782869 0.25867049 0.38468415]\n",
            " [0.3641268  0.39746527 0.41181116 0.28764979 0.40296262]] -> [0.39358476]\n",
            "[[0.212997   0.23093447 0.24775545 0.1387838  0.21492137]\n",
            " [0.21497503 0.28688462 0.26399133 0.25037738 0.30959934]\n",
            " [0.29830352 0.3254348  0.3418089  0.19441287 0.3405413 ]\n",
            " [0.31095255 0.33026832 0.34529242 0.18455432 0.33367179]\n",
            " [0.31778677 0.36331257 0.36782869 0.25867049 0.38468415]\n",
            " [0.3641268  0.39746527 0.41181116 0.28764979 0.40296262]\n",
            " [0.38085265 0.3826114  0.40817603 0.12441529 0.39358476]] -> [0.37118466]\n",
            "[[0.21497503 0.28688462 0.26399133 0.25037738 0.30959934]\n",
            " [0.29830352 0.3254348  0.3418089  0.19441287 0.3405413 ]\n",
            " [0.31095255 0.33026832 0.34529242 0.18455432 0.33367179]\n",
            " [0.31778677 0.36331257 0.36782869 0.25867049 0.38468415]\n",
            " [0.3641268  0.39746527 0.41181116 0.28764979 0.40296262]\n",
            " [0.38085265 0.3826114  0.40817603 0.12441529 0.39358476]\n",
            " [0.36868294 0.35791349 0.36885848 0.14085242 0.37118466]] -> [0.3230394]\n",
            "[[0.29830352 0.3254348  0.3418089  0.19441287 0.3405413 ]\n",
            " [0.31095255 0.33026832 0.34529242 0.18455432 0.33367179]\n",
            " [0.31778677 0.36331257 0.36782869 0.25867049 0.38468415]\n",
            " [0.3641268  0.39746527 0.41181116 0.28764979 0.40296262]\n",
            " [0.38085265 0.3826114  0.40817603 0.12441529 0.39358476]\n",
            " [0.36868294 0.35791349 0.36885848 0.14085242 0.37118466]\n",
            " [0.34931971 0.33710577 0.34698853 0.11932761 0.3230394 ]] -> [0.28845341]\n",
            "[[0.31095255 0.33026832 0.34529242 0.18455432 0.33367179]\n",
            " [0.31778677 0.36331257 0.36782869 0.25867049 0.38468415]\n",
            " [0.3641268  0.39746527 0.41181116 0.28764979 0.40296262]\n",
            " [0.38085265 0.3826114  0.40817603 0.12441529 0.39358476]\n",
            " [0.36868294 0.35791349 0.36885848 0.14085242 0.37118466]\n",
            " [0.34931971 0.33710577 0.34698853 0.11932761 0.3230394 ]\n",
            " [0.32318191 0.31010892 0.31094226 0.18250433 0.28845341]] -> [0.28905096]\n",
            "[[0.31778677 0.36331257 0.36782869 0.25867049 0.38468415]\n",
            " [0.3641268  0.39746527 0.41181116 0.28764979 0.40296262]\n",
            " [0.38085265 0.3826114  0.40817603 0.12441529 0.39358476]\n",
            " [0.36868294 0.35791349 0.36885848 0.14085242 0.37118466]\n",
            " [0.34931971 0.33710577 0.34698853 0.11932761 0.3230394 ]\n",
            " [0.32318191 0.31010892 0.31094226 0.18250433 0.28845341]\n",
            " [0.29278837 0.29283811 0.32269515 0.13980879 0.28905096]] -> [0.35852117]\n",
            "[[0.3641268  0.39746527 0.41181116 0.28764979 0.40296262]\n",
            " [0.38085265 0.3826114  0.40817603 0.12441529 0.39358476]\n",
            " [0.36868294 0.35791349 0.36885848 0.14085242 0.37118466]\n",
            " [0.34931971 0.33710577 0.34698853 0.11932761 0.3230394 ]\n",
            " [0.32318191 0.31010892 0.31094226 0.18250433 0.28845341]\n",
            " [0.29278837 0.29283811 0.32269515 0.13980879 0.28905096]\n",
            " [0.3174871  0.33168294 0.34710981 0.18593339 0.35852117]] -> [0.3603131]\n",
            "[[0.38085265 0.3826114  0.40817603 0.12441529 0.39358476]\n",
            " [0.36868294 0.35791349 0.36885848 0.14085242 0.37118466]\n",
            " [0.34931971 0.33710577 0.34698853 0.11932761 0.3230394 ]\n",
            " [0.32318191 0.31010892 0.31094226 0.18250433 0.28845341]\n",
            " [0.29278837 0.29283811 0.32269515 0.13980879 0.28905096]\n",
            " [0.3174871  0.33168294 0.34710981 0.18593339 0.35852117]\n",
            " [0.31095255 0.33740042 0.35774787 0.07508526 0.3603131 ]] -> [0.30553713]\n",
            "[[0.36868294 0.35791349 0.36885848 0.14085242 0.37118466]\n",
            " [0.34931971 0.33710577 0.34698853 0.11932761 0.3230394 ]\n",
            " [0.32318191 0.31010892 0.31094226 0.18250433 0.28845341]\n",
            " [0.29278837 0.29283811 0.32269515 0.13980879 0.28905096]\n",
            " [0.3174871  0.33168294 0.34710981 0.18593339 0.35852117]\n",
            " [0.31095255 0.33740042 0.35774787 0.07508526 0.3603131 ]\n",
            " [0.29056994 0.31588576 0.3219682  0.11748262 0.30553713]] -> [0.29926518]\n",
            "[[0.34931971 0.33710577 0.34698853 0.11932761 0.3230394 ]\n",
            " [0.32318191 0.31010892 0.31094226 0.18250433 0.28845341]\n",
            " [0.29278837 0.29283811 0.32269515 0.13980879 0.28905096]\n",
            " [0.3174871  0.33168294 0.34710981 0.18593339 0.35852117]\n",
            " [0.31095255 0.33740042 0.35774787 0.07508526 0.3603131 ]\n",
            " [0.29056994 0.31588576 0.3219682  0.11748262 0.30553713]\n",
            " [0.27282543 0.31323317 0.32245296 0.12540301 0.29926518]] -> [0.29872778]\n",
            "[[0.32318191 0.31010892 0.31094226 0.18250433 0.28845341]\n",
            " [0.29278837 0.29283811 0.32269515 0.13980879 0.28905096]\n",
            " [0.3174871  0.33168294 0.34710981 0.18593339 0.35852117]\n",
            " [0.31095255 0.33740042 0.35774787 0.07508526 0.3603131 ]\n",
            " [0.29056994 0.31588576 0.3219682  0.11748262 0.30553713]\n",
            " [0.27282543 0.31323317 0.32245296 0.12540301 0.29926518]\n",
            " [0.27264541 0.29572635 0.30191562 0.1341993  0.29872778]] -> [0.30261025]\n",
            "[[0.29278837 0.29283811 0.32269515 0.13980879 0.28905096]\n",
            " [0.3174871  0.33168294 0.34710981 0.18593339 0.35852117]\n",
            " [0.31095255 0.33740042 0.35774787 0.07508526 0.3603131 ]\n",
            " [0.29056994 0.31588576 0.3219682  0.11748262 0.30553713]\n",
            " [0.27282543 0.31323317 0.32245296 0.12540301 0.29926518]\n",
            " [0.27264541 0.29572635 0.30191562 0.1341993  0.29872778]\n",
            " [0.28775254 0.2987326  0.32729943 0.11712853 0.30261025]] -> [0.2514783]\n",
            "[[0.3174871  0.33168294 0.34710981 0.18593339 0.35852117]\n",
            " [0.31095255 0.33740042 0.35774787 0.07508526 0.3603131 ]\n",
            " [0.29056994 0.31588576 0.3219682  0.11748262 0.30553713]\n",
            " [0.27282543 0.31323317 0.32245296 0.12540301 0.29926518]\n",
            " [0.27264541 0.29572635 0.30191562 0.1341993  0.29872778]\n",
            " [0.28775254 0.2987326  0.32729943 0.11712853 0.30261025]\n",
            " [0.26323346 0.26142075 0.24218202 0.26025457 0.2514783 ]] -> [0.14013479]\n",
            "[[0.31095255 0.33740042 0.35774787 0.07508526 0.3603131 ]\n",
            " [0.29056994 0.31588576 0.3219682  0.11748262 0.30553713]\n",
            " [0.27282543 0.31323317 0.32245296 0.12540301 0.29926518]\n",
            " [0.27264541 0.29572635 0.30191562 0.1341993  0.29872778]\n",
            " [0.28775254 0.2987326  0.32729943 0.11712853 0.30261025]\n",
            " [0.26323346 0.26142075 0.24218202 0.26025457 0.2514783 ]\n",
            " [0.22570604 0.21526681 0.15247254 0.52487001 0.14013479]] -> [0.15202208]\n",
            "[[0.29056994 0.31588576 0.3219682  0.11748262 0.30553713]\n",
            " [0.27282543 0.31323317 0.32245296 0.12540301 0.29926518]\n",
            " [0.27264541 0.29572635 0.30191562 0.1341993  0.29872778]\n",
            " [0.28775254 0.2987326  0.32729943 0.11712853 0.30261025]\n",
            " [0.26323346 0.26142075 0.24218202 0.26025457 0.2514783 ]\n",
            " [0.22570604 0.21526681 0.15247254 0.52487001 0.14013479]\n",
            " [0.16647695 0.1778956  0.18250869 0.27889077 0.15202208]] -> [0.16534255]\n",
            "[[0.27282543 0.31323317 0.32245296 0.12540301 0.29926518]\n",
            " [0.27264541 0.29572635 0.30191562 0.1341993  0.29872778]\n",
            " [0.28775254 0.2987326  0.32729943 0.11712853 0.30261025]\n",
            " [0.26323346 0.26142075 0.24218202 0.26025457 0.2514783 ]\n",
            " [0.22570604 0.21526681 0.15247254 0.52487001 0.14013479]\n",
            " [0.16647695 0.1778956  0.18250869 0.27889077 0.15202208]\n",
            " [0.16425888 0.17960523 0.17402746 0.16392404 0.16534255]] -> [0.17442218]\n",
            "[[0.27264541 0.29572635 0.30191562 0.1341993  0.29872778]\n",
            " [0.28775254 0.2987326  0.32729943 0.11712853 0.30261025]\n",
            " [0.26323346 0.26142075 0.24218202 0.26025457 0.2514783 ]\n",
            " [0.22570604 0.21526681 0.15247254 0.52487001 0.14013479]\n",
            " [0.16647695 0.1778956  0.18250869 0.27889077 0.15202208]\n",
            " [0.16425888 0.17960523 0.17402746 0.16392404 0.16534255]\n",
            " [0.16821532 0.16834655 0.18052159 0.1115004  0.17442218]] -> [0.20076453]\n",
            "[[0.28775254 0.2987326  0.32729943 0.11712853 0.30261025]\n",
            " [0.26323346 0.26142075 0.24218202 0.26025457 0.2514783 ]\n",
            " [0.22570604 0.21526681 0.15247254 0.52487001 0.14013479]\n",
            " [0.16647695 0.1778956  0.18250869 0.27889077 0.15202208]\n",
            " [0.16425888 0.17960523 0.17402746 0.16392404 0.16534255]\n",
            " [0.16821532 0.16834655 0.18052159 0.1115004  0.17442218]\n",
            " [0.15856371 0.17477171 0.1436759  0.29545836 0.20076453]] -> [0.04157435]\n",
            "[[0.26323346 0.26142075 0.24218202 0.26025457 0.2514783 ]\n",
            " [0.22570604 0.21526681 0.15247254 0.52487001 0.14013479]\n",
            " [0.16647695 0.1778956  0.18250869 0.27889077 0.15202208]\n",
            " [0.16425888 0.17960523 0.17402746 0.16392404 0.16534255]\n",
            " [0.16821532 0.16834655 0.18052159 0.1115004  0.17442218]\n",
            " [0.15856371 0.17477171 0.1436759  0.29545836 0.20076453]\n",
            " [0.0249984  0.10079595 0.06158758 0.71991651 0.04157435]] -> [0.]\n",
            "[[0.22570604 0.21526681 0.15247254 0.52487001 0.14013479]\n",
            " [0.16647695 0.1778956  0.18250869 0.27889077 0.15202208]\n",
            " [0.16425888 0.17960523 0.17402746 0.16392404 0.16534255]\n",
            " [0.16821532 0.16834655 0.18052159 0.1115004  0.17442218]\n",
            " [0.15856371 0.17477171 0.1436759  0.29545836 0.20076453]\n",
            " [0.0249984  0.10079595 0.06158758 0.71991651 0.04157435]\n",
            " [0.         0.         0.         0.38073762 0.        ]] -> [0.07036599]\n",
            "[[0.16647695 0.1778956  0.18250869 0.27889077 0.15202208]\n",
            " [0.16425888 0.17960523 0.17402746 0.16392404 0.16534255]\n",
            " [0.16821532 0.16834655 0.18052159 0.1115004  0.17442218]\n",
            " [0.15856371 0.17477171 0.1436759  0.29545836 0.20076453]\n",
            " [0.0249984  0.10079595 0.06158758 0.71991651 0.04157435]\n",
            " [0.         0.         0.         0.38073762 0.        ]\n",
            " [0.04777874 0.04733291 0.05886133 0.29590563 0.07036599]] -> [0.0946776]\n",
            "[[0.16425888 0.17960523 0.17402746 0.16392404 0.16534255]\n",
            " [0.16821532 0.16834655 0.18052159 0.1115004  0.17442218]\n",
            " [0.15856371 0.17477171 0.1436759  0.29545836 0.20076453]\n",
            " [0.0249984  0.10079595 0.06158758 0.71991651 0.04157435]\n",
            " [0.         0.         0.         0.38073762 0.        ]\n",
            " [0.04777874 0.04733291 0.05886133 0.29590563 0.07036599]\n",
            " [0.07193813 0.08917787 0.10981049 0.25095511 0.0946776 ]] -> [0.14240469]\n",
            "[[0.16821532 0.16834655 0.18052159 0.1115004  0.17442218]\n",
            " [0.15856371 0.17477171 0.1436759  0.29545836 0.20076453]\n",
            " [0.0249984  0.10079595 0.06158758 0.71991651 0.04157435]\n",
            " [0.         0.         0.         0.38073762 0.        ]\n",
            " [0.04777874 0.04733291 0.05886133 0.29590563 0.07036599]\n",
            " [0.07193813 0.08917787 0.10981049 0.25095511 0.0946776 ]\n",
            " [0.08674523 0.11800776 0.12338113 0.18854246 0.14240469]] -> [0.18487556]\n",
            "[[0.15856371 0.17477171 0.1436759  0.29545836 0.20076453]\n",
            " [0.0249984  0.10079595 0.06158758 0.71991651 0.04157435]\n",
            " [0.         0.         0.         0.38073762 0.        ]\n",
            " [0.04777874 0.04733291 0.05886133 0.29590563 0.07036599]\n",
            " [0.07193813 0.08917787 0.10981049 0.25095511 0.0946776 ]\n",
            " [0.08674523 0.11800776 0.12338113 0.18854246 0.14240469]\n",
            " [0.12709078 0.16710894 0.17475441 0.14139287 0.18487556]] -> [0.15942898]\n",
            "[[0.0249984  0.10079595 0.06158758 0.71991651 0.04157435]\n",
            " [0.         0.         0.         0.38073762 0.        ]\n",
            " [0.04777874 0.04733291 0.05886133 0.29590563 0.07036599]\n",
            " [0.07193813 0.08917787 0.10981049 0.25095511 0.0946776 ]\n",
            " [0.08674523 0.11800776 0.12338113 0.18854246 0.14240469]\n",
            " [0.12709078 0.16710894 0.17475441 0.14139287 0.18487556]\n",
            " [0.15023079 0.14524026 0.15506531 0.16336495 0.15942898]] -> [0.17627391]\n",
            "[[0.         0.         0.         0.38073762 0.        ]\n",
            " [0.04777874 0.04733291 0.05886133 0.29590563 0.07036599]\n",
            " [0.07193813 0.08917787 0.10981049 0.25095511 0.0946776 ]\n",
            " [0.08674523 0.11800776 0.12338113 0.18854246 0.14240469]\n",
            " [0.12709078 0.16710894 0.17475441 0.14139287 0.18487556]\n",
            " [0.15023079 0.14524026 0.15506531 0.16336495 0.15942898]\n",
            " [0.11378203 0.17318008 0.15633767 0.1539164  0.17627391]] -> [0.1618779]\n",
            "[[0.04777874 0.04733291 0.05886133 0.29590563 0.07036599]\n",
            " [0.07193813 0.08917787 0.10981049 0.25095511 0.0946776 ]\n",
            " [0.08674523 0.11800776 0.12338113 0.18854246 0.14240469]\n",
            " [0.12709078 0.16710894 0.17475441 0.14139287 0.18487556]\n",
            " [0.15023079 0.14524026 0.15506531 0.16336495 0.15942898]\n",
            " [0.11378203 0.17318008 0.15633767 0.1539164  0.17627391]\n",
            " [0.16234049 0.15266738 0.15103676 0.13384521 0.1618779 ]] -> [0.22322443]\n",
            "[[0.07193813 0.08917787 0.10981049 0.25095511 0.0946776 ]\n",
            " [0.08674523 0.11800776 0.12338113 0.18854246 0.14240469]\n",
            " [0.12709078 0.16710894 0.17475441 0.14139287 0.18487556]\n",
            " [0.15023079 0.14524026 0.15506531 0.16336495 0.15942898]\n",
            " [0.11378203 0.17318008 0.15633767 0.1539164  0.17627391]\n",
            " [0.16234049 0.15266738 0.15103676 0.13384521 0.1618779 ]\n",
            " [0.17085307 0.19693504 0.20083485 0.18434932 0.22322443]] -> [0.27973254]\n",
            "[[0.08674523 0.11800776 0.12338113 0.18854246 0.14240469]\n",
            " [0.12709078 0.16710894 0.17475441 0.14139287 0.18487556]\n",
            " [0.15023079 0.14524026 0.15506531 0.16336495 0.15942898]\n",
            " [0.11378203 0.17318008 0.15633767 0.1539164  0.17627391]\n",
            " [0.16234049 0.15266738 0.15103676 0.13384521 0.1618779 ]\n",
            " [0.17085307 0.19693504 0.20083485 0.18434932 0.22322443]\n",
            " [0.22210892 0.26059544 0.26629347 0.09798915 0.27973254]] -> [0.31288462]\n",
            "[[0.12709078 0.16710894 0.17475441 0.14139287 0.18487556]\n",
            " [0.15023079 0.14524026 0.15506531 0.16336495 0.15942898]\n",
            " [0.11378203 0.17318008 0.15633767 0.1539164  0.17627391]\n",
            " [0.16234049 0.15266738 0.15103676 0.13384521 0.1618779 ]\n",
            " [0.17085307 0.19693504 0.20083485 0.18434932 0.22322443]\n",
            " [0.22210892 0.26059544 0.26629347 0.09798915 0.27973254]\n",
            " [0.28847189 0.29849695 0.31881782 0.13993925 0.31288462]] -> [0.29102191]\n",
            "[[0.15023079 0.14524026 0.15506531 0.16336495 0.15942898]\n",
            " [0.11378203 0.17318008 0.15633767 0.1539164  0.17627391]\n",
            " [0.16234049 0.15266738 0.15103676 0.13384521 0.1618779 ]\n",
            " [0.17085307 0.19693504 0.20083485 0.18434932 0.22322443]\n",
            " [0.22210892 0.26059544 0.26629347 0.09798915 0.27973254]\n",
            " [0.28847189 0.29849695 0.31881782 0.13993925 0.31288462]\n",
            " [0.31544869 0.30474509 0.32451253 0.06520807 0.29102191]] -> [0.31473634]\n",
            "[[0.11378203 0.17318008 0.15633767 0.1539164  0.17627391]\n",
            " [0.16234049 0.15266738 0.15103676 0.13384521 0.1618779 ]\n",
            " [0.17085307 0.19693504 0.20083485 0.18434932 0.22322443]\n",
            " [0.22210892 0.26059544 0.26629347 0.09798915 0.27973254]\n",
            " [0.28847189 0.29849695 0.31881782 0.13993925 0.31288462]\n",
            " [0.31544869 0.30474509 0.32451253 0.06520807 0.29102191]\n",
            " [0.30321934 0.29419409 0.33166151 0.06787305 0.31473634]] -> [0.30816543]\n",
            "[[0.16234049 0.15266738 0.15103676 0.13384521 0.1618779 ]\n",
            " [0.17085307 0.19693504 0.20083485 0.18434932 0.22322443]\n",
            " [0.22210892 0.26059544 0.26629347 0.09798915 0.27973254]\n",
            " [0.28847189 0.29849695 0.31881782 0.13993925 0.31288462]\n",
            " [0.31544869 0.30474509 0.32451253 0.06520807 0.29102191]\n",
            " [0.30321934 0.29419409 0.33166151 0.06787305 0.31473634]\n",
            " [0.32809772 0.31500144 0.33787093 0.12920479 0.30816543]] -> [0.39137465]\n",
            "[[0.17085307 0.19693504 0.20083485 0.18434932 0.22322443]\n",
            " [0.22210892 0.26059544 0.26629347 0.09798915 0.27973254]\n",
            " [0.28847189 0.29849695 0.31881782 0.13993925 0.31288462]\n",
            " [0.31544869 0.30474509 0.32451253 0.06520807 0.29102191]\n",
            " [0.30321934 0.29419409 0.33166151 0.06787305 0.31473634]\n",
            " [0.32809772 0.31500144 0.33787093 0.12920479 0.30816543]\n",
            " [0.30999354 0.37624526 0.35080521 0.13222386 0.39137465]] -> [0.41036989]\n",
            "[[0.22210892 0.26059544 0.26629347 0.09798915 0.27973254]\n",
            " [0.28847189 0.29849695 0.31881782 0.13993925 0.31288462]\n",
            " [0.31544869 0.30474509 0.32451253 0.06520807 0.29102191]\n",
            " [0.30321934 0.29419409 0.33166151 0.06787305 0.31473634]\n",
            " [0.32809772 0.31500144 0.33787093 0.12920479 0.30816543]\n",
            " [0.30999354 0.37624526 0.35080521 0.13222386 0.39137465]\n",
            " [0.35303648 0.38131443 0.39811961 0.11955124 0.41036989]] -> [0.43563708]\n",
            "[[0.28847189 0.29849695 0.31881782 0.13993925 0.31288462]\n",
            " [0.31544869 0.30474509 0.32451253 0.06520807 0.29102191]\n",
            " [0.30321934 0.29419409 0.33166151 0.06787305 0.31473634]\n",
            " [0.32809772 0.31500144 0.33787093 0.12920479 0.30816543]\n",
            " [0.30999354 0.37624526 0.35080521 0.13222386 0.39137465]\n",
            " [0.35303648 0.38131443 0.39811961 0.11955124 0.41036989]\n",
            " [0.39763814 0.41161218 0.44719075 0.13114296 0.43563708]] -> [0.4203453]\n",
            "[[0.31544869 0.30474509 0.32451253 0.06520807 0.29102191]\n",
            " [0.30321934 0.29419409 0.33166151 0.06787305 0.31473634]\n",
            " [0.32809772 0.31500144 0.33787093 0.12920479 0.30816543]\n",
            " [0.30999354 0.37624526 0.35080521 0.13222386 0.39137465]\n",
            " [0.35303648 0.38131443 0.39811961 0.11955124 0.41036989]\n",
            " [0.39763814 0.41161218 0.44719075 0.13114296 0.43563708]\n",
            " [0.41580232 0.40901859 0.43950302 0.0820552  0.4203453 ]] -> [0.44489572]\n",
            "[[0.30321934 0.29419409 0.33166151 0.06787305 0.31473634]\n",
            " [0.32809772 0.31500144 0.33787093 0.12920479 0.30816543]\n",
            " [0.30999354 0.37624526 0.35080521 0.13222386 0.39137465]\n",
            " [0.35303648 0.38131443 0.39811961 0.11955124 0.41036989]\n",
            " [0.39763814 0.41161218 0.44719075 0.13114296 0.43563708]\n",
            " [0.41580232 0.40901859 0.43950302 0.0820552  0.4203453 ]\n",
            " [0.42479459 0.41815498 0.44391946 0.12557073 0.44489572]] -> [0.42715502]\n",
            "[[0.32809772 0.31500144 0.33787093 0.12920479 0.30816543]\n",
            " [0.30999354 0.37624526 0.35080521 0.13222386 0.39137465]\n",
            " [0.35303648 0.38131443 0.39811961 0.11955124 0.41036989]\n",
            " [0.39763814 0.41161218 0.44719075 0.13114296 0.43563708]\n",
            " [0.41580232 0.40901859 0.43950302 0.0820552  0.4203453 ]\n",
            " [0.42479459 0.41815498 0.44391946 0.12557073 0.44489572]\n",
            " [0.41766071 0.41444142 0.44961417 0.08319201 0.42715502]] -> [0.41909076]\n",
            "[[0.30999354 0.37624526 0.35080521 0.13222386 0.39137465]\n",
            " [0.35303648 0.38131443 0.39811961 0.11955124 0.41036989]\n",
            " [0.39763814 0.41161218 0.44719075 0.13114296 0.43563708]\n",
            " [0.41580232 0.40901859 0.43950302 0.0820552  0.4203453 ]\n",
            " [0.42479459 0.41815498 0.44391946 0.12557073 0.44489572]\n",
            " [0.41766071 0.41444142 0.44961417 0.08319201 0.42715502]\n",
            " [0.40788909 0.40901859 0.43004636 0.11194767 0.41909076]] -> [0.43910174]\n",
            "[[0.35303648 0.38131443 0.39811961 0.11955124 0.41036989]\n",
            " [0.39763814 0.41161218 0.44719075 0.13114296 0.43563708]\n",
            " [0.41580232 0.40901859 0.43950302 0.0820552  0.4203453 ]\n",
            " [0.42479459 0.41815498 0.44391946 0.12557073 0.44489572]\n",
            " [0.41766071 0.41444142 0.44961417 0.08319201 0.42715502]\n",
            " [0.40788909 0.40901859 0.43004636 0.11194767 0.41909076]\n",
            " [0.40333331 0.42534644 0.44658508 0.17266442 0.43910174]] -> [0.46383117]\n",
            "[[0.39763814 0.41161218 0.44719075 0.13114296 0.43563708]\n",
            " [0.41580232 0.40901859 0.43950302 0.0820552  0.4203453 ]\n",
            " [0.42479459 0.41815498 0.44391946 0.12557073 0.44489572]\n",
            " [0.41766071 0.41444142 0.44961417 0.08319201 0.42715502]\n",
            " [0.40788909 0.40901859 0.43004636 0.11194767 0.41909076]\n",
            " [0.40333331 0.42534644 0.44658508 0.17266442 0.43910174]\n",
            " [0.45584784 0.45004436 0.46051883 0.54868708 0.46383117]] -> [0.60050167]\n",
            "[[0.41580232 0.40901859 0.43950302 0.0820552  0.4203453 ]\n",
            " [0.42479459 0.41815498 0.44391946 0.12557073 0.44489572]\n",
            " [0.41766071 0.41444142 0.44961417 0.08319201 0.42715502]\n",
            " [0.40788909 0.40901859 0.43004636 0.11194767 0.41909076]\n",
            " [0.40333331 0.42534644 0.44658508 0.17266442 0.43910174]\n",
            " [0.45584784 0.45004436 0.46051883 0.54868708 0.46383117]\n",
            " [0.60973577 0.62628933 0.62693732 0.60670158 0.60050167]] -> [0.62493287]\n",
            "[[0.42479459 0.41815498 0.44391946 0.12557073 0.44489572]\n",
            " [0.41766071 0.41444142 0.44961417 0.08319201 0.42715502]\n",
            " [0.40788909 0.40901859 0.43004636 0.11194767 0.41909076]\n",
            " [0.40333331 0.42534644 0.44658508 0.17266442 0.43910174]\n",
            " [0.45584784 0.45004436 0.46051883 0.54868708 0.46383117]\n",
            " [0.60973577 0.62628933 0.62693732 0.60670158 0.60050167]\n",
            " [0.5400757  0.637371   0.59252688 0.39406251 0.62493287]] -> [0.6141211]\n",
            "[[0.41766071 0.41444142 0.44961417 0.08319201 0.42715502]\n",
            " [0.40788909 0.40901859 0.43004636 0.11194767 0.41909076]\n",
            " [0.40333331 0.42534644 0.44658508 0.17266442 0.43910174]\n",
            " [0.45584784 0.45004436 0.46051883 0.54868708 0.46383117]\n",
            " [0.60973577 0.62628933 0.62693732 0.60670158 0.60050167]\n",
            " [0.5400757  0.637371   0.59252688 0.39406251 0.62493287]\n",
            " [0.58563637 0.6103155  0.63347989 0.22335489 0.6141211 ]] -> [0.62672481]\n",
            "[[0.40788909 0.40901859 0.43004636 0.11194767 0.41909076]\n",
            " [0.40333331 0.42534644 0.44658508 0.17266442 0.43910174]\n",
            " [0.45584784 0.45004436 0.46051883 0.54868708 0.46383117]\n",
            " [0.60973577 0.62628933 0.62693732 0.60670158 0.60050167]\n",
            " [0.5400757  0.637371   0.59252688 0.39406251 0.62493287]\n",
            " [0.58563637 0.6103155  0.63347989 0.22335489 0.6141211 ]\n",
            " [0.5765841  0.59481297 0.62724015 0.13071433 0.62672481]] -> [0.61734658]\n",
            "[[0.40333331 0.42534644 0.44658508 0.17266442 0.43910174]\n",
            " [0.45584784 0.45004436 0.46051883 0.54868708 0.46383117]\n",
            " [0.60973577 0.62628933 0.62693732 0.60670158 0.60050167]\n",
            " [0.5400757  0.637371   0.59252688 0.39406251 0.62493287]\n",
            " [0.58563637 0.6103155  0.63347989 0.22335489 0.6141211 ]\n",
            " [0.5765841  0.59481297 0.62724015 0.13071433 0.62672481]\n",
            " [0.60679799 0.59988214 0.63920492 0.10330047 0.61734658]] -> [0.68072385]\n",
            "[[0.45584784 0.45004436 0.46051883 0.54868708 0.46383117]\n",
            " [0.60973577 0.62628933 0.62693732 0.60670158 0.60050167]\n",
            " [0.5400757  0.637371   0.59252688 0.39406251 0.62493287]\n",
            " [0.58563637 0.6103155  0.63347989 0.22335489 0.6141211 ]\n",
            " [0.5765841  0.59481297 0.62724015 0.13071433 0.62672481]\n",
            " [0.60679799 0.59988214 0.63920492 0.10330047 0.61734658]\n",
            " [0.61615029 0.65275552 0.66068145 0.22646714 0.68072385]] -> [0.67797633]\n",
            "[[0.60973577 0.62628933 0.62693732 0.60670158 0.60050167]\n",
            " [0.5400757  0.637371   0.59252688 0.39406251 0.62493287]\n",
            " [0.58563637 0.6103155  0.63347989 0.22335489 0.6141211 ]\n",
            " [0.5765841  0.59481297 0.62724015 0.13071433 0.62672481]\n",
            " [0.60679799 0.59988214 0.63920492 0.10330047 0.61734658]\n",
            " [0.61615029 0.65275552 0.66068145 0.22646714 0.68072385]\n",
            " [0.66542774 0.65033894 0.69552192 0.09726234 0.67797633]] -> [0.69290973]\n",
            "[[0.5400757  0.637371   0.59252688 0.39406251 0.62493287]\n",
            " [0.58563637 0.6103155  0.63347989 0.22335489 0.6141211 ]\n",
            " [0.5765841  0.59481297 0.62724015 0.13071433 0.62672481]\n",
            " [0.60679799 0.59988214 0.63920492 0.10330047 0.61734658]\n",
            " [0.61615029 0.65275552 0.66068145 0.22646714 0.68072385]\n",
            " [0.66542774 0.65033894 0.69552192 0.09726234 0.67797633]\n",
            " [0.66003224 0.68753314 0.71054018 0.13658473 0.69290973]] -> [0.69541844]\n",
            "[[0.58563637 0.6103155  0.63347989 0.22335489 0.6141211 ]\n",
            " [0.5765841  0.59481297 0.62724015 0.13071433 0.62672481]\n",
            " [0.60679799 0.59988214 0.63920492 0.10330047 0.61734658]\n",
            " [0.61615029 0.65275552 0.66068145 0.22646714 0.68072385]\n",
            " [0.66542774 0.65033894 0.69552192 0.09726234 0.67797633]\n",
            " [0.66003224 0.68753314 0.71054018 0.13658473 0.69290973]\n",
            " [0.67591872 0.67498973 0.72391669 0.03734695 0.69541844]] -> [0.69643381]\n",
            "[[0.5765841  0.59481297 0.62724015 0.13071433 0.62672481]\n",
            " [0.60679799 0.59988214 0.63920492 0.10330047 0.61734658]\n",
            " [0.61615029 0.65275552 0.66068145 0.22646714 0.68072385]\n",
            " [0.66542774 0.65033894 0.69552192 0.09726234 0.67797633]\n",
            " [0.66003224 0.68753314 0.71054018 0.13658473 0.69290973]\n",
            " [0.67591872 0.67498973 0.72391669 0.03734695 0.69541844]\n",
            " [0.68341227 0.69230766 0.7250796  0.07251346 0.69643381]] -> [0.68669721]\n",
            "[[0.60679799 0.59988214 0.63920492 0.10330047 0.61734658]\n",
            " [0.61615029 0.65275552 0.66068145 0.22646714 0.68072385]\n",
            " [0.66542774 0.65033894 0.69552192 0.09726234 0.67797633]\n",
            " [0.66003224 0.68753314 0.71054018 0.13658473 0.69290973]\n",
            " [0.67591872 0.67498973 0.72391669 0.03734695 0.69541844]\n",
            " [0.68341227 0.69230766 0.7250796  0.07251346 0.69643381]\n",
            " [0.66243032 0.65484831 0.70951039 0.02879293 0.68669721]] -> [0.68203817]\n",
            "[[0.61615029 0.65275552 0.66068145 0.22646714 0.68072385]\n",
            " [0.66542774 0.65033894 0.69552192 0.09726234 0.67797633]\n",
            " [0.66003224 0.68753314 0.71054018 0.13658473 0.69290973]\n",
            " [0.67591872 0.67498973 0.72391669 0.03734695 0.69541844]\n",
            " [0.68341227 0.69230766 0.7250796  0.07251346 0.69643381]\n",
            " [0.66243032 0.65484831 0.70951039 0.02879293 0.68669721]\n",
            " [0.67591872 0.67898606 0.70775328 0.06563671 0.68203817]] -> [0.65037944]\n",
            "[[0.66542774 0.65033894 0.69552192 0.09726234 0.67797633]\n",
            " [0.66003224 0.68753314 0.71054018 0.13658473 0.69290973]\n",
            " [0.67591872 0.67498973 0.72391669 0.03734695 0.69541844]\n",
            " [0.68341227 0.69230766 0.7250796  0.07251346 0.69643381]\n",
            " [0.66243032 0.65484831 0.70951039 0.02879293 0.68669721]\n",
            " [0.67591872 0.67898606 0.70775328 0.06563671 0.68203817]\n",
            " [0.65523643 0.64061289 0.66736934 0.08237202 0.65037944]] -> [0.6669254]\n",
            "[[0.66003224 0.68753314 0.71054018 0.13658473 0.69290973]\n",
            " [0.67591872 0.67498973 0.72391669 0.03734695 0.69541844]\n",
            " [0.68341227 0.69230766 0.7250796  0.07251346 0.69643381]\n",
            " [0.66243032 0.65484831 0.70951039 0.02879293 0.68669721]\n",
            " [0.67591872 0.67898606 0.70775328 0.06563671 0.68203817]\n",
            " [0.65523643 0.64061289 0.66736934 0.08237202 0.65037944]\n",
            " [0.63737191 0.63961093 0.6678907  0.06302764 0.6669254 ]] -> [0.65252976]\n",
            "[[0.67591872 0.67498973 0.72391669 0.03734695 0.69541844]\n",
            " [0.68341227 0.69230766 0.7250796  0.07251346 0.69643381]\n",
            " [0.66243032 0.65484831 0.70951039 0.02879293 0.68669721]\n",
            " [0.67591872 0.67898606 0.70775328 0.06563671 0.68203817]\n",
            " [0.65523643 0.64061289 0.66736934 0.08237202 0.65037944]\n",
            " [0.63737191 0.63961093 0.6678907  0.06302764 0.6669254 ]\n",
            " [0.65349806 0.65169456 0.68891242 0.02486069 0.65252976]] -> [0.64010507]\n",
            "[[0.68341227 0.69230766 0.7250796  0.07251346 0.69643381]\n",
            " [0.66243032 0.65484831 0.70951039 0.02879293 0.68669721]\n",
            " [0.67591872 0.67898606 0.70775328 0.06563671 0.68203817]\n",
            " [0.65523643 0.64061289 0.66736934 0.08237202 0.65037944]\n",
            " [0.63737191 0.63961093 0.6678907  0.06302764 0.6669254 ]\n",
            " [0.65349806 0.65169456 0.68891242 0.02486069 0.65252976]\n",
            " [0.62346383 0.61774225 0.66546728 0.05134274 0.64010507]] -> [0.62057243]\n",
            "[[0.66243032 0.65484831 0.70951039 0.02879293 0.68669721]\n",
            " [0.67591872 0.67898606 0.70775328 0.06563671 0.68203817]\n",
            " [0.65523643 0.64061289 0.66736934 0.08237202 0.65037944]\n",
            " [0.63737191 0.63961093 0.6678907  0.06302764 0.6669254 ]\n",
            " [0.65349806 0.65169456 0.68891242 0.02486069 0.65252976]\n",
            " [0.62346383 0.61774225 0.66546728 0.05134274 0.64010507]\n",
            " [0.61309287 0.60265238 0.64680797 0.06809668 0.62057243]] -> [0.62015425]\n",
            "[[0.67591872 0.67898606 0.70775328 0.06563671 0.68203817]\n",
            " [0.65523643 0.64061289 0.66736934 0.08237202 0.65037944]\n",
            " [0.63737191 0.63961093 0.6678907  0.06302764 0.6669254 ]\n",
            " [0.65349806 0.65169456 0.68891242 0.02486069 0.65252976]\n",
            " [0.62346383 0.61774225 0.66546728 0.05134274 0.64010507]\n",
            " [0.61309287 0.60265238 0.64680797 0.06809668 0.62057243]\n",
            " [0.62634123 0.61385204 0.65731883 0.06377309 0.62015425]] -> [0.60557924]\n",
            "[[0.65523643 0.64061289 0.66736934 0.08237202 0.65037944]\n",
            " [0.63737191 0.63961093 0.6678907  0.06302764 0.6669254 ]\n",
            " [0.65349806 0.65169456 0.68891242 0.02486069 0.65252976]\n",
            " [0.62346383 0.61774225 0.66546728 0.05134274 0.64010507]\n",
            " [0.61309287 0.60265238 0.64680797 0.06809668 0.62057243]\n",
            " [0.62634123 0.61385204 0.65731883 0.06377309 0.62015425]\n",
            " [0.59696672 0.60241673 0.6287547  0.09057195 0.60557924]] -> [0.60420512]\n",
            "[[0.63737191 0.63961093 0.6678907  0.06302764 0.6669254 ]\n",
            " [0.65349806 0.65169456 0.68891242 0.02486069 0.65252976]\n",
            " [0.62346383 0.61774225 0.66546728 0.05134274 0.64010507]\n",
            " [0.61309287 0.60265238 0.64680797 0.06809668 0.62057243]\n",
            " [0.62634123 0.61385204 0.65731883 0.06377309 0.62015425]\n",
            " [0.59696672 0.60241673 0.6287547  0.09057195 0.60557924]\n",
            " [0.57550507 0.5870322  0.6052186  0.06353081 0.60420512]] -> [0.60498169]\n",
            "[[0.65349806 0.65169456 0.68891242 0.02486069 0.65252976]\n",
            " [0.62346383 0.61774225 0.66546728 0.05134274 0.64010507]\n",
            " [0.61309287 0.60265238 0.64680797 0.06809668 0.62057243]\n",
            " [0.62634123 0.61385204 0.65731883 0.06377309 0.62015425]\n",
            " [0.59696672 0.60241673 0.6287547  0.09057195 0.60557924]\n",
            " [0.57550507 0.5870322  0.6052186  0.06353081 0.60420512]\n",
            " [0.58749476 0.61173012 0.62136353 0.10822043 0.60498169]] -> [0.62057243]\n",
            "[[0.62346383 0.61774225 0.66546728 0.05134274 0.64010507]\n",
            " [0.61309287 0.60265238 0.64680797 0.06809668 0.62057243]\n",
            " [0.62634123 0.61385204 0.65731883 0.06377309 0.62015425]\n",
            " [0.59696672 0.60241673 0.6287547  0.09057195 0.60557924]\n",
            " [0.57550507 0.5870322  0.6052186  0.06353081 0.60420512]\n",
            " [0.58749476 0.61173012 0.62136353 0.10822043 0.60498169]\n",
            " [0.58593604 0.60530497 0.62599812 0.04875231 0.62057243]] -> [0.60229397]\n",
            "[[0.61309287 0.60265238 0.64680797 0.06809668 0.62057243]\n",
            " [0.62634123 0.61385204 0.65731883 0.06377309 0.62015425]\n",
            " [0.59696672 0.60241673 0.6287547  0.09057195 0.60557924]\n",
            " [0.57550507 0.5870322  0.6052186  0.06353081 0.60420512]\n",
            " [0.58749476 0.61173012 0.62136353 0.10822043 0.60498169]\n",
            " [0.58593604 0.60530497 0.62599812 0.04875231 0.62057243]\n",
            " [0.58947316 0.60221633 0.62736144 0.10138094 0.60229397]] -> [0.59010808]\n",
            "[[0.62634123 0.61385204 0.65731883 0.06377309 0.62015425]\n",
            " [0.59696672 0.60241673 0.6287547  0.09057195 0.60557924]\n",
            " [0.57550507 0.5870322  0.6052186  0.06353081 0.60420512]\n",
            " [0.58749476 0.61173012 0.62136353 0.10822043 0.60498169]\n",
            " [0.58593604 0.60530497 0.62599812 0.04875231 0.62057243]\n",
            " [0.58947316 0.60221633 0.62736144 0.10138094 0.60229397]\n",
            " [0.57556508 0.57052771 0.61851636 0.12348348 0.59010808]] -> [0.60044224]\n",
            "[[0.59696672 0.60241673 0.6287547  0.09057195 0.60557924]\n",
            " [0.57550507 0.5870322  0.6052186  0.06353081 0.60420512]\n",
            " [0.58749476 0.61173012 0.62136353 0.10822043 0.60498169]\n",
            " [0.58593604 0.60530497 0.62599812 0.04875231 0.62057243]\n",
            " [0.58947316 0.60221633 0.62736144 0.10138094 0.60229397]\n",
            " [0.57556508 0.57052771 0.61851636 0.12348348 0.59010808]\n",
            " [0.58899347 0.58190403 0.61197342 0.06319536 0.60044224]] -> [0.6164508]\n",
            "[[0.57550507 0.5870322  0.6052186  0.06353081 0.60420512]\n",
            " [0.58749476 0.61173012 0.62136353 0.10822043 0.60498169]\n",
            " [0.58593604 0.60530497 0.62599812 0.04875231 0.62057243]\n",
            " [0.58947316 0.60221633 0.62736144 0.10138094 0.60229397]\n",
            " [0.57556508 0.57052771 0.61851636 0.12348348 0.59010808]\n",
            " [0.58899347 0.58190403 0.61197342 0.06319536 0.60044224]\n",
            " [0.61153415 0.59899782 0.63687246 0.0907024  0.6164508 ]] -> [0.66794113]\n",
            "[[0.58749476 0.61173012 0.62136353 0.10822043 0.60498169]\n",
            " [0.58593604 0.60530497 0.62599812 0.04875231 0.62057243]\n",
            " [0.58947316 0.60221633 0.62736144 0.10138094 0.60229397]\n",
            " [0.57556508 0.57052771 0.61851636 0.12348348 0.59010808]\n",
            " [0.58899347 0.58190403 0.61197342 0.06319536 0.60044224]\n",
            " [0.61153415 0.59899782 0.63687246 0.0907024  0.6164508 ]\n",
            " [0.61417189 0.64662538 0.65256332 0.15967498 0.66794113]] -> [0.66955369]\n",
            "[[0.58593604 0.60530497 0.62599812 0.04875231 0.62057243]\n",
            " [0.58947316 0.60221633 0.62736144 0.10138094 0.60229397]\n",
            " [0.57556508 0.57052771 0.61851636 0.12348348 0.59010808]\n",
            " [0.58899347 0.58190403 0.61197342 0.06319536 0.60044224]\n",
            " [0.61153415 0.59899782 0.63687246 0.0907024  0.6164508 ]\n",
            " [0.61417189 0.64662538 0.65256332 0.15967498 0.66794113]\n",
            " [0.65343805 0.65092824 0.68406595 0.0573436  0.66955369]] -> [0.63950788]\n",
            "[[0.58947316 0.60221633 0.62736144 0.10138094 0.60229397]\n",
            " [0.57556508 0.57052771 0.61851636 0.12348348 0.59010808]\n",
            " [0.58899347 0.58190403 0.61197342 0.06319536 0.60044224]\n",
            " [0.61153415 0.59899782 0.63687246 0.0907024  0.6164508 ]\n",
            " [0.61417189 0.64662538 0.65256332 0.15967498 0.66794113]\n",
            " [0.65343805 0.65092824 0.68406595 0.0573436  0.66955369]\n",
            " [0.64498548 0.63689934 0.66819353 0.12752753 0.63950788]] -> [0.54596486]\n",
            "[[0.57556508 0.57052771 0.61851636 0.12348348 0.59010808]\n",
            " [0.58899347 0.58190403 0.61197342 0.06319536 0.60044224]\n",
            " [0.61153415 0.59899782 0.63687246 0.0907024  0.6164508 ]\n",
            " [0.61417189 0.64662538 0.65256332 0.15967498 0.66794113]\n",
            " [0.65343805 0.65092824 0.68406595 0.0573436  0.66955369]\n",
            " [0.64498548 0.63689934 0.66819353 0.12752753 0.63950788]\n",
            " [0.59408895 0.59501911 0.58386335 0.24217745 0.54596486]] -> [0.60187579]\n",
            "[[0.58899347 0.58190403 0.61197342 0.06319536 0.60044224]\n",
            " [0.61153415 0.59899782 0.63687246 0.0907024  0.6164508 ]\n",
            " [0.61417189 0.64662538 0.65256332 0.15967498 0.66794113]\n",
            " [0.65343805 0.65092824 0.68406595 0.0573436  0.66955369]\n",
            " [0.64498548 0.63689934 0.66819353 0.12752753 0.63950788]\n",
            " [0.59408895 0.59501911 0.58386335 0.24217745 0.54596486]\n",
            " [0.5043463  0.5776008  0.5495742  0.13511247 0.60187579]] -> [0.54614424]\n",
            "[[0.61153415 0.59899782 0.63687246 0.0907024  0.6164508 ]\n",
            " [0.61417189 0.64662538 0.65256332 0.15967498 0.66794113]\n",
            " [0.65343805 0.65092824 0.68406595 0.0573436  0.66955369]\n",
            " [0.64498548 0.63689934 0.66819353 0.12752753 0.63950788]\n",
            " [0.59408895 0.59501911 0.58386335 0.24217745 0.54596486]\n",
            " [0.5043463  0.5776008  0.5495742  0.13511247 0.60187579]\n",
            " [0.56039794 0.5536102  0.56047886 0.15076688 0.54614424]] -> [0.56286957]\n",
            "[[0.61417189 0.64662538 0.65256332 0.15967498 0.66794113]\n",
            " [0.65343805 0.65092824 0.68406595 0.0573436  0.66955369]\n",
            " [0.64498548 0.63689934 0.66819353 0.12752753 0.63950788]\n",
            " [0.59408895 0.59501911 0.58386335 0.24217745 0.54596486]\n",
            " [0.5043463  0.5776008  0.5495742  0.13511247 0.60187579]\n",
            " [0.56039794 0.5536102  0.56047886 0.15076688 0.54614424]\n",
            " [0.53120308 0.56221628 0.58053142 0.09344192 0.56286957]] -> [0.61824274]\n",
            "[[0.65343805 0.65092824 0.68406595 0.0573436  0.66955369]\n",
            " [0.64498548 0.63689934 0.66819353 0.12752753 0.63950788]\n",
            " [0.59408895 0.59501911 0.58386335 0.24217745 0.54596486]\n",
            " [0.5043463  0.5776008  0.5495742  0.13511247 0.60187579]\n",
            " [0.56039794 0.5536102  0.56047886 0.15076688 0.54614424]\n",
            " [0.53120308 0.56221628 0.58053142 0.09344192 0.56286957]\n",
            " [0.55086635 0.59829051 0.5856811  0.13401293 0.61824274]] -> [0.60103943]\n",
            "[[0.64498548 0.63689934 0.66819353 0.12752753 0.63950788]\n",
            " [0.59408895 0.59501911 0.58386335 0.24217745 0.54596486]\n",
            " [0.5043463  0.5776008  0.5495742  0.13511247 0.60187579]\n",
            " [0.56039794 0.5536102  0.56047886 0.15076688 0.54614424]\n",
            " [0.53120308 0.56221628 0.58053142 0.09344192 0.56286957]\n",
            " [0.55086635 0.59829051 0.5856811  0.13401293 0.61824274]\n",
            " [0.59199089 0.57441791 0.61415427 0.27270355 0.60103943]] -> [0.58204419]\n",
            "[[0.59408895 0.59501911 0.58386335 0.24217745 0.54596486]\n",
            " [0.5043463  0.5776008  0.5495742  0.13511247 0.60187579]\n",
            " [0.56039794 0.5536102  0.56047886 0.15076688 0.54614424]\n",
            " [0.53120308 0.56221628 0.58053142 0.09344192 0.56286957]\n",
            " [0.55086635 0.59829051 0.5856811  0.13401293 0.61824274]\n",
            " [0.59199089 0.57441791 0.61415427 0.27270355 0.60103943]\n",
            " [0.60799703 0.59946948 0.61282756 0.10935724 0.58204419]] -> [0.61615184]\n",
            "[[0.5043463  0.5776008  0.5495742  0.13511247 0.60187579]\n",
            " [0.56039794 0.5536102  0.56047886 0.15076688 0.54614424]\n",
            " [0.53120308 0.56221628 0.58053142 0.09344192 0.56286957]\n",
            " [0.55086635 0.59829051 0.5856811  0.13401293 0.61824274]\n",
            " [0.59199089 0.57441791 0.61415427 0.27270355 0.60103943]\n",
            " [0.60799703 0.59946948 0.61282756 0.10935724 0.58204419]\n",
            " [0.58749476 0.59552027 0.63759978 0.07316573 0.61615184]] -> [0.64488369]\n",
            "[[0.56039794 0.5536102  0.56047886 0.15076688 0.54614424]\n",
            " [0.53120308 0.56221628 0.58053142 0.09344192 0.56286957]\n",
            " [0.55086635 0.59829051 0.5856811  0.13401293 0.61824274]\n",
            " [0.59199089 0.57441791 0.61415427 0.27270355 0.60103943]\n",
            " [0.60799703 0.59946948 0.61282756 0.10935724 0.58204419]\n",
            " [0.58749476 0.59552027 0.63759978 0.07316573 0.61615184]\n",
            " [0.60943573 0.6180959  0.63621243 0.10842543 0.64488369]] -> [0.71053122]\n",
            "[[0.53120308 0.56221628 0.58053142 0.09344192 0.56286957]\n",
            " [0.55086635 0.59829051 0.5856811  0.13401293 0.61824274]\n",
            " [0.59199089 0.57441791 0.61415427 0.27270355 0.60103943]\n",
            " [0.60799703 0.59946948 0.61282756 0.10935724 0.58204419]\n",
            " [0.58749476 0.59552027 0.63759978 0.07316573 0.61615184]\n",
            " [0.60943573 0.6180959  0.63621243 0.10842543 0.64488369]\n",
            " [0.65343805 0.69289697 0.69763622 0.1677631  0.71053122]] -> [0.70867949]\n",
            "[[0.55086635 0.59829051 0.5856811  0.13401293 0.61824274]\n",
            " [0.59199089 0.57441791 0.61415427 0.27270355 0.60103943]\n",
            " [0.60799703 0.59946948 0.61282756 0.10935724 0.58204419]\n",
            " [0.58749476 0.59552027 0.63759978 0.07316573 0.61615184]\n",
            " [0.60943573 0.6180959  0.63621243 0.10842543 0.64488369]\n",
            " [0.65343805 0.69289697 0.69763622 0.1677631  0.71053122]\n",
            " [0.69294424 0.68747414 0.73222858 0.1539164  0.70867949]] -> [0.63287754]\n",
            "[[0.59199089 0.57441791 0.61415427 0.27270355 0.60103943]\n",
            " [0.60799703 0.59946948 0.61282756 0.10935724 0.58204419]\n",
            " [0.58749476 0.59552027 0.63759978 0.07316573 0.61615184]\n",
            " [0.60943573 0.6180959  0.63621243 0.10842543 0.64488369]\n",
            " [0.65343805 0.69289697 0.69763622 0.1677631  0.71053122]\n",
            " [0.69294424 0.68747414 0.73222858 0.1539164  0.70867949]\n",
            " [0.66986386 0.65098725 0.6651038  0.17652211 0.63287754]] -> [0.68544303]\n",
            "[[0.60799703 0.59946948 0.61282756 0.10935724 0.58204419]\n",
            " [0.58749476 0.59552027 0.63759978 0.07316573 0.61615184]\n",
            " [0.60943573 0.6180959  0.63621243 0.10842543 0.64488369]\n",
            " [0.65343805 0.69289697 0.69763622 0.1677631  0.71053122]\n",
            " [0.69294424 0.68747414 0.73222858 0.1539164  0.70867949]\n",
            " [0.66986386 0.65098725 0.6651038  0.17652211 0.63287754]\n",
            " [0.62646125 0.67014433 0.67260369 0.10570454 0.68544303]] -> [0.67678159]\n",
            "[[0.58749476 0.59552027 0.63759978 0.07316573 0.61615184]\n",
            " [0.60943573 0.6180959  0.63621243 0.10842543 0.64488369]\n",
            " [0.65343805 0.69289697 0.69763622 0.1677631  0.71053122]\n",
            " [0.69294424 0.68747414 0.73222858 0.1539164  0.70867949]\n",
            " [0.66986386 0.65098725 0.6651038  0.17652211 0.63287754]\n",
            " [0.62646125 0.67014433 0.67260369 0.10570454 0.68544303]\n",
            " [0.64054899 0.64550542 0.67661413 0.09761643 0.67678159]] -> [0.63765616]\n",
            "[[0.60943573 0.6180959  0.63621243 0.10842543 0.64488369]\n",
            " [0.65343805 0.69289697 0.69763622 0.1677631  0.71053122]\n",
            " [0.69294424 0.68747414 0.73222858 0.1539164  0.70867949]\n",
            " [0.66986386 0.65098725 0.6651038  0.17652211 0.63287754]\n",
            " [0.62646125 0.67014433 0.67260369 0.10570454 0.68544303]\n",
            " [0.64054899 0.64550542 0.67661413 0.09761643 0.67678159]\n",
            " [0.66207064 0.66902437 0.67214334 0.13580201 0.63765616]] -> [0.65127523]\n",
            "[[0.65343805 0.69289697 0.69763622 0.1677631  0.71053122]\n",
            " [0.69294424 0.68747414 0.73222858 0.1539164  0.70867949]\n",
            " [0.66986386 0.65098725 0.6651038  0.17652211 0.63287754]\n",
            " [0.62646125 0.67014433 0.67260369 0.10570454 0.68544303]\n",
            " [0.64054899 0.64550542 0.67661413 0.09761643 0.67678159]\n",
            " [0.66207064 0.66902437 0.67214334 0.13580201 0.63765616]\n",
            " [0.63143707 0.64037724 0.67128327 0.18623157 0.65127523]] -> [0.62302135]\n",
            "[[0.69294424 0.68747414 0.73222858 0.1539164  0.70867949]\n",
            " [0.66986386 0.65098725 0.6651038  0.17652211 0.63287754]\n",
            " [0.62646125 0.67014433 0.67260369 0.10570454 0.68544303]\n",
            " [0.64054899 0.64550542 0.67661413 0.09761643 0.67678159]\n",
            " [0.66207064 0.66902437 0.67214334 0.13580201 0.63765616]\n",
            " [0.63143707 0.64037724 0.67128327 0.18623157 0.65127523]\n",
            " [0.61896769 0.61164161 0.64347604 0.12911161 0.62302135]] -> [0.64613823]\n",
            "[[0.66986386 0.65098725 0.6651038  0.17652211 0.63287754]\n",
            " [0.62646125 0.67014433 0.67260369 0.10570454 0.68544303]\n",
            " [0.64054899 0.64550542 0.67661413 0.09761643 0.67678159]\n",
            " [0.66207064 0.66902437 0.67214334 0.13580201 0.63765616]\n",
            " [0.63143707 0.64037724 0.67128327 0.18623157 0.65127523]\n",
            " [0.61896769 0.61164161 0.64347604 0.12911161 0.62302135]\n",
            " [0.62963869 0.62723265 0.66401338 0.11468719 0.64613823]] -> [0.64637703]\n",
            "[[0.62646125 0.67014433 0.67260369 0.10570454 0.68544303]\n",
            " [0.64054899 0.64550542 0.67661413 0.09761643 0.67678159]\n",
            " [0.66207064 0.66902437 0.67214334 0.13580201 0.63765616]\n",
            " [0.63143707 0.64037724 0.67128327 0.18623157 0.65127523]\n",
            " [0.61896769 0.61164161 0.64347604 0.12911161 0.62302135]\n",
            " [0.62963869 0.62723265 0.66401338 0.11468719 0.64613823]\n",
            " [0.6493016  0.64703804 0.68073402 0.16310405 0.64637703]] -> [0.64870672]\n",
            "[[0.64054899 0.64550542 0.67661413 0.09761643 0.67678159]\n",
            " [0.66207064 0.66902437 0.67214334 0.13580201 0.63765616]\n",
            " [0.63143707 0.64037724 0.67128327 0.18623157 0.65127523]\n",
            " [0.61896769 0.61164161 0.64347604 0.12911161 0.62302135]\n",
            " [0.62963869 0.62723265 0.66401338 0.11468719 0.64613823]\n",
            " [0.6493016  0.64703804 0.68073402 0.16310405 0.64637703]\n",
            " [0.64744321 0.63766564 0.68006734 0.09032967 0.64870672]] -> [0.63807433]\n",
            "[[0.66207064 0.66902437 0.67214334 0.13580201 0.63765616]\n",
            " [0.63143707 0.64037724 0.67128327 0.18623157 0.65127523]\n",
            " [0.61896769 0.61164161 0.64347604 0.12911161 0.62302135]\n",
            " [0.62963869 0.62723265 0.66401338 0.11468719 0.64613823]\n",
            " [0.6493016  0.64703804 0.68073402 0.16310405 0.64637703]\n",
            " [0.64744321 0.63766564 0.68006734 0.09032967 0.64870672]\n",
            " [0.65139964 0.63283212 0.65104877 0.0647049  0.63807433]] -> [0.70294493]\n",
            "[[0.63143707 0.64037724 0.67128327 0.18623157 0.65127523]\n",
            " [0.61896769 0.61164161 0.64347604 0.12911161 0.62302135]\n",
            " [0.62963869 0.62723265 0.66401338 0.11468719 0.64613823]\n",
            " [0.6493016  0.64703804 0.68073402 0.16310405 0.64637703]\n",
            " [0.64744321 0.63766564 0.68006734 0.09032967 0.64870672]\n",
            " [0.65139964 0.63283212 0.65104877 0.0647049  0.63807433]\n",
            " [0.63970999 0.69012673 0.68206664 0.1097486  0.70294493]] -> [0.68580142]\n",
            "[[0.61896769 0.61164161 0.64347604 0.12911161 0.62302135]\n",
            " [0.62963869 0.62723265 0.66401338 0.11468719 0.64613823]\n",
            " [0.6493016  0.64703804 0.68073402 0.16310405 0.64637703]\n",
            " [0.64744321 0.63766564 0.68006734 0.09032967 0.64870672]\n",
            " [0.65139964 0.63283212 0.65104877 0.0647049  0.63807433]\n",
            " [0.63970999 0.69012673 0.68206664 0.1097486  0.70294493]\n",
            " [0.69336355 0.70722089 0.71060082 0.14657373 0.68580142]] -> [0.70413968]\n",
            "[[0.62963869 0.62723265 0.66401338 0.11468719 0.64613823]\n",
            " [0.6493016  0.64703804 0.68073402 0.16310405 0.64637703]\n",
            " [0.64744321 0.63766564 0.68006734 0.09032967 0.64870672]\n",
            " [0.65139964 0.63283212 0.65104877 0.0647049  0.63807433]\n",
            " [0.63970999 0.69012673 0.68206664 0.1097486  0.70294493]\n",
            " [0.69336355 0.70722089 0.71060082 0.14657373 0.68580142]\n",
            " [0.67597873 0.68275862 0.71956682 0.06548762 0.70413968]] -> [0.65665139]\n",
            "[[0.6493016  0.64703804 0.68073402 0.16310405 0.64637703]\n",
            " [0.64744321 0.63766564 0.68006734 0.09032967 0.64870672]\n",
            " [0.65139964 0.63283212 0.65104877 0.0647049  0.63807433]\n",
            " [0.63970999 0.69012673 0.68206664 0.1097486  0.70294493]\n",
            " [0.69336355 0.70722089 0.71060082 0.14657373 0.68580142]\n",
            " [0.67597873 0.68275862 0.71956682 0.06548762 0.70413968]\n",
            " [0.66075159 0.64202751 0.66467969 0.14523193 0.65665139]] -> [0.6586825]\n",
            "[[0.64744321 0.63766564 0.68006734 0.09032967 0.64870672]\n",
            " [0.65139964 0.63283212 0.65104877 0.0647049  0.63807433]\n",
            " [0.63970999 0.69012673 0.68206664 0.1097486  0.70294493]\n",
            " [0.69336355 0.70722089 0.71060082 0.14657373 0.68580142]\n",
            " [0.67597873 0.68275862 0.71956682 0.06548762 0.70413968]\n",
            " [0.66075159 0.64202751 0.66467969 0.14523193 0.65665139]\n",
            " [0.66332969 0.65811971 0.68285424 0.04966548 0.6586825 ]] -> [0.66722436]\n",
            "[[0.65139964 0.63283212 0.65104877 0.0647049  0.63807433]\n",
            " [0.63970999 0.69012673 0.68206664 0.1097486  0.70294493]\n",
            " [0.69336355 0.70722089 0.71060082 0.14657373 0.68580142]\n",
            " [0.67597873 0.68275862 0.71956682 0.06548762 0.70413968]\n",
            " [0.66075159 0.64202751 0.66467969 0.14523193 0.65665139]\n",
            " [0.66332969 0.65811971 0.68285424 0.04966548 0.6586825 ]\n",
            " [0.65223901 0.66931902 0.69194151 0.09448555 0.66722436]] -> [0.75861669]\n",
            "[[0.63970999 0.69012673 0.68206664 0.1097486  0.70294493]\n",
            " [0.69336355 0.70722089 0.71060082 0.14657373 0.68580142]\n",
            " [0.67597873 0.68275862 0.71956682 0.06548762 0.70413968]\n",
            " [0.66075159 0.64202751 0.66467969 0.14523193 0.65665139]\n",
            " [0.66332969 0.65811971 0.68285424 0.04966548 0.6586825 ]\n",
            " [0.65223901 0.66931902 0.69194151 0.09448555 0.66722436]\n",
            " [0.70049744 0.7622162  0.74080077 0.27411991 0.75861669]] -> [0.7958904]\n",
            "[[0.69336355 0.70722089 0.71060082 0.14657373 0.68580142]\n",
            " [0.67597873 0.68275862 0.71956682 0.06548762 0.70413968]\n",
            " [0.66075159 0.64202751 0.66467969 0.14523193 0.65665139]\n",
            " [0.66332969 0.65811971 0.68285424 0.04966548 0.6586825 ]\n",
            " [0.65223901 0.66931902 0.69194151 0.09448555 0.66722436]\n",
            " [0.70049744 0.7622162  0.74080077 0.27411991 0.75861669]\n",
            " [0.76278395 0.77984067 0.8163162  0.22005628 0.7958904 ]] -> [0.7688309]\n",
            "[[0.67597873 0.68275862 0.71956682 0.06548762 0.70413968]\n",
            " [0.66075159 0.64202751 0.66467969 0.14523193 0.65665139]\n",
            " [0.66332969 0.65811971 0.68285424 0.04966548 0.6586825 ]\n",
            " [0.65223901 0.66931902 0.69194151 0.09448555 0.66722436]\n",
            " [0.70049744 0.7622162  0.74080077 0.27411991 0.75861669]\n",
            " [0.76278395 0.77984067 0.8163162  0.22005628 0.7958904 ]\n",
            " [0.79311786 0.77612711 0.80419983 0.21832311 0.7688309 ]] -> [0.78316711]\n",
            "[[0.66075159 0.64202751 0.66467969 0.14523193 0.65665139]\n",
            " [0.66332969 0.65811971 0.68285424 0.04966548 0.6586825 ]\n",
            " [0.65223901 0.66931902 0.69194151 0.09448555 0.66722436]\n",
            " [0.70049744 0.7622162  0.74080077 0.27411991 0.75861669]\n",
            " [0.76278395 0.77984067 0.8163162  0.22005628 0.7958904 ]\n",
            " [0.79311786 0.77612711 0.80419983 0.21832311 0.7688309 ]\n",
            " [0.74336072 0.74977892 0.79190154 0.12676345 0.78316711]] -> [0.86524101]\n",
            "[[0.66332969 0.65811971 0.68285424 0.04966548 0.6586825 ]\n",
            " [0.65223901 0.66931902 0.69194151 0.09448555 0.66722436]\n",
            " [0.70049744 0.7622162  0.74080077 0.27411991 0.75861669]\n",
            " [0.76278395 0.77984067 0.8163162  0.22005628 0.7958904 ]\n",
            " [0.79311786 0.77612711 0.80419983 0.21832311 0.7688309 ]\n",
            " [0.74336072 0.74977892 0.79190154 0.12676345 0.78316711]\n",
            " [0.80270982 0.84220445 0.85745113 0.20714139 0.86524101]] -> [0.83274593]\n",
            "[[0.65223901 0.66931902 0.69194151 0.09448555 0.66722436]\n",
            " [0.70049744 0.7622162  0.74080077 0.27411991 0.75861669]\n",
            " [0.76278395 0.77984067 0.8163162  0.22005628 0.7958904 ]\n",
            " [0.79311786 0.77612711 0.80419983 0.21832311 0.7688309 ]\n",
            " [0.74336072 0.74977892 0.79190154 0.12676345 0.78316711]\n",
            " [0.80270982 0.84220445 0.85745113 0.20714139 0.86524101]\n",
            " [0.87332891 0.85104618 0.8593898  0.18457295 0.83274593]] -> [0.78137517]\n",
            "[[0.70049744 0.7622162  0.74080077 0.27411991 0.75861669]\n",
            " [0.76278395 0.77984067 0.8163162  0.22005628 0.7958904 ]\n",
            " [0.79311786 0.77612711 0.80419983 0.21832311 0.7688309 ]\n",
            " [0.74336072 0.74977892 0.79190154 0.12676345 0.78316711]\n",
            " [0.80270982 0.84220445 0.85745113 0.20714139 0.86524101]\n",
            " [0.87332891 0.85104618 0.8593898  0.18457295 0.83274593]\n",
            " [0.81134242 0.79386957 0.80595657 0.19786056 0.78137517]] -> [0.75915409]\n",
            "[[0.76278395 0.77984067 0.8163162  0.22005628 0.7958904 ]\n",
            " [0.79311786 0.77612711 0.80419983 0.21832311 0.7688309 ]\n",
            " [0.74336072 0.74977892 0.79190154 0.12676345 0.78316711]\n",
            " [0.80270982 0.84220445 0.85745113 0.20714139 0.86524101]\n",
            " [0.87332891 0.85104618 0.8593898  0.18457295 0.83274593]\n",
            " [0.81134242 0.79386957 0.80595657 0.19786056 0.78137517]\n",
            " [0.77932979 0.77329787 0.77675608 0.40313834 0.75915409]] -> [0.75927367]\n",
            "[[0.79311786 0.77612711 0.80419983 0.21832311 0.7688309 ]\n",
            " [0.74336072 0.74977892 0.79190154 0.12676345 0.78316711]\n",
            " [0.80270982 0.84220445 0.85745113 0.20714139 0.86524101]\n",
            " [0.87332891 0.85104618 0.8593898  0.18457295 0.83274593]\n",
            " [0.81134242 0.79386957 0.80595657 0.19786056 0.78137517]\n",
            " [0.77932979 0.77329787 0.77675608 0.40313834 0.75915409]\n",
            " [0.82339175 0.84403172 0.78941785 0.68653907 0.75927367]] -> [0.69458208]\n",
            "[[0.74336072 0.74977892 0.79190154 0.12676345 0.78316711]\n",
            " [0.80270982 0.84220445 0.85745113 0.20714139 0.86524101]\n",
            " [0.87332891 0.85104618 0.8593898  0.18457295 0.83274593]\n",
            " [0.81134242 0.79386957 0.80595657 0.19786056 0.78137517]\n",
            " [0.77932979 0.77329787 0.77675608 0.40313834 0.75915409]\n",
            " [0.82339175 0.84403172 0.78941785 0.68653907 0.75927367]\n",
            " [0.74617812 0.73421738 0.7313197  0.3431484  0.69458208]] -> [0.6890269]\n",
            "[[0.80270982 0.84220445 0.85745113 0.20714139 0.86524101]\n",
            " [0.87332891 0.85104618 0.8593898  0.18457295 0.83274593]\n",
            " [0.81134242 0.79386957 0.80595657 0.19786056 0.78137517]\n",
            " [0.77932979 0.77329787 0.77675608 0.40313834 0.75915409]\n",
            " [0.82339175 0.84403172 0.78941785 0.68653907 0.75927367]\n",
            " [0.74617812 0.73421738 0.7313197  0.3431484  0.69458208]\n",
            " [0.67076324 0.69077504 0.68006734 0.33925343 0.6890269 ]] -> [0.59996427]\n",
            "[[0.87332891 0.85104618 0.8593898  0.18457295 0.83274593]\n",
            " [0.81134242 0.79386957 0.80595657 0.19786056 0.78137517]\n",
            " [0.77932979 0.77329787 0.77675608 0.40313834 0.75915409]\n",
            " [0.82339175 0.84403172 0.78941785 0.68653907 0.75927367]\n",
            " [0.74617812 0.73421738 0.7313197  0.3431484  0.69458208]\n",
            " [0.67076324 0.69077504 0.68006734 0.33925343 0.6890269 ]\n",
            " [0.6426474  0.64456245 0.6068241  0.23973611 0.59996427]] -> [0.56071925]\n",
            "[[0.81134242 0.79386957 0.80595657 0.19786056 0.78137517]\n",
            " [0.77932979 0.77329787 0.77675608 0.40313834 0.75915409]\n",
            " [0.82339175 0.84403172 0.78941785 0.68653907 0.75927367]\n",
            " [0.74617812 0.73421738 0.7313197  0.3431484  0.69458208]\n",
            " [0.67076324 0.69077504 0.68006734 0.33925343 0.6890269 ]\n",
            " [0.6426474  0.64456245 0.6068241  0.23973611 0.59996427]\n",
            " [0.57700378 0.57559688 0.58004704 0.25293054 0.56071925]] -> [0.56006227]\n",
            "[[0.77932979 0.77329787 0.77675608 0.40313834 0.75915409]\n",
            " [0.82339175 0.84403172 0.78941785 0.68653907 0.75927367]\n",
            " [0.74617812 0.73421738 0.7313197  0.3431484  0.69458208]\n",
            " [0.67076324 0.69077504 0.68006734 0.33925343 0.6890269 ]\n",
            " [0.6426474  0.64456245 0.6068241  0.23973611 0.59996427]\n",
            " [0.57700378 0.57559688 0.58004704 0.25293054 0.56071925]\n",
            " [0.47754915 0.57801346 0.52873995 0.28863751 0.56006227]] -> [0.68251615]\n",
            "[[0.82339175 0.84403172 0.78941785 0.68653907 0.75927367]\n",
            " [0.74617812 0.73421738 0.7313197  0.3431484  0.69458208]\n",
            " [0.67076324 0.69077504 0.68006734 0.33925343 0.6890269 ]\n",
            " [0.6426474  0.64456245 0.6068241  0.23973611 0.59996427]\n",
            " [0.57700378 0.57559688 0.58004704 0.25293054 0.56071925]\n",
            " [0.47754915 0.57801346 0.52873995 0.28863751 0.56006227]\n",
            " [0.62046641 0.66542881 0.66195343 0.1861943  0.68251615]] -> [0.73024323]\n",
            "[[0.74617812 0.73421738 0.7313197  0.3431484  0.69458208]\n",
            " [0.67076324 0.69077504 0.68006734 0.33925343 0.6890269 ]\n",
            " [0.6426474  0.64456245 0.6068241  0.23973611 0.59996427]\n",
            " [0.57700378 0.57559688 0.58004704 0.25293054 0.56071925]\n",
            " [0.47754915 0.57801346 0.52873995 0.28863751 0.56006227]\n",
            " [0.62046641 0.66542881 0.66195343 0.1861943  0.68251615]\n",
            " [0.67382067 0.72698491 0.70823804 0.14252968 0.73024323]] -> [0.69918169]\n",
            "[[0.67076324 0.69077504 0.68006734 0.33925343 0.6890269 ]\n",
            " [0.6426474  0.64456245 0.6068241  0.23973611 0.59996427]\n",
            " [0.57700378 0.57559688 0.58004704 0.25293054 0.56071925]\n",
            " [0.47754915 0.57801346 0.52873995 0.28863751 0.56006227]\n",
            " [0.62046641 0.66542881 0.66195343 0.1861943  0.68251615]\n",
            " [0.67382067 0.72698491 0.70823804 0.14252968 0.73024323]\n",
            " [0.65307837 0.70101376 0.6566222  0.3758363  0.69918169]] -> [0.56328776]\n",
            "[[0.6426474  0.64456245 0.6068241  0.23973611 0.59996427]\n",
            " [0.57700378 0.57559688 0.58004704 0.25293054 0.56071925]\n",
            " [0.47754915 0.57801346 0.52873995 0.28863751 0.56006227]\n",
            " [0.62046641 0.66542881 0.66195343 0.1861943  0.68251615]\n",
            " [0.67382067 0.72698491 0.70823804 0.14252968 0.73024323]\n",
            " [0.65307837 0.70101376 0.6566222  0.3758363  0.69918169]\n",
            " [0.72040036 0.70067774 0.53854826 0.7741665  0.56328776]] -> [0.51227539]\n",
            "[[0.57700378 0.57559688 0.58004704 0.25293054 0.56071925]\n",
            " [0.47754915 0.57801346 0.52873995 0.28863751 0.56006227]\n",
            " [0.62046641 0.66542881 0.66195343 0.1861943  0.68251615]\n",
            " [0.67382067 0.72698491 0.70823804 0.14252968 0.73024323]\n",
            " [0.65307837 0.70101376 0.6566222  0.3758363  0.69918169]\n",
            " [0.72040036 0.70067774 0.53854826 0.7741665  0.56328776]\n",
            " [0.51279887 0.52154452 0.5276436  0.33694254 0.51227539]] -> [0.4051133]\n",
            "[[0.47754915 0.57801346 0.52873995 0.28863751 0.56006227]\n",
            " [0.62046641 0.66542881 0.66195343 0.1861943  0.68251615]\n",
            " [0.67382067 0.72698491 0.70823804 0.14252968 0.73024323]\n",
            " [0.65307837 0.70101376 0.6566222  0.3758363  0.69918169]\n",
            " [0.72040036 0.70067774 0.53854826 0.7741665  0.56328776]\n",
            " [0.51279887 0.52154452 0.5276436  0.33694254 0.51227539]\n",
            " [0.5071637  0.50427336 0.38927453 0.56760283 0.4051133 ]] -> [0.53897613]\n",
            "[[0.62046641 0.66542881 0.66195343 0.1861943  0.68251615]\n",
            " [0.67382067 0.72698491 0.70823804 0.14252968 0.73024323]\n",
            " [0.65307837 0.70101376 0.6566222  0.3758363  0.69918169]\n",
            " [0.72040036 0.70067774 0.53854826 0.7741665  0.56328776]\n",
            " [0.51279887 0.52154452 0.5276436  0.33694254 0.51227539]\n",
            " [0.5071637  0.50427336 0.38927453 0.56760283 0.4051133 ]\n",
            " [0.45542816 0.5429768  0.50698499 0.32710263 0.53897613]] -> [0.5747565]\n",
            "[[0.67382067 0.72698491 0.70823804 0.14252968 0.73024323]\n",
            " [0.65307837 0.70101376 0.6566222  0.3758363  0.69918169]\n",
            " [0.72040036 0.70067774 0.53854826 0.7741665  0.56328776]\n",
            " [0.51279887 0.52154452 0.5276436  0.33694254 0.51227539]\n",
            " [0.5071637  0.50427336 0.38927453 0.56760283 0.4051133 ]\n",
            " [0.45542816 0.5429768  0.50698499 0.32710263 0.53897613]\n",
            " [0.50476598 0.55443551 0.5284312  0.16384949 0.5747565 ]] -> [0.61507668]\n",
            "[[0.65307837 0.70101376 0.6566222  0.3758363  0.69918169]\n",
            " [0.72040036 0.70067774 0.53854826 0.7741665  0.56328776]\n",
            " [0.51279887 0.52154452 0.5276436  0.33694254 0.51227539]\n",
            " [0.5071637  0.50427336 0.38927453 0.56760283 0.4051133 ]\n",
            " [0.45542816 0.5429768  0.50698499 0.32710263 0.53897613]\n",
            " [0.50476598 0.55443551 0.5284312  0.16384949 0.5747565 ]\n",
            " [0.57502538 0.59180672 0.61154929 0.13063978 0.61507668]] -> [0.55122145]\n",
            "[[0.72040036 0.70067774 0.53854826 0.7741665  0.56328776]\n",
            " [0.51279887 0.52154452 0.5276436  0.33694254 0.51227539]\n",
            " [0.5071637  0.50427336 0.38927453 0.56760283 0.4051133 ]\n",
            " [0.45542816 0.5429768  0.50698499 0.32710263 0.53897613]\n",
            " [0.50476598 0.55443551 0.5284312  0.16384949 0.5747565 ]\n",
            " [0.57502538 0.59180672 0.61154929 0.13063978 0.61507668]\n",
            " [0.60170251 0.60536397 0.5859233  0.17521758 0.55122145]] -> [0.60295095]\n",
            "[[0.51279887 0.52154452 0.5276436  0.33694254 0.51227539]\n",
            " [0.5071637  0.50427336 0.38927453 0.56760283 0.4051133 ]\n",
            " [0.45542816 0.5429768  0.50698499 0.32710263 0.53897613]\n",
            " [0.50476598 0.55443551 0.5284312  0.16384949 0.5747565 ]\n",
            " [0.57502538 0.59180672 0.61154929 0.13063978 0.61507668]\n",
            " [0.60170251 0.60536397 0.5859233  0.17521758 0.55122145]\n",
            " [0.54918761 0.57412325 0.58955806 0.13734881 0.60295095]] -> [0.59739577]\n",
            "[[0.5071637  0.50427336 0.38927453 0.56760283 0.4051133 ]\n",
            " [0.45542816 0.5429768  0.50698499 0.32710263 0.53897613]\n",
            " [0.50476598 0.55443551 0.5284312  0.16384949 0.5747565 ]\n",
            " [0.57502538 0.59180672 0.61154929 0.13063978 0.61507668]\n",
            " [0.60170251 0.60536397 0.5859233  0.17521758 0.55122145]\n",
            " [0.54918761 0.57412325 0.58955806 0.13734881 0.60295095]\n",
            " [0.60925608 0.6169173  0.62833058 0.18744293 0.59739577]] -> [0.55390953]\n",
            "[[0.45542816 0.5429768  0.50698499 0.32710263 0.53897613]\n",
            " [0.50476598 0.55443551 0.5284312  0.16384949 0.5747565 ]\n",
            " [0.57502538 0.59180672 0.61154929 0.13063978 0.61507668]\n",
            " [0.60170251 0.60536397 0.5859233  0.17521758 0.55122145]\n",
            " [0.54918761 0.57412325 0.58955806 0.13734881 0.60295095]\n",
            " [0.60925608 0.6169173  0.62833058 0.18744293 0.59739577]\n",
            " [0.57988119 0.56577081 0.55714693 0.16612311 0.55390953]] -> [0.55803117]\n",
            "[[0.50476598 0.55443551 0.5284312  0.16384949 0.5747565 ]\n",
            " [0.57502538 0.59180672 0.61154929 0.13063978 0.61507668]\n",
            " [0.60170251 0.60536397 0.5859233  0.17521758 0.55122145]\n",
            " [0.54918761 0.57412325 0.58955806 0.13734881 0.60295095]\n",
            " [0.60925608 0.6169173  0.62833058 0.18744293 0.59739577]\n",
            " [0.57988119 0.56577081 0.55714693 0.16612311 0.55390953]\n",
            " [0.55907926 0.54641909 0.58907368 0.         0.55803117]] -> [0.59721639]\n",
            "[[0.57502538 0.59180672 0.61154929 0.13063978 0.61507668]\n",
            " [0.60170251 0.60536397 0.5859233  0.17521758 0.55122145]\n",
            " [0.54918761 0.57412325 0.58955806 0.13734881 0.60295095]\n",
            " [0.60925608 0.6169173  0.62833058 0.18744293 0.59739577]\n",
            " [0.57988119 0.56577081 0.55714693 0.16612311 0.55390953]\n",
            " [0.55907926 0.54641909 0.58907368 0.         0.55803117]\n",
            " [0.53354116 0.63206617 0.58471159 0.29560745 0.59721639]] -> [0.61274735]\n",
            "[[0.60170251 0.60536397 0.5859233  0.17521758 0.55122145]\n",
            " [0.54918761 0.57412325 0.58955806 0.13734881 0.60295095]\n",
            " [0.60925608 0.6169173  0.62833058 0.18744293 0.59739577]\n",
            " [0.57988119 0.56577081 0.55714693 0.16612311 0.55390953]\n",
            " [0.55907926 0.54641909 0.58907368 0.         0.55803117]\n",
            " [0.53354116 0.63206617 0.58471159 0.29560745 0.59721639]\n",
            " [0.60266189 0.62599468 0.63584267 0.18980972 0.61274735]] -> [0.53628805]\n",
            "[[0.54918761 0.57412325 0.58955806 0.13734881 0.60295095]\n",
            " [0.60925608 0.6169173  0.62833058 0.18744293 0.59739577]\n",
            " [0.57988119 0.56577081 0.55714693 0.16612311 0.55390953]\n",
            " [0.55907926 0.54641909 0.58907368 0.         0.55803117]\n",
            " [0.53354116 0.63206617 0.58471159 0.29560745 0.59721639]\n",
            " [0.60266189 0.62599468 0.63584267 0.18980972 0.61274735]\n",
            " [0.59390929 0.59351599 0.55460259 0.33168714 0.53628805]] -> [0.47583768]\n",
            "[[0.60925608 0.6169173  0.62833058 0.18744293 0.59739577]\n",
            " [0.57988119 0.56577081 0.55714693 0.16612311 0.55390953]\n",
            " [0.55907926 0.54641909 0.58907368 0.         0.55803117]\n",
            " [0.53354116 0.63206617 0.58471159 0.29560745 0.59721639]\n",
            " [0.60266189 0.62599468 0.63584267 0.18980972 0.61274735]\n",
            " [0.59390929 0.59351599 0.55460259 0.33168714 0.53628805]\n",
            " [0.51819437 0.51606234 0.44673668 0.44930021 0.47583768]] -> [0.49124905]\n",
            "[[0.57988119 0.56577081 0.55714693 0.16612311 0.55390953]\n",
            " [0.55907926 0.54641909 0.58907368 0.         0.55803117]\n",
            " [0.53354116 0.63206617 0.58471159 0.29560745 0.59721639]\n",
            " [0.60266189 0.62599468 0.63584267 0.18980972 0.61274735]\n",
            " [0.59390929 0.59351599 0.55460259 0.33168714 0.53628805]\n",
            " [0.51819437 0.51606234 0.44673668 0.44930021 0.47583768]\n",
            " [0.44116077 0.48157973 0.48353985 0.16077452 0.49124905]] -> [0.56304895]\n",
            "[[0.55907926 0.54641909 0.58907368 0.         0.55803117]\n",
            " [0.53354116 0.63206617 0.58471159 0.29560745 0.59721639]\n",
            " [0.60266189 0.62599468 0.63584267 0.18980972 0.61274735]\n",
            " [0.59390929 0.59351599 0.55460259 0.33168714 0.53628805]\n",
            " [0.51819437 0.51606234 0.44673668 0.44930021 0.47583768]\n",
            " [0.44116077 0.48157973 0.48353985 0.16077452 0.49124905]\n",
            " [0.5198131  0.5399353  0.54291034 0.1485119  0.56304895]] -> [0.54267959]\n",
            "[[0.53354116 0.63206617 0.58471159 0.29560745 0.59721639]\n",
            " [0.60266189 0.62599468 0.63584267 0.18980972 0.61274735]\n",
            " [0.59390929 0.59351599 0.55460259 0.33168714 0.53628805]\n",
            " [0.51819437 0.51606234 0.44673668 0.44930021 0.47583768]\n",
            " [0.44116077 0.48157973 0.48353985 0.16077452 0.49124905]\n",
            " [0.5198131  0.5399353  0.54291034 0.1485119  0.56304895]\n",
            " [0.56189666 0.56899508 0.56980869 0.19508377 0.54267959]] -> [0.61483787]\n",
            "[[0.60266189 0.62599468 0.63584267 0.18980972 0.61274735]\n",
            " [0.59390929 0.59351599 0.55460259 0.33168714 0.53628805]\n",
            " [0.51819437 0.51606234 0.44673668 0.44930021 0.47583768]\n",
            " [0.44116077 0.48157973 0.48353985 0.16077452 0.49124905]\n",
            " [0.5198131  0.5399353  0.54291034 0.1485119  0.56304895]\n",
            " [0.56189666 0.56899508 0.56980869 0.19508377 0.54267959]\n",
            " [0.53953601 0.58390795 0.56047886 0.21707449 0.61483787]] -> [0.64607843]\n",
            "[[0.59390929 0.59351599 0.55460259 0.33168714 0.53628805]\n",
            " [0.51819437 0.51606234 0.44673668 0.44930021 0.47583768]\n",
            " [0.44116077 0.48157973 0.48353985 0.16077452 0.49124905]\n",
            " [0.5198131  0.5399353  0.54291034 0.1485119  0.56304895]\n",
            " [0.56189666 0.56899508 0.56980869 0.19508377 0.54267959]\n",
            " [0.53953601 0.58390795 0.56047886 0.21707449 0.61483787]\n",
            " [0.6083567  0.6241084  0.62972384 0.16068134 0.64607843]] -> [0.72295554]\n",
            "[[0.51819437 0.51606234 0.44673668 0.44930021 0.47583768]\n",
            " [0.44116077 0.48157973 0.48353985 0.16077452 0.49124905]\n",
            " [0.5198131  0.5399353  0.54291034 0.1485119  0.56304895]\n",
            " [0.56189666 0.56899508 0.56980869 0.19508377 0.54267959]\n",
            " [0.53953601 0.58390795 0.56047886 0.21707449 0.61483787]\n",
            " [0.6083567  0.6241084  0.62972384 0.16068134 0.64607843]\n",
            " [0.65343805 0.69042139 0.70115598 0.22260944 0.72295554]] -> [0.72283633]\n",
            "[[0.44116077 0.48157973 0.48353985 0.16077452 0.49124905]\n",
            " [0.5198131  0.5399353  0.54291034 0.1485119  0.56304895]\n",
            " [0.56189666 0.56899508 0.56980869 0.19508377 0.54267959]\n",
            " [0.53953601 0.58390795 0.56047886 0.21707449 0.61483787]\n",
            " [0.6083567  0.6241084  0.62972384 0.16068134 0.64607843]\n",
            " [0.65343805 0.69042139 0.70115598 0.22260944 0.72295554]\n",
            " [0.68365194 0.7011494  0.73347024 0.28161166 0.72283633]] -> [0.76363411]\n",
            "[[0.5198131  0.5399353  0.54291034 0.1485119  0.56304895]\n",
            " [0.56189666 0.56899508 0.56980869 0.19508377 0.54267959]\n",
            " [0.53953601 0.58390795 0.56047886 0.21707449 0.61483787]\n",
            " [0.6083567  0.6241084  0.62972384 0.16068134 0.64607843]\n",
            " [0.65343805 0.69042139 0.70115598 0.22260944 0.72295554]\n",
            " [0.68365194 0.7011494  0.73347024 0.28161166 0.72283633]\n",
            " [0.73676653 0.77854405 0.7879033  0.28584208 0.76363411]] -> [0.76942846]\n",
            "[[0.56189666 0.56899508 0.56980869 0.19508377 0.54267959]\n",
            " [0.53953601 0.58390795 0.56047886 0.21707449 0.61483787]\n",
            " [0.6083567  0.6241084  0.62972384 0.16068134 0.64607843]\n",
            " [0.65343805 0.69042139 0.70115598 0.22260944 0.72295554]\n",
            " [0.68365194 0.7011494  0.73347024 0.28161166 0.72283633]\n",
            " [0.73676653 0.77854405 0.7879033  0.28584208 0.76363411]\n",
            " [0.75774849 0.77630412 0.79196218 0.20099145 0.76942846]] -> [0.77408749]\n",
            "[[0.53953601 0.58390795 0.56047886 0.21707449 0.61483787]\n",
            " [0.6083567  0.6241084  0.62972384 0.16068134 0.64607843]\n",
            " [0.65343805 0.69042139 0.70115598 0.22260944 0.72295554]\n",
            " [0.68365194 0.7011494  0.73347024 0.28161166 0.72283633]\n",
            " [0.73676653 0.77854405 0.7879033  0.28584208 0.76363411]\n",
            " [0.75774849 0.77630412 0.79196218 0.20099145 0.76942846]\n",
            " [0.75738882 0.77040963 0.7853586  0.19338788 0.77408749]] -> [0.73197538]\n",
            "[[0.6083567  0.6241084  0.62972384 0.16068134 0.64607843]\n",
            " [0.65343805 0.69042139 0.70115598 0.22260944 0.72295554]\n",
            " [0.68365194 0.7011494  0.73347024 0.28161166 0.72283633]\n",
            " [0.73676653 0.77854405 0.7879033  0.28584208 0.76363411]\n",
            " [0.75774849 0.77630412 0.79196218 0.20099145 0.76942846]\n",
            " [0.75738882 0.77040963 0.7853586  0.19338788 0.77408749]\n",
            " [0.77573302 0.75777195 0.76942555 0.34333476 0.73197538]] -> [0.75228494]\n",
            "[[0.65343805 0.69042139 0.70115598 0.22260944 0.72295554]\n",
            " [0.68365194 0.7011494  0.73347024 0.28161166 0.72283633]\n",
            " [0.73676653 0.77854405 0.7879033  0.28584208 0.76363411]\n",
            " [0.75774849 0.77630412 0.79196218 0.20099145 0.76942846]\n",
            " [0.75738882 0.77040963 0.7853586  0.19338788 0.77408749]\n",
            " [0.77573302 0.75777195 0.76942555 0.34333476 0.73197538]\n",
            " [0.71470519 0.7389329  0.7450719  0.11925306 0.75228494]] -> [0.76554563]\n",
            "[[0.68365194 0.7011494  0.73347024 0.28161166 0.72283633]\n",
            " [0.73676653 0.77854405 0.7879033  0.28584208 0.76363411]\n",
            " [0.75774849 0.77630412 0.79196218 0.20099145 0.76942846]\n",
            " [0.75738882 0.77040963 0.7853586  0.19338788 0.77408749]\n",
            " [0.77573302 0.75777195 0.76942555 0.34333476 0.73197538]\n",
            " [0.71470519 0.7389329  0.7450719  0.11925306 0.75228494]\n",
            " [0.7539117  0.74476875 0.78747919 0.06319536 0.76554563]] -> [0.75443527]\n",
            "[[0.73676653 0.77854405 0.7879033  0.28584208 0.76363411]\n",
            " [0.75774849 0.77630412 0.79196218 0.20099145 0.76942846]\n",
            " [0.75738882 0.77040963 0.7853586  0.19338788 0.77408749]\n",
            " [0.77573302 0.75777195 0.76942555 0.34333476 0.73197538]\n",
            " [0.71470519 0.7389329  0.7450719  0.11925306 0.75228494]\n",
            " [0.7539117  0.74476875 0.78747919 0.06319536 0.76554563]\n",
            " [0.74839655 0.73313303 0.75009993 0.11604763 0.75443527]] -> [0.73472325]\n",
            "[[0.75774849 0.77630412 0.79196218 0.20099145 0.76942846]\n",
            " [0.75738882 0.77040963 0.7853586  0.19338788 0.77408749]\n",
            " [0.77573302 0.75777195 0.76942555 0.34333476 0.73197538]\n",
            " [0.71470519 0.7389329  0.7450719  0.11925306 0.75228494]\n",
            " [0.7539117  0.74476875 0.78747919 0.06319536 0.76554563]\n",
            " [0.74839655 0.73313303 0.75009993 0.11604763 0.75443527]\n",
            " [0.72753424 0.71335103 0.75906629 0.07139529 0.73472325]] -> [0.726659]\n",
            "[[0.75738882 0.77040963 0.7853586  0.19338788 0.77408749]\n",
            " [0.77573302 0.75777195 0.76942555 0.34333476 0.73197538]\n",
            " [0.71470519 0.7389329  0.7450719  0.11925306 0.75228494]\n",
            " [0.7539117  0.74476875 0.78747919 0.06319536 0.76554563]\n",
            " [0.74839655 0.73313303 0.75009993 0.11604763 0.75443527]\n",
            " [0.72753424 0.71335103 0.75906629 0.07139529 0.73472325]\n",
            " [0.718782   0.70993212 0.75119072 0.00696994 0.726659  ]] -> [0.7364554]\n",
            "[[0.77573302 0.75777195 0.76942555 0.34333476 0.73197538]\n",
            " [0.71470519 0.7389329  0.7450719  0.11925306 0.75228494]\n",
            " [0.7539117  0.74476875 0.78747919 0.06319536 0.76554563]\n",
            " [0.74839655 0.73313303 0.75009993 0.11604763 0.75443527]\n",
            " [0.72753424 0.71335103 0.75906629 0.07139529 0.73472325]\n",
            " [0.718782   0.70993212 0.75119072 0.00696994 0.726659  ]\n",
            " [0.71746295 0.74011187 0.75347437 0.03785013 0.7364554 ]] -> [0.69762856]\n",
            "[[0.71470519 0.7389329  0.7450719  0.11925306 0.75228494]\n",
            " [0.7539117  0.74476875 0.78747919 0.06319536 0.76554563]\n",
            " [0.74839655 0.73313303 0.75009993 0.11604763 0.75443527]\n",
            " [0.72753424 0.71335103 0.75906629 0.07139529 0.73472325]\n",
            " [0.718782   0.70993212 0.75119072 0.00696994 0.726659  ]\n",
            " [0.71746295 0.74011187 0.75347437 0.03785013 0.7364554 ]\n",
            " [0.73556749 0.71871485 0.72647323 0.10188412 0.69762856]] -> [0.6841287]\n",
            "[[0.7539117  0.74476875 0.78747919 0.06319536 0.76554563]\n",
            " [0.74839655 0.73313303 0.75009993 0.11604763 0.75443527]\n",
            " [0.72753424 0.71335103 0.75906629 0.07139529 0.73472325]\n",
            " [0.718782   0.70993212 0.75119072 0.00696994 0.726659  ]\n",
            " [0.71746295 0.74011187 0.75347437 0.03785013 0.7364554 ]\n",
            " [0.73556749 0.71871485 0.72647323 0.10188412 0.69762856]\n",
            " [0.67340098 0.66979068 0.70054403 0.02910975 0.6841287 ]] -> [0.61860112]\n",
            "[[0.74839655 0.73313303 0.75009993 0.11604763 0.75443527]\n",
            " [0.72753424 0.71335103 0.75906629 0.07139529 0.73472325]\n",
            " [0.718782   0.70993212 0.75119072 0.00696994 0.726659  ]\n",
            " [0.71746295 0.74011187 0.75347437 0.03785013 0.7364554 ]\n",
            " [0.73556749 0.71871485 0.72647323 0.10188412 0.69762856]\n",
            " [0.67340098 0.66979068 0.70054403 0.02910975 0.6841287 ]\n",
            " [0.66992387 0.65122326 0.64898883 0.21882629 0.61860112]] -> [0.70413968]\n",
            "[[0.72753424 0.71335103 0.75906629 0.07139529 0.73472325]\n",
            " [0.718782   0.70993212 0.75119072 0.00696994 0.726659  ]\n",
            " [0.71746295 0.74011187 0.75347437 0.03785013 0.7364554 ]\n",
            " [0.73556749 0.71871485 0.72647323 0.10188412 0.69762856]\n",
            " [0.67340098 0.66979068 0.70054403 0.02910975 0.6841287 ]\n",
            " [0.66992387 0.65122326 0.64898883 0.21882629 0.61860112]\n",
            " [0.64630417 0.69160036 0.68164253 0.1970033  0.70413968]] -> [0.70867949]\n",
            "[[0.718782   0.70993212 0.75119072 0.00696994 0.726659  ]\n",
            " [0.71746295 0.74011187 0.75347437 0.03785013 0.7364554 ]\n",
            " [0.73556749 0.71871485 0.72647323 0.10188412 0.69762856]\n",
            " [0.67340098 0.66979068 0.70054403 0.02910975 0.6841287 ]\n",
            " [0.66992387 0.65122326 0.64898883 0.21882629 0.61860112]\n",
            " [0.64630417 0.69160036 0.68164253 0.1970033  0.70413968]\n",
            " [0.70355486 0.70168006 0.72623067 0.08934196 0.70867949]] -> [0.75120979]\n",
            "[[0.71746295 0.74011187 0.75347437 0.03785013 0.7364554 ]\n",
            " [0.73556749 0.71871485 0.72647323 0.10188412 0.69762856]\n",
            " [0.67340098 0.66979068 0.70054403 0.02910975 0.6841287 ]\n",
            " [0.66992387 0.65122326 0.64898883 0.21882629 0.61860112]\n",
            " [0.64630417 0.69160036 0.68164253 0.1970033  0.70413968]\n",
            " [0.70355486 0.70168006 0.72623067 0.08934196 0.70867949]\n",
            " [0.68988681 0.72018847 0.73749917 0.13593246 0.75120979]] -> [0.82366666]\n",
            "[[0.73556749 0.71871485 0.72647323 0.10188412 0.69762856]\n",
            " [0.67340098 0.66979068 0.70054403 0.02910975 0.6841287 ]\n",
            " [0.66992387 0.65122326 0.64898883 0.21882629 0.61860112]\n",
            " [0.64630417 0.69160036 0.68164253 0.1970033  0.70413968]\n",
            " [0.70355486 0.70168006 0.72623067 0.08934196 0.70867949]\n",
            " [0.68988681 0.72018847 0.73749917 0.13593246 0.75120979]\n",
            " [0.74491944 0.79929276 0.78102091 0.19279152 0.82366666]] -> [0.82665334]\n",
            "[[0.67340098 0.66979068 0.70054403 0.02910975 0.6841287 ]\n",
            " [0.66992387 0.65122326 0.64898883 0.21882629 0.61860112]\n",
            " [0.64630417 0.69160036 0.68164253 0.1970033  0.70413968]\n",
            " [0.70355486 0.70168006 0.72623067 0.08934196 0.70867949]\n",
            " [0.68988681 0.72018847 0.73749917 0.13593246 0.75120979]\n",
            " [0.74491944 0.79929276 0.78102091 0.19279152 0.82366666]\n",
            " [0.81170209 0.81147065 0.8453954  0.1279189  0.82665334]] -> [0.81554262]\n",
            "[[0.66992387 0.65122326 0.64898883 0.21882629 0.61860112]\n",
            " [0.64630417 0.69160036 0.68164253 0.1970033  0.70413968]\n",
            " [0.70355486 0.70168006 0.72623067 0.08934196 0.70867949]\n",
            " [0.68988681 0.72018847 0.73749917 0.13593246 0.75120979]\n",
            " [0.74491944 0.79929276 0.78102091 0.19279152 0.82366666]\n",
            " [0.81170209 0.81147065 0.8453954  0.1279189  0.82665334]\n",
            " [0.82045433 0.80654287 0.84951493 0.10963678 0.81554262]] -> [0.83417947]\n",
            "[[0.64630417 0.69160036 0.68164253 0.1970033  0.70413968]\n",
            " [0.70355486 0.70168006 0.72623067 0.08934196 0.70867949]\n",
            " [0.68988681 0.72018847 0.73749917 0.13593246 0.75120979]\n",
            " [0.74491944 0.79929276 0.78102091 0.19279152 0.82366666]\n",
            " [0.81170209 0.81147065 0.8453954  0.1279189  0.82665334]\n",
            " [0.82045433 0.80654287 0.84951493 0.10963678 0.81554262]\n",
            " [0.80330916 0.80076638 0.83655032 0.08794424 0.83417947]] -> [0.82492084]\n",
            "[[0.70355486 0.70168006 0.72623067 0.08934196 0.70867949]\n",
            " [0.68988681 0.72018847 0.73749917 0.13593246 0.75120979]\n",
            " [0.74491944 0.79929276 0.78102091 0.19279152 0.82366666]\n",
            " [0.81170209 0.81147065 0.8453954  0.1279189  0.82665334]\n",
            " [0.82045433 0.80654287 0.84951493 0.10963678 0.81554262]\n",
            " [0.80330916 0.80076638 0.83655032 0.08794424 0.83417947]\n",
            " [0.81613822 0.79628652 0.82322225 0.14256695 0.82492084]] -> [0.83400046]\n",
            "[[0.68988681 0.72018847 0.73749917 0.13593246 0.75120979]\n",
            " [0.74491944 0.79929276 0.78102091 0.19279152 0.82366666]\n",
            " [0.81170209 0.81147065 0.8453954  0.1279189  0.82665334]\n",
            " [0.82045433 0.80654287 0.84951493 0.10963678 0.81554262]\n",
            " [0.80330916 0.80076638 0.83655032 0.08794424 0.83417947]\n",
            " [0.81613822 0.79628652 0.82322225 0.14256695 0.82492084]\n",
            " [0.81817626 0.81888589 0.8687799  0.0939451  0.83400046]] -> [0.81446746]\n",
            "[[0.74491944 0.79929276 0.78102091 0.19279152 0.82366666]\n",
            " [0.81170209 0.81147065 0.8453954  0.1279189  0.82665334]\n",
            " [0.82045433 0.80654287 0.84951493 0.10963678 0.81554262]\n",
            " [0.80330916 0.80076638 0.83655032 0.08794424 0.83417947]\n",
            " [0.81613822 0.79628652 0.82322225 0.14256695 0.82492084]\n",
            " [0.81817626 0.81888589 0.8687799  0.0939451  0.83400046]\n",
            " [0.81577854 0.7948129  0.83049214 0.14346149 0.81446746]] -> [0.82318869]\n",
            "[[0.81170209 0.81147065 0.8453954  0.1279189  0.82665334]\n",
            " [0.82045433 0.80654287 0.84951493 0.10963678 0.81554262]\n",
            " [0.80330916 0.80076638 0.83655032 0.08794424 0.83417947]\n",
            " [0.81613822 0.79628652 0.82322225 0.14256695 0.82492084]\n",
            " [0.81817626 0.81888589 0.8687799  0.0939451  0.83400046]\n",
            " [0.81577854 0.7948129  0.83049214 0.14346149 0.81446746]\n",
            " [0.80816498 0.78930156 0.83424818 0.13181386 0.82318869]] -> [0.79992234]\n",
            "[[0.82045433 0.80654287 0.84951493 0.10963678 0.81554262]\n",
            " [0.80330916 0.80076638 0.83655032 0.08794424 0.83417947]\n",
            " [0.81613822 0.79628652 0.82322225 0.14256695 0.82492084]\n",
            " [0.81817626 0.81888589 0.8687799  0.0939451  0.83400046]\n",
            " [0.81577854 0.7948129  0.83049214 0.14346149 0.81446746]\n",
            " [0.80816498 0.78930156 0.83424818 0.13181386 0.82318869]\n",
            " [0.80402851 0.80860579 0.8391553  0.06075402 0.79992234]] -> [0.81691674]\n",
            "[[0.80330916 0.80076638 0.83655032 0.08794424 0.83417947]\n",
            " [0.81613822 0.79628652 0.82322225 0.14256695 0.82492084]\n",
            " [0.81817626 0.81888589 0.8687799  0.0939451  0.83400046]\n",
            " [0.81577854 0.7948129  0.83049214 0.14346149 0.81446746]\n",
            " [0.80816498 0.78930156 0.83424818 0.13181386 0.82318869]\n",
            " [0.80402851 0.80860579 0.8391553  0.06075402 0.79992234]\n",
            " [0.81475915 0.79345692 0.83848898 0.19735739 0.81691674]] -> [0.90227592]\n",
            "[[0.81613822 0.79628652 0.82322225 0.14256695 0.82492084]\n",
            " [0.81817626 0.81888589 0.8687799  0.0939451  0.83400046]\n",
            " [0.81577854 0.7948129  0.83049214 0.14346149 0.81446746]\n",
            " [0.80816498 0.78930156 0.83424818 0.13181386 0.82318869]\n",
            " [0.80402851 0.80860579 0.8391553  0.06075402 0.79992234]\n",
            " [0.81475915 0.79345692 0.83848898 0.19735739 0.81691674]\n",
            " [0.81679756 0.8757441  0.85090819 0.24517788 0.90227592]] -> [0.92951443]\n",
            "[[0.81817626 0.81888589 0.8687799  0.0939451  0.83400046]\n",
            " [0.81577854 0.7948129  0.83049214 0.14346149 0.81446746]\n",
            " [0.80816498 0.78930156 0.83424818 0.13181386 0.82318869]\n",
            " [0.80402851 0.80860579 0.8391553  0.06075402 0.79992234]\n",
            " [0.81475915 0.79345692 0.83848898 0.19735739 0.81691674]\n",
            " [0.81679756 0.8757441  0.85090819 0.24517788 0.90227592]\n",
            " [0.9070199  0.90539354 0.93621343 0.16306677 0.92951443]] -> [1.]\n",
            "[[0.81577854 0.7948129  0.83049214 0.14346149 0.81446746]\n",
            " [0.80816498 0.78930156 0.83424818 0.13181386 0.82318869]\n",
            " [0.80402851 0.80860579 0.8391553  0.06075402 0.79992234]\n",
            " [0.81475915 0.79345692 0.83848898 0.19735739 0.81691674]\n",
            " [0.81679756 0.8757441  0.85090819 0.24517788 0.90227592]\n",
            " [0.9070199  0.90539354 0.93621343 0.16306677 0.92951443]\n",
            " [0.9509022  0.96357211 0.98006871 0.1693099  1.        ]] -> [0.97897402]\n",
            "[[0.80816498 0.78930156 0.83424818 0.13181386 0.82318869]\n",
            " [0.80402851 0.80860579 0.8391553  0.06075402 0.79992234]\n",
            " [0.81475915 0.79345692 0.83848898 0.19735739 0.81691674]\n",
            " [0.81679756 0.8757441  0.85090819 0.24517788 0.90227592]\n",
            " [0.9070199  0.90539354 0.93621343 0.16306677 0.92951443]\n",
            " [0.9509022  0.96357211 0.98006871 0.1693099  1.        ]\n",
            " [1.         0.9767167  0.99188224 0.40038018 0.97897402]] -> [0.92616936]\n",
            "[[0.80402851 0.80860579 0.8391553  0.06075402 0.79992234]\n",
            " [0.81475915 0.79345692 0.83848898 0.19735739 0.81691674]\n",
            " [0.81679756 0.8757441  0.85090819 0.24517788 0.90227592]\n",
            " [0.9070199  0.90539354 0.93621343 0.16306677 0.92951443]\n",
            " [0.9509022  0.96357211 0.98006871 0.1693099  1.        ]\n",
            " [1.         0.9767167  0.99188224 0.40038018 0.97897402]\n",
            " [0.98141613 1.         0.95207992 0.44089528 0.92616936]] -> [0.80078859]\n",
            "[[0.81475915 0.79345692 0.83848898 0.19735739 0.81691674]\n",
            " [0.81679756 0.8757441  0.85090819 0.24517788 0.90227592]\n",
            " [0.9070199  0.90539354 0.93621343 0.16306677 0.92951443]\n",
            " [0.9509022  0.96357211 0.98006871 0.1693099  1.        ]\n",
            " [1.         0.9767167  0.99188224 0.40038018 0.97897402]\n",
            " [0.98141613 1.         0.95207992 0.44089528 0.92616936]\n",
            " [0.8612192  0.84609501 0.82703893 0.4925362  0.80078859]] -> [0.76775574]\n",
            "[[0.81679756 0.8757441  0.85090819 0.24517788 0.90227592]\n",
            " [0.9070199  0.90539354 0.93621343 0.16306677 0.92951443]\n",
            " [0.9509022  0.96357211 0.98006871 0.1693099  1.        ]\n",
            " [1.         0.9767167  0.99188224 0.40038018 0.97897402]\n",
            " [0.98141613 1.         0.95207992 0.44089528 0.92616936]\n",
            " [0.8612192  0.84609501 0.82703893 0.4925362  0.80078859]\n",
            " [0.75451104 0.76009427 0.77081918 0.29025886 0.76775574]] -> [0.76121509]\n",
            "[[0.9070199  0.90539354 0.93621343 0.16306677 0.92951443]\n",
            " [0.9509022  0.96357211 0.98006871 0.1693099  1.        ]\n",
            " [1.         0.9767167  0.99188224 0.40038018 0.97897402]\n",
            " [0.98141613 1.         0.95207992 0.44089528 0.92616936]\n",
            " [0.8612192  0.84609501 0.82703893 0.4925362  0.80078859]\n",
            " [0.75451104 0.76009427 0.77081918 0.29025886 0.76775574]\n",
            " [0.77141655 0.75974062 0.77487806 0.26785814 0.76121509]] -> [0.77814969]\n",
            "[[0.9509022  0.96357211 0.98006871 0.1693099  1.        ]\n",
            " [1.         0.9767167  0.99188224 0.40038018 0.97897402]\n",
            " [0.98141613 1.         0.95207992 0.44089528 0.92616936]\n",
            " [0.8612192  0.84609501 0.82703893 0.4925362  0.80078859]\n",
            " [0.75451104 0.76009427 0.77081918 0.29025886 0.76775574]\n",
            " [0.77141655 0.75974062 0.77487806 0.26785814 0.76121509]\n",
            " [0.73616683 0.76864135 0.77978517 0.17514303 0.77814969]] -> [0.7958306]\n",
            "[[1.         0.9767167  0.99188224 0.40038018 0.97897402]\n",
            " [0.98141613 1.         0.95207992 0.44089528 0.92616936]\n",
            " [0.8612192  0.84609501 0.82703893 0.4925362  0.80078859]\n",
            " [0.75451104 0.76009427 0.77081918 0.29025886 0.76775574]\n",
            " [0.77141655 0.75974062 0.77487806 0.26785814 0.76121509]\n",
            " [0.73616683 0.76864135 0.77978517 0.17514303 0.77814969]\n",
            " [0.79125947 0.78809309 0.83049214 0.16295496 0.7958306 ]] -> [0.79493482]\n",
            "[[0.98141613 1.         0.95207992 0.44089528 0.92616936]\n",
            " [0.8612192  0.84609501 0.82703893 0.4925362  0.80078859]\n",
            " [0.75451104 0.76009427 0.77081918 0.29025886 0.76775574]\n",
            " [0.77141655 0.75974062 0.77487806 0.26785814 0.76121509]\n",
            " [0.73616683 0.76864135 0.77978517 0.17514303 0.77814969]\n",
            " [0.79125947 0.78809309 0.83049214 0.16295496 0.7958306 ]\n",
            " [0.77153656 0.76256986 0.79947427 0.10961814 0.79493482]] -> [0.8285645]\n",
            "[[0.8612192  0.84609501 0.82703893 0.4925362  0.80078859]\n",
            " [0.75451104 0.76009427 0.77081918 0.29025886 0.76775574]\n",
            " [0.77141655 0.75974062 0.77487806 0.26785814 0.76121509]\n",
            " [0.73616683 0.76864135 0.77978517 0.17514303 0.77814969]\n",
            " [0.79125947 0.78809309 0.83049214 0.16295496 0.7958306 ]\n",
            " [0.77153656 0.76256986 0.79947427 0.10961814 0.79493482]\n",
            " [0.79725431 0.81461829 0.83903438 0.12098623 0.8285645 ]] -> [0.83698714]\n",
            "[[0.75451104 0.76009427 0.77081918 0.29025886 0.76775574]\n",
            " [0.77141655 0.75974062 0.77487806 0.26785814 0.76121509]\n",
            " [0.73616683 0.76864135 0.77978517 0.17514303 0.77814969]\n",
            " [0.79125947 0.78809309 0.83049214 0.16295496 0.7958306 ]\n",
            " [0.77153656 0.76256986 0.79947427 0.10961814 0.79493482]\n",
            " [0.79725431 0.81461829 0.83903438 0.12098623 0.8285645 ]\n",
            " [0.81529885 0.82251706 0.84757626 0.1060959  0.83698714]] -> [0.84403567]\n",
            "[[0.77141655 0.75974062 0.77487806 0.26785814 0.76121509]\n",
            " [0.73616683 0.76864135 0.77978517 0.17514303 0.77814969]\n",
            " [0.79125947 0.78809309 0.83049214 0.16295496 0.7958306 ]\n",
            " [0.77153656 0.76256986 0.79947427 0.10961814 0.79493482]\n",
            " [0.79725431 0.81461829 0.83903438 0.12098623 0.8285645 ]\n",
            " [0.81529885 0.82251706 0.84757626 0.1060959  0.83698714]\n",
            " [0.83034597 0.81556125 0.85575466 0.07523435 0.84403567]] -> [0.86858608]\n",
            "[[0.73616683 0.76864135 0.77978517 0.17514303 0.77814969]\n",
            " [0.79125947 0.78809309 0.83049214 0.16295496 0.7958306 ]\n",
            " [0.77153656 0.76256986 0.79947427 0.10961814 0.79493482]\n",
            " [0.79725431 0.81461829 0.83903438 0.12098623 0.8285645 ]\n",
            " [0.81529885 0.82251706 0.84757626 0.1060959  0.83698714]\n",
            " [0.83034597 0.81556125 0.85575466 0.07523435 0.84403567]\n",
            " [0.84347469 0.8426171  0.88749985 0.10121322 0.86858608]] -> [0.90185774]\n",
            "[[0.79125947 0.78809309 0.83049214 0.16295496 0.7958306 ]\n",
            " [0.77153656 0.76256986 0.79947427 0.10961814 0.79493482]\n",
            " [0.79725431 0.81461829 0.83903438 0.12098623 0.8285645 ]\n",
            " [0.81529885 0.82251706 0.84757626 0.1060959  0.83698714]\n",
            " [0.83034597 0.81556125 0.85575466 0.07523435 0.84403567]\n",
            " [0.84347469 0.8426171  0.88749985 0.10121322 0.86858608]\n",
            " [0.86925245 0.87626864 0.92209184 0.1140722  0.90185774]] -> [0.90908564]\n",
            "[[0.77153656 0.76256986 0.79947427 0.10961814 0.79493482]\n",
            " [0.79725431 0.81461829 0.83903438 0.12098623 0.8285645 ]\n",
            " [0.81529885 0.82251706 0.84757626 0.1060959  0.83698714]\n",
            " [0.83034597 0.81556125 0.85575466 0.07523435 0.84403567]\n",
            " [0.84347469 0.8426171  0.88749985 0.10121322 0.86858608]\n",
            " [0.86925245 0.87626864 0.92209184 0.1140722  0.90185774]\n",
            " [0.88723699 0.88829938 0.92518158 0.08714288 0.90908564]] -> [0.90030461]\n",
            "[[0.79725431 0.81461829 0.83903438 0.12098623 0.8285645 ]\n",
            " [0.81529885 0.82251706 0.84757626 0.1060959  0.83698714]\n",
            " [0.83034597 0.81556125 0.85575466 0.07523435 0.84403567]\n",
            " [0.84347469 0.8426171  0.88749985 0.10121322 0.86858608]\n",
            " [0.86925245 0.87626864 0.92209184 0.1140722  0.90185774]\n",
            " [0.88723699 0.88829938 0.92518158 0.08714288 0.90908564]\n",
            " [0.88939504 0.88829938 0.94014512 0.13380794 0.90030461]] -> [0.93124657]\n",
            "[[0.81529885 0.82251706 0.84757626 0.1060959  0.83698714]\n",
            " [0.83034597 0.81556125 0.85575466 0.07523435 0.84403567]\n",
            " [0.84347469 0.8426171  0.88749985 0.10121322 0.86858608]\n",
            " [0.86925245 0.87626864 0.92209184 0.1140722  0.90185774]\n",
            " [0.88723699 0.88829938 0.92518158 0.08714288 0.90908564]\n",
            " [0.88939504 0.88829938 0.94014512 0.13380794 0.90030461]\n",
            " [0.89281215 0.89655181 0.94323484 0.12965206 0.93124657]] -> [0.95460261]\n",
            "[[0.83034597 0.81556125 0.85575466 0.07523435 0.84403567]\n",
            " [0.84347469 0.8426171  0.88749985 0.10121322 0.86858608]\n",
            " [0.86925245 0.87626864 0.92209184 0.1140722  0.90185774]\n",
            " [0.88723699 0.88829938 0.92518158 0.08714288 0.90908564]\n",
            " [0.88939504 0.88829938 0.94014512 0.13380794 0.90030461]\n",
            " [0.89281215 0.89655181 0.94323484 0.12965206 0.93124657]\n",
            " [0.91133638 0.91818448 0.95944078 0.1885611  0.95460261]] -> [0.97604677]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URN9ejycaIwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, layers):\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = torch.nn.LSTM(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_dim, output_dim, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _status = self.rnn(x)\n",
        "        x = self.fc(x[:, -1])\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net(data_dim, hidden_dim, output_dim, 1)    # 1은 layer 수!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qWKfLwxaIgQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss & optimizer setting\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXb-xW-uagrt",
        "colab_type": "code",
        "outputId": "c09e8af9-3147-4009-95eb-7d77ffad974c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# start training\n",
        "for i in range(iterations):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(trainX_tensor)\n",
        "    loss = criterion(outputs, trainY_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(i, loss.item())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.38587284088134766\n",
            "1 0.32303759455680847\n",
            "2 0.26773858070373535\n",
            "3 0.21690312027931213\n",
            "4 0.1696799248456955\n",
            "5 0.12657985091209412\n",
            "6 0.08947892487049103\n",
            "7 0.061235327273607254\n",
            "8 0.04523169994354248\n",
            "9 0.044270940124988556\n",
            "10 0.055936116725206375\n",
            "11 0.06871342658996582\n",
            "12 0.07331983000040054\n",
            "13 0.06908948719501495\n",
            "14 0.05963829532265663\n",
            "15 0.04895836487412453\n",
            "16 0.03976350650191307\n",
            "17 0.03321840241551399\n",
            "18 0.02932601422071457\n",
            "19 0.027472171932458878\n",
            "20 0.026851527392864227\n",
            "21 0.026711570098996162\n",
            "22 0.026459092274308205\n",
            "23 0.025689803063869476\n",
            "24 0.024184392765164375\n",
            "25 0.02189532481133938\n",
            "26 0.018933309242129326\n",
            "27 0.015551356598734856\n",
            "28 0.012115015648305416\n",
            "29 0.009043709374964237\n",
            "30 0.0067159696482121944\n",
            "31 0.005354215390980244\n",
            "32 0.004931488074362278\n",
            "33 0.00515475869178772\n",
            "34 0.005560787860304117\n",
            "35 0.005704517010599375\n",
            "36 0.00534848915413022\n",
            "37 0.0045427000150084496\n",
            "38 0.0035567875020205975\n",
            "39 0.0027232172433286905\n",
            "40 0.0022845978382974863\n",
            "41 0.0023086871951818466\n",
            "42 0.002687606494873762\n",
            "43 0.003203348722308874\n",
            "44 0.0036245982628315687\n",
            "45 0.003795249853283167\n",
            "46 0.003682179609313607\n",
            "47 0.0033688745461404324\n",
            "48 0.0030044098384678364\n",
            "49 0.002733117202296853\n",
            "50 0.002634502248838544\n",
            "51 0.0026970880571752787\n",
            "52 0.002835379447788\n",
            "53 0.0029395068995654583\n",
            "54 0.0029310539830476046\n",
            "55 0.0027965304907411337\n",
            "56 0.002584523055702448\n",
            "57 0.0023733528796583414\n",
            "58 0.002229958539828658\n",
            "59 0.0021810245234519243\n",
            "60 0.0022077204193919897\n",
            "61 0.002262421417981386\n",
            "62 0.002295958809554577\n",
            "63 0.002280924702063203\n",
            "64 0.0022207293659448624\n",
            "65 0.0021424994338303804\n",
            "66 0.0020797906909137964\n",
            "67 0.0020548671018332243\n",
            "68 0.002069107722491026\n",
            "69 0.0021051352377980947\n",
            "70 0.0021381182596087456\n",
            "71 0.002149301115423441\n",
            "72 0.0021344416309148073\n",
            "73 0.0021034569945186377\n",
            "74 0.002072765724733472\n",
            "75 0.0020555199589580297\n",
            "76 0.0020552147179841995\n",
            "77 0.0020654876716434956\n",
            "78 0.0020751936826854944\n",
            "79 0.002075206721201539\n",
            "80 0.0020630452781915665\n",
            "81 0.002043193206191063\n",
            "82 0.0020236033014953136\n",
            "83 0.002010854659602046\n",
            "84 0.002006804570555687\n",
            "85 0.002008365234360099\n",
            "86 0.002010080497711897\n",
            "87 0.002007639268413186\n",
            "88 0.0020001688972115517\n",
            "89 0.0019901697523891926\n",
            "90 0.001981484703719616\n",
            "91 0.0019768006168305874\n",
            "92 0.0019762252923101187\n",
            "93 0.0019776129629462957\n",
            "94 0.0019781712908297777\n",
            "95 0.001976198283955455\n",
            "96 0.0019718867260962725\n",
            "97 0.0019668645691126585\n",
            "98 0.001962939975783229\n",
            "99 0.0019609436858445406\n",
            "100 0.001960362307727337\n",
            "101 0.001959874527528882\n",
            "102 0.0019583122339099646\n",
            "103 0.001955373678356409\n",
            "104 0.0019516770262271166\n",
            "105 0.0019482189090922475\n",
            "106 0.0019456773297861218\n",
            "107 0.0019440328469499946\n",
            "108 0.0019427072256803513\n",
            "109 0.0019410402746871114\n",
            "110 0.0019387432839721441\n",
            "111 0.001936030457727611\n",
            "112 0.0019333942327648401\n",
            "113 0.0019312282092869282\n",
            "114 0.0019295781385153532\n",
            "115 0.001928165671415627\n",
            "116 0.0019266317831352353\n",
            "117 0.0019247898599132895\n",
            "118 0.001922724535688758\n",
            "119 0.0019206799333915114\n",
            "120 0.0019188582664355636\n",
            "121 0.0019172802567481995\n",
            "122 0.0019157972419634461\n",
            "123 0.001914222608320415\n",
            "124 0.001912469044327736\n",
            "125 0.0019105938263237476\n",
            "126 0.0019087317632511258\n",
            "127 0.0019069849513471127\n",
            "128 0.0019053536234423518\n",
            "129 0.001903755241073668\n",
            "130 0.0019020992331206799\n",
            "131 0.0019003588240593672\n",
            "132 0.0018985789502039552\n",
            "133 0.0018968319054692984\n",
            "134 0.0018951589008793235\n",
            "135 0.001893543405458331\n",
            "136 0.0018919334979727864\n",
            "137 0.001890287734568119\n",
            "138 0.0018886050675064325\n",
            "139 0.0018869176274165511\n",
            "140 0.001885260222479701\n",
            "141 0.0018836420495063066\n",
            "142 0.001882043550722301\n",
            "143 0.0018804353894665837\n",
            "144 0.0018788052257150412\n",
            "145 0.001877162721939385\n",
            "146 0.0018755276687443256\n",
            "147 0.001873913686722517\n",
            "148 0.001872315420769155\n",
            "149 0.0018707184353843331\n",
            "150 0.0018691110890358686\n",
            "151 0.0018674938473850489\n",
            "152 0.0018658768385648727\n",
            "153 0.0018642711220309138\n",
            "154 0.0018626770470291376\n",
            "155 0.0018610884435474873\n",
            "156 0.001859497744590044\n",
            "157 0.0018579029710963368\n",
            "158 0.0018563090125098825\n",
            "159 0.0018547215731814504\n",
            "160 0.0018531419336795807\n",
            "161 0.0018515678821131587\n",
            "162 0.0018499937141314149\n",
            "163 0.0018484176835045218\n",
            "164 0.0018468420021235943\n",
            "165 0.0018452698132023215\n",
            "166 0.0018437030958011746\n",
            "167 0.0018421403365209699\n",
            "168 0.0018405785085633397\n",
            "169 0.0018390167970210314\n",
            "170 0.0018374569481238723\n",
            "171 0.0018358994275331497\n",
            "172 0.0018343463307246566\n",
            "173 0.0018327961442992091\n",
            "174 0.0018312478205189109\n",
            "175 0.001829700660891831\n",
            "176 0.0018281551310792565\n",
            "177 0.0018266125116497278\n",
            "178 0.001825073384679854\n",
            "179 0.0018235370516777039\n",
            "180 0.001822003279812634\n",
            "181 0.0018204712541773915\n",
            "182 0.0018189416732639074\n",
            "183 0.0018174151191487908\n",
            "184 0.0018158918246626854\n",
            "185 0.0018143713241443038\n",
            "186 0.0018128528026863933\n",
            "187 0.0018113373080268502\n",
            "188 0.0018098235595971346\n",
            "189 0.0018083134200423956\n",
            "190 0.0018068059580400586\n",
            "191 0.0018053011735901237\n",
            "192 0.0018037991831079125\n",
            "193 0.0018022999865934253\n",
            "194 0.00180080346763134\n",
            "195 0.0017993097426369786\n",
            "196 0.001797819510102272\n",
            "197 0.001796331605874002\n",
            "198 0.0017948464956134558\n",
            "199 0.0017933641793206334\n",
            "200 0.0017918848898261786\n",
            "201 0.0017904082778841257\n",
            "202 0.0017889348091557622\n",
            "203 0.0017874641343951225\n",
            "204 0.0017859966028481722\n",
            "205 0.0017845313996076584\n",
            "206 0.0017830695724114776\n",
            "207 0.0017816107720136642\n",
            "208 0.0017801545327529311\n",
            "209 0.0017787012038752437\n",
            "210 0.0017772512510418892\n",
            "211 0.0017758036265149713\n",
            "212 0.0017743590287864208\n",
            "213 0.0017729171086102724\n",
            "214 0.0017714789137244225\n",
            "215 0.0017700428143143654\n",
            "216 0.0017686102073639631\n",
            "217 0.0017671800451353192\n",
            "218 0.0017657532589510083\n",
            "219 0.0017643290339037776\n",
            "220 0.0017629078356549144\n",
            "221 0.0017614897806197405\n",
            "222 0.0017600742867216468\n",
            "223 0.0017586618196219206\n",
            "224 0.00175725226290524\n",
            "225 0.0017558455001562834\n",
            "226 0.0017544417642056942\n",
            "227 0.0017530409386381507\n",
            "228 0.0017516430234536529\n",
            "229 0.001750247785821557\n",
            "230 0.0017488556914031506\n",
            "231 0.0017474661581218243\n",
            "232 0.0017460797680541873\n",
            "233 0.0017446959391236305\n",
            "234 0.0017433149041607976\n",
            "235 0.0017419372452422976\n",
            "236 0.001740562147460878\n",
            "237 0.0017391897272318602\n",
            "238 0.001737820217385888\n",
            "239 0.0017364532686769962\n",
            "240 0.0017350894631817937\n",
            "241 0.0017337285680696368\n",
            "242 0.0017323700012639165\n",
            "243 0.0017310143448412418\n",
            "244 0.0017296615988016129\n",
            "245 0.001728311413899064\n",
            "246 0.0017269644886255264\n",
            "247 0.0017256196588277817\n",
            "248 0.0017242779722437263\n",
            "249 0.0017229387303814292\n",
            "250 0.0017216021660715342\n",
            "251 0.0017202687449753284\n",
            "252 0.001718937768600881\n",
            "253 0.0017176094697788358\n",
            "254 0.0017162837320938706\n",
            "255 0.0017149611376225948\n",
            "256 0.0017136408714577556\n",
            "257 0.0017123230500146747\n",
            "258 0.0017110082553699613\n",
            "259 0.0017096959054470062\n",
            "260 0.0017083864659070969\n",
            "261 0.001707079354673624\n",
            "262 0.0017057751538231969\n",
            "263 0.0017044731648638844\n",
            "264 0.0017031742027029395\n",
            "265 0.001701877685263753\n",
            "266 0.0017005839617922902\n",
            "267 0.0016992923337966204\n",
            "268 0.0016980034997686744\n",
            "269 0.001696717576123774\n",
            "270 0.0016954339807853103\n",
            "271 0.0016941528301686049\n",
            "272 0.001692874589934945\n",
            "273 0.0016915984451770782\n",
            "274 0.001690325210802257\n",
            "275 0.001689054537564516\n",
            "276 0.0016877860762178898\n",
            "277 0.0016865207580849528\n",
            "278 0.001685257419012487\n",
            "279 0.001683996757492423\n",
            "280 0.0016827387735247612\n",
            "281 0.0016814832342788577\n",
            "282 0.0016802300233393908\n",
            "283 0.0016789796063676476\n",
            "284 0.0016777314012870193\n",
            "285 0.0016764859901741147\n",
            "286 0.0016752430237829685\n",
            "287 0.0016740025021135807\n",
            "288 0.0016727644251659513\n",
            "289 0.0016715290257707238\n",
            "290 0.001670295838266611\n",
            "291 0.0016690650954842567\n",
            "292 0.0016678371466696262\n",
            "293 0.0016666115261614323\n",
            "294 0.0016653882339596748\n",
            "295 0.001664167968556285\n",
            "296 0.0016629497986286879\n",
            "297 0.0016617339570075274\n",
            "298 0.0016605209093540907\n",
            "299 0.0016593100735917687\n",
            "300 0.0016581019153818488\n",
            "301 0.0016568959690630436\n",
            "302 0.0016556928167119622\n",
            "303 0.0016544917598366737\n",
            "304 0.0016532937297597528\n",
            "305 0.001652097562327981\n",
            "306 0.0016509040724486113\n",
            "307 0.0016497132601216435\n",
            "308 0.0016485246596857905\n",
            "309 0.001647338503971696\n",
            "310 0.001646155142225325\n",
            "311 0.0016449737595394254\n",
            "312 0.0016437954036518931\n",
            "313 0.0016426191432401538\n",
            "314 0.001641445211134851\n",
            "315 0.0016402739565819502\n",
            "316 0.0016391052631661296\n",
            "317 0.0016379386652261019\n",
            "318 0.0016367747448384762\n",
            "319 0.0016356133855879307\n",
            "320 0.0016344542382284999\n",
            "321 0.0016332977684214711\n",
            "322 0.0016321439761668444\n",
            "323 0.0016309921629726887\n",
            "324 0.001629843027330935\n",
            "325 0.0016286963364109397\n",
            "326 0.0016275522066280246\n",
            "327 0.001626410405151546\n",
            "328 0.0016252710483968258\n",
            "329 0.0016241343691945076\n",
            "330 0.0016229997854679823\n",
            "331 0.001621867879293859\n",
            "332 0.001620738534256816\n",
            "333 0.001619611750356853\n",
            "334 0.001618487061932683\n",
            "335 0.0016173652838915586\n",
            "336 0.0016162453684955835\n",
            "337 0.001615128479897976\n",
            "338 0.0016140141524374485\n",
            "339 0.0016129016876220703\n",
            "340 0.001611792016774416\n",
            "341 0.0016106849070638418\n",
            "342 0.0016095801256597042\n",
            "343 0.0016084779053926468\n",
            "344 0.0016073781298473477\n",
            "345 0.0016062806826084852\n",
            "346 0.0016051859129220247\n",
            "347 0.0016040935879573226\n",
            "348 0.0016030034748837352\n",
            "349 0.0016019161557778716\n",
            "350 0.0016008311649784446\n",
            "351 0.0015997487353160977\n",
            "352 0.0015986685175448656\n",
            "353 0.001597591326572001\n",
            "354 0.001596516347490251\n",
            "355 0.0015954435802996159\n",
            "356 0.0015943734906613827\n",
            "357 0.0015933056129142642\n",
            "358 0.0015922406455501914\n",
            "359 0.0015911780064925551\n",
            "360 0.0015901176957413554\n",
            "361 0.0015890601789578795\n",
            "362 0.0015880046412348747\n",
            "363 0.0015869520138949156\n",
            "364 0.0015859019476920366\n",
            "365 0.0015848539769649506\n",
            "366 0.0015838084509596229\n",
            "367 0.0015827656025066972\n",
            "368 0.0015817253151908517\n",
            "369 0.0015806873561814427\n",
            "370 0.001579651958309114\n",
            "371 0.0015786187723279\n",
            "372 0.0015775883803144097\n",
            "373 0.001576560316607356\n",
            "374 0.0015755348140373826\n",
            "375 0.0015745116397738457\n",
            "376 0.001573490910232067\n",
            "377 0.001572472625412047\n",
            "378 0.001571456901729107\n",
            "379 0.001570443739183247\n",
            "380 0.0015694330213591456\n",
            "381 0.001568424515426159\n",
            "382 0.0015674184542149305\n",
            "383 0.0015664150705561042\n",
            "384 0.0015654138987883925\n",
            "385 0.001564415404573083\n",
            "386 0.0015634192386642098\n",
            "387 0.001562425633892417\n",
            "388 0.0015614344738423824\n",
            "389 0.0015604457585141063\n",
            "390 0.0015594590222463012\n",
            "391 0.0015584751963615417\n",
            "392 0.0015574938151985407\n",
            "393 0.0015565147623419762\n",
            "394 0.0015555380377918482\n",
            "395 0.0015545637579634786\n",
            "396 0.0015535920392721891\n",
            "397 0.0015526226488873363\n",
            "398 0.0015516557032242417\n",
            "399 0.0015506912022829056\n",
            "400 0.001549729029648006\n",
            "401 0.0015487691853195429\n",
            "402 0.0015478117857128382\n",
            "403 0.0015468569472432137\n",
            "404 0.0015459044370800257\n",
            "405 0.001544954371638596\n",
            "406 0.0015440064016729593\n",
            "407 0.0015430609928444028\n",
            "408 0.0015421181451529264\n",
            "409 0.0015411775093525648\n",
            "410 0.0015402392018586397\n",
            "411 0.0015393034555017948\n",
            "412 0.0015383698046207428\n",
            "413 0.0015374383656308055\n",
            "414 0.001536509720608592\n",
            "415 0.0015355832874774933\n",
            "416 0.001534659182652831\n",
            "417 0.0015337372897192836\n",
            "418 0.0015328179579228163\n",
            "419 0.0015319010708481073\n",
            "420 0.0015309862792491913\n",
            "421 0.0015300735831260681\n",
            "422 0.0015291634481400251\n",
            "423 0.0015282557578757405\n",
            "424 0.0015273503959178925\n",
            "425 0.0015264471294358373\n",
            "426 0.0015255463076755404\n",
            "427 0.0015246475813910365\n",
            "428 0.001523751299828291\n",
            "429 0.00152285723015666\n",
            "430 0.0015219654887914658\n",
            "431 0.0015210758429020643\n",
            "432 0.001520188758149743\n",
            "433 0.0015193040017038584\n",
            "434 0.001518421107903123\n",
            "435 0.0015175406588241458\n",
            "436 0.0015166625380516052\n",
            "437 0.0015157863963395357\n",
            "438 0.0015149128157645464\n",
            "439 0.0015140412142500281\n",
            "440 0.001513172290287912\n",
            "441 0.0015123049961403012\n",
            "442 0.001511440030299127\n",
            "443 0.0015105776255950332\n",
            "444 0.0015097170835360885\n",
            "445 0.0015088588697835803\n",
            "446 0.0015080026350915432\n",
            "447 0.0015071488451212645\n",
            "448 0.0015062972670421004\n",
            "449 0.0015054475516080856\n",
            "450 0.0015046001644805074\n",
            "451 0.0015037552220746875\n",
            "452 0.0015029119094833732\n",
            "453 0.0015020710416138172\n",
            "454 0.0015012322692200541\n",
            "455 0.001500395592302084\n",
            "456 0.0014995611272752285\n",
            "457 0.0014987288741394877\n",
            "458 0.0014978984836488962\n",
            "459 0.0014970701886340976\n",
            "460 0.0014962442219257355\n",
            "461 0.0014954200014472008\n",
            "462 0.0014945982256904244\n",
            "463 0.0014937783125787973\n",
            "464 0.0014929603785276413\n",
            "465 0.0014921447727829218\n",
            "466 0.0014913312625139952\n",
            "467 0.0014905193820595741\n",
            "468 0.0014897097134962678\n",
            "469 0.0014889026060700417\n",
            "470 0.001488097128458321\n",
            "471 0.0014872936299070716\n",
            "472 0.0014864921104162931\n",
            "473 0.0014856929192319512\n",
            "474 0.001484895241446793\n",
            "475 0.0014841000083833933\n",
            "476 0.001483306405134499\n",
            "477 0.0014825150137767196\n",
            "478 0.0014817256014794111\n",
            "479 0.001480938051827252\n",
            "480 0.0014801525976508856\n",
            "481 0.0014793691225349903\n",
            "482 0.0014785873936489224\n",
            "483 0.0014778078766539693\n",
            "484 0.0014770302223041654\n",
            "485 0.0014762544305995107\n",
            "486 0.0014754807343706489\n",
            "487 0.0014747083187103271\n",
            "488 0.001473938231356442\n",
            "489 0.0014731704723089933\n",
            "490 0.0014724041102454066\n",
            "491 0.0014716396108269691\n",
            "492 0.0014708772068843246\n",
            "493 0.0014701164327561855\n",
            "494 0.001469357986934483\n",
            "495 0.0014686008216813207\n",
            "496 0.0014678461011499166\n",
            "497 0.0014670928940176964\n",
            "498 0.0014663416659459472\n",
            "499 0.0014655921841040254\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L09EanmVagVz",
        "colab_type": "code",
        "outputId": "4aa57216-0e50-47a5-9b8e-9867f45992cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.plot(testY)\n",
        "plt.plot(net(testX_tensor).data.numpy())\n",
        "plt.legend(['original', 'prediction'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nK3CvgAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8rz2Fzme3LX",
        "colab_type": "text"
      },
      "source": [
        "[another example](https://stackabuse.com/time-series-prediction-using-lstm-with-pytorch-in-python/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bjj-tSibglOI",
        "colab_type": "text"
      },
      "source": [
        "[Bitcoin example](https://towardsdatascience.com/lstm-for-time-series-prediction-de8aeb26f2ca)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQJZdFO4niOq",
        "colab_type": "text"
      },
      "source": [
        "### PackedSequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r17WoALunmqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_sequence, pack_padded_sequence, pad_packed_sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wjBPk2anopH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Random word from random word generator\n",
        "data = ['hello world',\n",
        "        'midnight',\n",
        "        'calculation',\n",
        "        'path',\n",
        "        'short circuit']\n",
        "\n",
        "# Make dictionary\n",
        "char_set = ['<pad>'] + list(set(char for seq in data for char in seq)) # Get all characters and include pad token\n",
        "char2idx = {char: idx for idx, char in enumerate(char_set)} # Constuct character to index dictionary\n",
        "print('char_set:', char_set)\n",
        "print('char_set length:', len(char_set))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4q-Jm50nol0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert character to index and make list of tensors\n",
        "X = [torch.LongTensor([char2idx[char] for char in seq]) for seq in data]\n",
        "\n",
        "# Check converted result\n",
        "for sequence in X:\n",
        "    print(sequence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEnCUkQFnoiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make length tensor (will be used later in 'pack_padded_sequence' function)\n",
        "lengths = [len(seq) for seq in X]\n",
        "print('lengths:', lengths)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBKBpL6lnofv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a Tensor of shape (Batch x Maximum_Sequence_Length)\n",
        "padded_sequence = pad_sequence(X, batch_first=True) # X is now padded sequence\n",
        "print(padded_sequence)\n",
        "print(padded_sequence.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Qhcfx2PoGWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq5hvlYUnn-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sort by descending lengths\n",
        "sorted_idx = sorted(range(len(lengths)), key=lengths.__getitem__, reverse=True)\n",
        "sorted_X = [X[idx] for idx in sorted_idx]\n",
        "\n",
        "# Check converted result\n",
        "for sequence in sorted_X:\n",
        "    print(sequence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWSHEABRnmaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "packed_sequence = pack_sequence(sorted_X)\n",
        "print(packed_sequence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CMlQAUYnmXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one-hot embedding using PaddedSequence\n",
        "eye = torch.eye(len(char_set)) # Identity matrix of shape (len(char_set), len(char_set))\n",
        "embedded_tensor = eye[padded_sequence]\n",
        "print(embedded_tensor.shape) # shape: (Batch_size, max_sequence_length, number_of_input_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHTwTeP7nmUS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one-hot embedding using PackedSequence\n",
        "embedded_packed_seq = pack_sequence([eye[X[idx]] for idx in sorted_idx])\n",
        "print(embedded_packed_seq.data.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXqGjRPIn5Yh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# declare RNN\n",
        "rnn = torch.nn.RNN(input_size=len(char_set), hidden_size=30, batch_first=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2dV93Wen5XA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_output, hidden = rnn(embedded_tensor)\n",
        "print(rnn_output.shape) # shape: (batch_size, max_seq_length, hidden_size)\n",
        "print(hidden.shape)     # shape: (num_layers * num_directions, batch_size, hidden_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO_Ur6Gfn5VQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_output, hidden = rnn(embedded_packed_seq)\n",
        "print(rnn_output.data.shape)\n",
        "print(hidden.data.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYN3eGRdn5TW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unpacked_sequence, seq_lengths = pad_packed_sequence(embedded_packed_seq, batch_first=True)\n",
        "print(unpacked_sequence.shape)\n",
        "print(seq_lengths)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvw1QUFSn5Rl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedded_padded_sequence = eye[pad_sequence(sorted_X, batch_first=True)]\n",
        "print(embedded_padded_sequence.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvUQH85Vn5Pr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sorted_lengths = sorted(lengths, reverse=True)\n",
        "new_packed_sequence = pack_padded_sequence(embedded_padded_sequence, sorted_lengths, batch_first=True)\n",
        "print(new_packed_sequence.data.shape)\n",
        "print(new_packed_sequence.batch_sizes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3eRh91BgIAZ",
        "colab_type": "text"
      },
      "source": [
        "## Seq2seq (Chat bot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rj8iMO1htAf",
        "colab_type": "text"
      },
      "source": [
        "### About Seq2seq\n",
        "- Seq2se1은 sequence의 마지막까지 보고 처리하여야 하는 챗봇에 사용하기 적합한 모델\n",
        "- Encoder와 Decoder의 구조로 됨\n",
        "\n",
        "> 방식\n",
        "- 입력된 sequence를 vector의 형태로 압축을 한다. 그 뒤 vector를 뒤에 decoder로 전달해준다.\n",
        "- Decoder는 받은 vector를를 첫 cell의 hidden state로 넣어주고 \n",
        "- 이 문장이 시작한다는 start flag와 함께 모델을 시작\n",
        "- Cell에서 나온 output을 답변의 첫 단어로 두고, 단어가 2번째 cell로 들어가서 이전 cell의 hidden state와 함께 다음 단어를 예측\n",
        "- Decoder에서 이 과정을 반복하여 하나의 문장을 생성하는 것"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ0DKmYqgOI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_NrvTvagPXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(0)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eciR6O7jgPAx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw = [\"I feel hungry.\t나는 배가 고프다.\",\n",
        "       \"Pytorch is very easy.\t파이토치는 매우 쉽다.\",\n",
        "       \"Pytorch is a framework for deep learning.\t파이토치는 딥러닝을 위한 프레임워크이다.\",\n",
        "       \"Pytorch is very clear to use.\t파이토치는 사용하기 매우 직관적이다.\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEQCss7wgO70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fix token for \"start of sentence\" and \"end of sentence\"\n",
        "SOS_token = 0\n",
        "EOS_token = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzYR_PZkgO3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class for vocabulary related information of data\n",
        "class Vocab:\n",
        "    def __init__(self):\n",
        "        self.vocab2index = {\"<SOS>\": SOS_token, \"<EOS>\": EOS_token}\n",
        "        self.index2vocab = {SOS_token: \"<SOS>\", EOS_token: \"<EOS>\"}\n",
        "        self.vocab_count = {}\n",
        "        self.n_vocab = len(self.vocab2index)\n",
        "\n",
        "    def add_vocab(self, sentence):\n",
        "        for word in sentence.split(\" \"):\n",
        "            if word not in self.vocab2index:\n",
        "                self.vocab2index[word] = self.n_vocab\n",
        "                self.vocab_count[word] = 1\n",
        "                self.index2vocab[self.n_vocab] = word\n",
        "                self.n_vocab += 1\n",
        "            else:\n",
        "                self.vocab_count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YwjcTOMgO0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filter out the long sentence from source and target data\n",
        "def filter_pair(pair, source_max_length, target_max_length):\n",
        "    return len(pair[0].split(\" \")) < source_max_length and len(pair[1].split(\" \")) < target_max_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YVGwrW7gOx6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read and preprocess the corpus data\n",
        "def preprocess(corpus, source_max_length, target_max_length):\n",
        "    print(\"reading corpus...\")\n",
        "    pairs = []\n",
        "    for line in corpus:\n",
        "        pairs.append([s for s in line.strip().lower().split(\"\\t\")])    # 탭으로 split\n",
        "    print(\"Read {} sentence pairs\".format(len(pairs)))\n",
        "\n",
        "    pairs = [pair for pair in pairs if filter_pair(pair, source_max_length, target_max_length)]\n",
        "    print(\"Trimmed to {} sentence pairs\".format(len(pairs)))\n",
        "\n",
        "    source_vocab = Vocab()\n",
        "    target_vocab = Vocab()\n",
        "\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        source_vocab.add_vocab(pair[0])\n",
        "        target_vocab.add_vocab(pair[1])\n",
        "    print(\"source vocab size =\", source_vocab.n_vocab)\n",
        "    print(\"target vocab size =\", target_vocab.n_vocab)\n",
        "\n",
        "    return pairs, source_vocab, target_vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HejPP_YgOwy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# declare simple encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)  # embedding은 큰 matrix라 생각!\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embedding(x).view(1, 1, -1)\n",
        "        x, hidden = self.gru(x, hidden)\n",
        "        return x, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfEHaZkxgOuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# declare simple decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embedding(x).view(1, 1, -1)\n",
        "        x, hidden = self.gru(x, hidden)\n",
        "        x = self.softmax(self.out(x[0]))\n",
        "        return x, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwK-j8OjgOgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert sentence to the index tensor\n",
        "def tensorize(vocab, sentence):\n",
        "    indexes = [vocab.vocab2index[word] for word in sentence.split(\" \")]\n",
        "    indexes.append(vocab.vocab2index[\"<EOS>\"])\n",
        "    return torch.Tensor(indexes).long().to(device).view(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Digs-CSgofO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training seq2seq\n",
        "def train(pairs, source_vocab, target_vocab, encoder, decoder, n_iter, print_every=1000, learning_rate=0.01):\n",
        "    loss_total = 0\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    training_batch = [random.choice(pairs) for _ in range(n_iter)]\n",
        "    training_source = [tensorize(source_vocab, pair[0]) for pair in training_batch]\n",
        "    training_target = [tensorize(target_vocab, pair[1]) for pair in training_batch]\n",
        "\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for i in range(1, n_iter + 1):\n",
        "        source_tensor = training_source[i - 1]\n",
        "        target_tensor = training_target[i - 1]\n",
        "\n",
        "        encoder_hidden = torch.zeros([1, 1, encoder.hidden_size]).to(device)\n",
        "\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        source_length = source_tensor.size(0)\n",
        "        target_length = target_tensor.size(0)\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        for enc_input in range(source_length):\n",
        "            _, encoder_hidden = encoder(source_tensor[enc_input], encoder_hidden)\n",
        "\n",
        "        decoder_input = torch.Tensor([[SOS_token]]).long().to(device)\n",
        "        decoder_hidden = encoder_hidden # connect encoder output to decoder input\n",
        "\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # teacher forcing\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "        loss_iter = loss.item() / target_length\n",
        "        loss_total += loss_iter\n",
        "\n",
        "        if i % print_every == 0:\n",
        "            loss_avg = loss_total / print_every\n",
        "            loss_total = 0\n",
        "            print(\"[{} - {}%] loss = {:05.4f}\".format(i, i / n_iter * 100, loss_avg))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc5495Sxgorn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# insert given sentence to check the training\n",
        "def evaluate(pairs, source_vocab, target_vocab, encoder, decoder, target_max_length):\n",
        "    for pair in pairs:\n",
        "        print(\">\", pair[0])\n",
        "        print(\"=\", pair[1])\n",
        "        source_tensor = tensorize(source_vocab, pair[0])\n",
        "        source_length = source_tensor.size()[0]\n",
        "        encoder_hidden = torch.zeros([1, 1, encoder.hidden_size]).to(device)\n",
        "\n",
        "        for ei in range(source_length):\n",
        "            _, encoder_hidden = encoder(source_tensor[ei], encoder_hidden)\n",
        "\n",
        "        decoder_input = torch.Tensor([[SOS_token]], device=device).long()\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(target_max_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            _, top_index = decoder_output.data.topk(1)\n",
        "            if top_index.item() == EOS_token:\n",
        "                decoded_words.append(\"<EOS>\")\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(target_vocab.index2vocab[top_index.item()])\n",
        "\n",
        "            decoder_input = top_index.squeeze().detach()\n",
        "\n",
        "        predict_words = decoded_words\n",
        "        predict_sentence = \" \".join(predict_words)\n",
        "        print(\"<\", predict_sentence)\n",
        "        print(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5FWjgougodM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# declare max length for sentence\n",
        "SOURCE_MAX_LENGTH = 10\n",
        "TARGET_MAX_LENGTH = 12"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYuOya3IgobZ",
        "colab_type": "code",
        "outputId": "e033833c-0111-4ad8-a9c5-2bcda00cf391",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "# preprocess the corpus\n",
        "load_pairs, load_source_vocab, load_target_vocab = preprocess(raw, SOURCE_MAX_LENGTH, TARGET_MAX_LENGTH)\n",
        "print(random.choice(load_pairs))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading corpus...\n",
            "Read 4 sentence pairs\n",
            "Trimmed to 4 sentence pairs\n",
            "Counting words...\n",
            "source vocab size = 17\n",
            "target vocab size = 13\n",
            "['pytorch is very easy.', '파이토치는 매우 쉽다.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC3en-BsgoFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# declare the encoder and the decoder\n",
        "enc_hidden_size = 16\n",
        "dec_hidden_size = enc_hidden_size\n",
        "enc = Encoder(load_source_vocab.n_vocab, enc_hidden_size).to(device)\n",
        "dec = Decoder(dec_hidden_size, load_target_vocab.n_vocab).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VAbizc4gwD3",
        "colab_type": "code",
        "outputId": "bc24b322-92bb-4222-9e95-8064d7b8a1e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# train seq2seq model\n",
        "train(load_pairs, load_source_vocab, load_target_vocab, enc, dec, 5000, print_every=1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1000 - 20.0%] loss = 0.7390\n",
            "[2000 - 40.0%] loss = 0.1076\n",
            "[3000 - 60.0%] loss = 0.0341\n",
            "[4000 - 80.0%] loss = 0.0184\n",
            "[5000 - 100.0%] loss = 0.0126\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zob5CbpBgwa-",
        "colab_type": "code",
        "outputId": "cc12f2c4-0959-4469-dc8d-6b1df2b180ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "source": [
        "# check the model with given data\n",
        "evaluate(load_pairs, load_source_vocab, load_target_vocab, enc, dec, TARGET_MAX_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> i feel hungry.\n",
            "= 나는 배가 고프다.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-96-ee57b2130249>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_source_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_target_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTARGET_MAX_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-69-95857238d354>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(pairs, source_vocab, target_vocab, encoder, decoder, target_max_length)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mei\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSOS_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mdecoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdecoded_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: legacy constructor for device type: cpu was passed device type: cuda, but device type must be: cpu"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO2oZH7Twwsu",
        "colab_type": "text"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUgVRKiOwyJD",
        "colab_type": "text"
      },
      "source": [
        "- One-hot vector의 의미적 연산과 확장성의 한계(inner product이 항상 0이 나오기에 단어, 문장 간의 의미적 차이나 유사도를 구하는 것이 불가능)를 극복하기 위한 방법\n",
        "- 간단하게 말하면 단어, 알파벳 같은 기본 단위 요소들을 일정한 길이를 가지는 vector 공간에 투영하는 것\n",
        "- 단어를 벡터화하는 것이 word2vec\n",
        "- 대표적으로 CBOW, skip-gram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tyDWij89PWj",
        "colab_type": "text"
      },
      "source": [
        "## LSTM(Long Short-Term Memeory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csEoex679ZhH",
        "colab_type": "text"
      },
      "source": [
        "### About LSTM\n",
        "- 기존의 RNN에 장기기억을 담당하는 부분(cell state)을 추가한 것\n",
        "- LSTM은 cell state를 추가함으로써 sequence가 길더라도 gradient가 잘 전달되어 long-term dependency 문제가 해결되어 긴 sequence 데이터에 적합\n",
        "![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
        "> 하나하나 뜯어보자!\n",
        "- Cell state\n",
        "  - 장기기억을 담당하는 부분\n",
        "  - 곱하기(X) 부분: 기존 정보를 얼마나 남길 것인지에 따라 비중을 곱하는 부분\n",
        "  - 더하기(+) 부분: 현재 들어온 데이터와 기존의 은닉 상태를 통해 정보를 추가하는 부분\n",
        "![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png)\n",
        "- Forget gate\n",
        "  - 기존의 정보들로 구성된 cell state의 값을 얼마나 잊어버릴 것인지 정하는 부분\n",
        "  - 현 시점의 입력값 + 직전 hidden state 값을 받는 neural network이라 보면 됨!\n",
        "![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png)\n",
        "- Input gate\n",
        "  - 어떤 정보를 얼만큼 cell state에 새롭게 저장할지 정하는 부분\n",
        "  - tanh값: -1~1로 cell state에 새롭게 추가할 정보\n",
        "  - sigmoid값: 0~1로 새롭게 추가할 정보를 얼마큼의 비중으로 cell state에 더할지 정함\n",
        "![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png)\n",
        "\n",
        "- Updating Cell state\n",
        "  - 여기까지 정보는 현재 시점의 input과 직전 시점의 hidden state 값의 조합으로 기존 cell state의 정보를 얼마큼 전달하지 정하고, 어떤 정보를 얼마큼의 비중으로 더할지 정하는 것\n",
        "![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png)\n",
        "\n",
        "- Updating Hidden state\n",
        "![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Q_1BKW792xq",
        "colab_type": "text"
      },
      "source": [
        "## GRU(Gated Recurrent Unit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8Tb6DZ396JN",
        "colab_type": "text"
      },
      "source": [
        "### About GRU\n",
        "- LSTM을 보다 단순화환 RNN의 변형모델\n",
        "![](https://t1.daumcdn.net/cfile/tistory/9982923F5ACB86A10E)\n",
        "- Cell state와 hidden state를 분리하지 않고 hidden state 하나로 합침.\n",
        "\n",
        "> 첫 번째 수식\n",
        "  - Update gate. 현재 시점의 input과 직전 시점의 hidden state에 가중치를 곱하고 sigmoid function을 통과해 업데이트할 비중을 결정\n",
        "\n",
        "> 두 번째 수식\n",
        "- Reset gate. update gate와 같은 input을 받아서 sigmoid로 비중을 정하고, 이 비중은 세 번째 수식에서 ht를 구할 때 기존 hidden state 값을 얼마큼 반영할지 정하는 데 사용\n",
        "\n",
        "> 세 번째 수식\n",
        "- 기존 hidden state에 가중치가 곱해진 값 & input을 입력 받아 가중치를 곱한 후 tanh 함수를 통과해 새로운 정보의 값을 리턴\n",
        "\n",
        "> 마지막 수식\n",
        "- 새로운 hidden state를 구하는 부분. 첫 번째 수식에서 구한 가중치로 기존 hidden state와 ht의 가중 합을 곱해서 구함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7WiUoh7cYG6",
        "colab_type": "text"
      },
      "source": [
        "# 9. Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCgxftJTca6M",
        "colab_type": "text"
      },
      "source": [
        "## Transfer Learning란? (전이학습이란?)\n",
        "- 특정 조건에서 얻어진 어떤 지식을 다른 상황에 맞게 그대로 '전이'해서 활용하는 학습 방법\n",
        "> 장점\n",
        "  - 데이터 부족을 어느 정도 해결가능\n",
        "  - 학습에 걸리는 시간이 줄어듬\n",
        "  - 시뮬레이션에서 학습된 모델을 현실에 적용할 수 있게 함\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvcNk-e3d3PP",
        "colab_type": "text"
      },
      "source": [
        "### 실제 모델 불러와서 사용하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmCG4h5zca_P",
        "colab_type": "code",
        "outputId": "877843df-709b-4319-ff94-a4254265735c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import torchvision.models as models\n",
        "resnet = models.resnet50(pretrained=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 173MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LP8D71dld7CB",
        "colab_type": "code",
        "outputId": "9b942475-9bb9-4fb6-d7dc-f11343f8c527",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "for name, modul in resnet.named_children():\n",
        "  print(name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv1\n",
            "bn1\n",
            "relu\n",
            "maxpool\n",
            "layer1\n",
            "layer2\n",
            "layer3\n",
            "layer4\n",
            "avgpool\n",
            "fc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNVb089VeEAG",
        "colab_type": "text"
      },
      "source": [
        "위의 결과는 layer4까지는 이미지 데이터에서 특성을 추출하고 이를 avgpool에서 평균을 낸 후, Fully connected layer를 통과시켜 분류를 완료함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4i8Gp5BeNg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''미리 학습된 부분과 새롭게 붙일 분류기를 만드는 코드'''\n",
        "class Resnet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Resnet, self).__init__()\n",
        "    self.layer0 = nn.Sequential(*list(resnet.childeren())[0:-1])   \n",
        "    '''\n",
        "    .children(): 직속 자식 node들을 모듈만 불러옴. 이 모듈 안에는 학습된 변수들이 있음\n",
        "    return 값이 generator이기에 이를 list로 바꾸고,\n",
        "    이 중에서 fc 층은 사용하지 않기에 [0:-1]해서 fc를 빼줌\n",
        "    *를 사용해서 unpacking\n",
        "    '''\n",
        "    self.layer1 = nn.Sequential(\n",
        "    '''\n",
        "    self.layer1은 avgpool 층을 통과하고 나온 tensor를 입력값으로 받아 구분하고자 category 개수대로 분류되게 됨.\n",
        "    '''\n",
        "        nn.Linear(2048,500),\n",
        "        nn.BatchNorm1d(500),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(500, num_category),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.layer0(x)\n",
        "    out = out.view(batch_size, -1)\n",
        "    '''\n",
        "    out.view(batch_size, -1)이 하는 일: 앞의 연산을 통과하고 나온 4차원 tensor를 nn.Linear 함수에 맞게 2차원으로 변환해주는 것\n",
        "    '''\n",
        "    out = self.layer1(out)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn6UkRX7gCNT",
        "colab_type": "text"
      },
      "source": [
        "학습의 대상이 되는 변수의 범위 설정하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQrDEvOsgGbU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for params in model.layer0.parameters():\n",
        "  params.require_grad = False\n",
        "'''\n",
        "이 코드가 하는 일: layer0에 있는 변수들을 require_grad를 False로 바꿔줌\n",
        "즉 해당 변수에 대한 기울기를 계산하지 않아 업데이트가 이루어지지 않게 하는 것\n",
        "'''\n",
        "\n",
        "for parms in model.layer1.parameters():\n",
        "  parms.requires_grad = True\n",
        "'''\n",
        "학습된 모델에서 변수를 가져와서 새로운 모델을 만들고\n",
        "학습 범위까지 지정하면 이제 여태까지 해왔던 대로 data를 넣고\n",
        "optimizer를 통해 모델을 학습하면 됨\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUvS_mYYAsPV",
        "colab_type": "text"
      },
      "source": [
        "## 코드 살펴봅시다~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKNp0vmHAmNZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 필요한 이미지들을 다운받습니다.\n",
        "\n",
        "!rm -r images\n",
        "import os \n",
        "\n",
        "try:\n",
        "  os.mkdir(\"images\")\n",
        "  os.mkdir(\"images/content\")\n",
        "  os.mkdir(\"images/style\")\n",
        "except:\n",
        "  pass\n",
        "\n",
        "!wget https://upload.wikimedia.org/wikipedia/commons/0/00/Tuebingen_Neckarfront.jpg -P images/content\n",
        "!wget https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/1280px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg -P images/style"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjfprPR5Bap7",
        "colab_type": "text"
      },
      "source": [
        "#### 1. Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nx17u18SBxr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 1) Import required libraries \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils as utils\n",
        "import torch.utils.data as data\n",
        "import torchvision.models as models\n",
        "import torchvision.utils as v_utils\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "## 2) Hyperparameter\n",
        "# 컨텐츠 손실을 어느 지점에서 맞출것인지 지정해놓습니다.\n",
        "content_layer_num = 1\n",
        "image_size = 512\n",
        "epoch = 5000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3s_DOX1BzCy",
        "colab_type": "text"
      },
      "source": [
        "#### 2. Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuLLmT1aB5Iz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 1) Directory\n",
        "content_dir = \"./images/content/Tuebingen_Neckarfront.jpg\"\n",
        "style_dir = \"./images/style/1280px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg\"\n",
        "\n",
        "## 2) Prepocessing Function\n",
        "- 전처리 함수\n",
        "# 이미 학습된 ResNet 모델이 이미지넷으로 학습된 모델이기 때문에 이에 따라 정규화해줍니다.\n",
        "\n",
        "def image_preprocess(img_dir):\n",
        "    img = Image.open(img_dir)\n",
        "    transform = transforms.Compose([\n",
        "                    transforms.Resize(image_size),\n",
        "                    transforms.CenterCrop(image_size),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(mean=[0.40760392, 0.45795686, 0.48501961], \n",
        "                                         std=[1,1,1]),\n",
        "                ])\n",
        "    img = transform(img).view((-1,3,image_size,image_size))\n",
        "    return img\n",
        "\n",
        "## 3) Post processing Function - 후처리 함수\n",
        "# 정규화 된 상태로 연산을 진행하고 다시 이미지화 해서 보기위해 뺐던 값들을 다시 더해줍니다.\n",
        "# 또한 이미지가 0에서 1사이의 값을 가지게 해줍니다.\n",
        "\n",
        "def image_postprocess(tensor):\n",
        "    transform = transforms.Normalize(mean=[-0.40760392, -0.45795686, -0.48501961], \n",
        "                                     std=[1,1,1])\n",
        "    img = transform(tensor.clone())\n",
        "    img = img.clamp(0,1)\n",
        "    img = torch.transpose(img,0,1)\n",
        "    img = torch.transpose(img,1,2)\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc3Y3xkgB6d-",
        "colab_type": "text"
      },
      "source": [
        "#### 3. Model & Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR0unVsaAmg_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 1) Resnet\n",
        "# 미리 학습된 resnet50를 사용합니다.\n",
        "resnet = models.resnet50(pretrained=True)\n",
        "for name,module in resnet.named_children():\n",
        "    print(name)\n",
        "\n",
        "## 2) Delete Fully Connected Layer\n",
        "# 레이어마다 결과값을 가져올 수 있게 forward를 정의합니다.\n",
        "\n",
        "class Resnet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Resnet,self).__init__()\n",
        "        self.layer0 = nn.Sequential(*list(resnet.children())[0:1])\n",
        "        self.layer1 = nn.Sequential(*list(resnet.children())[1:4])\n",
        "        self.layer2 = nn.Sequential(*list(resnet.children())[4:5])\n",
        "        self.layer3 = nn.Sequential(*list(resnet.children())[5:6])\n",
        "        self.layer4 = nn.Sequential(*list(resnet.children())[6:7])\n",
        "        self.layer5 = nn.Sequential(*list(resnet.children())[7:8])\n",
        "\n",
        "    def forward(self,x):\n",
        "        out_0 = self.layer0(x)\n",
        "        out_1 = self.layer1(out_0)\n",
        "        out_2 = self.layer2(out_1)\n",
        "        out_3 = self.layer3(out_2)\n",
        "        out_4 = self.layer4(out_3)\n",
        "        out_5 = self.layer5(out_4)\n",
        "        return out_0, out_1, out_2, out_3, out_4, out_5\n",
        "\n",
        "## 3) Gram Matrix Function\n",
        "# 그람 행렬을 생성하는 클래스 및 함수를 정의합니다. \n",
        "# [batch,channel,height,width] -> [b,c,h*w]\n",
        "# [b,c,h*w] x [b,h*w,c] = [b,c,c]\n",
        "\n",
        "class GramMatrix(nn.Module):\n",
        "    def forward(self, input):\n",
        "        b,c,h,w = input.size()\n",
        "        F = input.view(b, c, h*w)\n",
        "        G = torch.bmm(F, F.transpose(1,2)) \n",
        "        return G\n",
        "\n",
        "## 4) Model on GPU\n",
        "# 모델을 학습의 대상이 아니기 때문에 requires_grad를 False로 설정합니다.\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "resnet = Resnet().to(device)\n",
        "for param in resnet.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "## 5) Gram Matrix Loss\n",
        "# 그람행렬간의 손실을 계산하는 클래스 및 함수를 정의합니다.\n",
        "\n",
        "class GramMSELoss(nn.Module):\n",
        "    def forward(self, input, target):\n",
        "        out = nn.MSELoss()(GramMatrix()(input), target)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv0xwzeZCAp-",
        "colab_type": "text"
      },
      "source": [
        "#### 4. Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZYlIjMeAm92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 1) Prepare Images\n",
        "\n",
        "# 컨텐츠 이미지, 스타일 이미지, 학습의 대상이 되는 이미지를 정의합니다.\n",
        "\n",
        "content = image_preprocess(content_dir).to(device)\n",
        "style = image_preprocess(style_dir).to(device)\n",
        "generated = content.clone().requires_grad_().to(device)\n",
        "\n",
        "print(content.requires_grad,style.requires_grad,generated.requires_grad)\n",
        "\n",
        "# 각각을 시각화 합니다.\n",
        "\n",
        "plt.imshow(image_postprocess(content[0].cpu()))\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(image_postprocess(style[0].cpu()))\n",
        "plt.show()\n",
        "\n",
        "gen_img = image_postprocess(generated[0].cpu()).data.numpy()\n",
        "plt.imshow(gen_img)\n",
        "plt.show()\n",
        "\n",
        "## 2) Set Targets & Style Weights\n",
        "# 목표값을 설정하고 행렬의 크기에 따른 가중치도 함께 정의해놓습니다\n",
        "\n",
        "style_target = list(GramMatrix().to(device)(i) for i in resnet(style))\n",
        "content_target = resnet(content)[content_layer_num]\n",
        "style_weight = [1/n**2 for n in [64,64,256,512,1024,2048]]\n",
        "\n",
        "## 3) Train\n",
        "\n",
        "# LBFGS 최적화 함수를 사용합니다.\n",
        "# 이때 학습의 대상은 모델의 가중치가 아닌 이미지 자체입니다.\n",
        "# for more info about LBFGS -> http://pytorch.org/docs/optim.html?highlight=lbfgs#torch.optim.LBFGS\n",
        "\n",
        "optimizer = optim.LBFGS([generated])\n",
        "\n",
        "iteration = [0]\n",
        "while iteration[0] < epoch:\n",
        "    def closure():\n",
        "        optimizer.zero_grad()\n",
        "        out = resnet(generated)\n",
        "        \n",
        "        # 스타일 손실을 각각의 목표값에 따라 계산하고 이를 리스트로 저장합니다.\n",
        "        style_loss = [GramMSELoss().to(device)(out[i],style_target[i])*style_weight[i] for i in range(len(style_target))]\n",
        "        \n",
        "        # 컨텐츠 손실은 지정한 위치에서만 계산되므로 하나의 수치로 저장됩니다.\n",
        "        content_loss = nn.MSELoss().to(device)(out[content_layer_num],content_target)\n",
        "        \n",
        "        # 스타일:컨텐츠 = 1000:1의 비중으로 총 손실을 계산합니다.\n",
        "        total_loss = 1000 * sum(style_loss) + torch.sum(content_loss)\n",
        "        total_loss.backward()\n",
        "\n",
        "        if iteration[0] % 100 == 0:\n",
        "            print(total_loss)\n",
        "        iteration[0] += 1\n",
        "        return total_loss\n",
        "\n",
        "    optimizer.step(closure)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BzIGgdcCOnX",
        "colab_type": "text"
      },
      "source": [
        "#### 5. Check Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywtgjxo-Am8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 학습된 결과 이미지를 확인합니다.\n",
        "\n",
        "gen_img = image_postprocess(generated[0].cpu()).data.numpy()\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(gen_img)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUwJK1wC9nos",
        "colab_type": "text"
      },
      "source": [
        "## Style Transfer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylGlQ4ks9wPz",
        "colab_type": "text"
      },
      "source": [
        "### Image Style Transfer\n",
        "\n",
        "![alt text](https://bloglunit.files.wordpress.com/2017/04/e18489e185b3e1848fe185b3e18485e185b5e186abe18489e185a3e186ba-2017-05-16-e1848be185a9e18492e185ae-1-50-07.png)\n",
        "\n",
        "- A Neural Algorithm of Artistic Style (https://arxiv.org/abs/1508.06576)\n",
        "- Pretrained ResNet50\n",
        "- Reference below\n",
        "- https://discuss.pytorch.org/t/how-to-extract-features-of-an-image-from-a-trained-model/119/3\n",
        "- https://github.com/leongatys/PytorchNeuralStyleTransfer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E16QxFTBUrv_",
        "colab_type": "text"
      },
      "source": [
        "# 10. GAN (Generative Adversial Network)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BHj1QXRxRFq",
        "colab_type": "text"
      },
      "source": [
        "## GAN이란?\n",
        "![대체 텍스트](http://www.codingwoman.com/wp-content/uploads/2018/09/gan-2-700x529.jpg)  \n",
        "- GAN은 가짜 이미지를 생성하는 비지도학습 모델\n",
        "- 텍스트를 받아서 이미지를 생성하는 text-to-iamge synthesis 작업에 사용되기도 함\n",
        "- 이 외에도 모델의 학습을 안정화하는 방향, 이미지로 된 텍스트 같은 여러 종류의 데이터를 활용하는 방향, 더 사실적이고 고화질의 영상을 생성하는 방향 등 다양한 분야로 GAN이 활용되고 있음!\n",
        "> 구분자 학습 부분\n",
        " - 구분자에 가짜 데이터를 넣어주면 구분자의 입장에서는 가짜라고 구분해야 하기에 0 라벨을 사용해서 손실을 계산\n",
        " - 구분자에 진짜 데이터를 넣어주면 구분자의 입장에서는 진짜라고 구분해야 하기에 1 라벨을 사용해서 손실을 계산\n",
        " - 이렇게 계산한 두 손실을 더해서 최종적인 구분자 손실을 구하고 이를 통해 모델을 업데이트\n",
        "\n",
        " > 생성자 학습 부분\n",
        "  - 가짜 데이터를 진짜로 구분되는 것이 생성자의 목적이므로 1 라벨을 이용해 손실을 계산하고 모델을 업데이트\n",
        "\n",
        "- Layer에 Linear 함수 대신 Convolution 연산으로 바꾸고 모델의 layerㄹㄹ 늘리면 이미지 데이터에 대해 더 좋은 결과를 낼 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdeSFL6NxROu",
        "colab_type": "text"
      },
      "source": [
        "## 코드 살펴봅시다~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4Pv7-37oUER",
        "colab_type": "text"
      },
      "source": [
        "### 1. import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CEORYeUoULS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 단순한 GAN 모델 생성 및 OrderedDict 사용법\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils as utils\n",
        "import torch.nn.init as init\n",
        "import torchvision.utils as v_utils\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import OrderedDict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD2eiQL_oUTl",
        "colab_type": "text"
      },
      "source": [
        "### 참고"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYvvh2kSoaiL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 참고\n",
        "# 전치 컨볼루션 연산으로 이미지 크기를 2배로 늘리는 방법 2가지\n",
        "# 둘중에 kernel_size=4,stride=2,padding=1 세팅이 체커보드 아티팩트가 덜합니다.\n",
        "\n",
        "test = torch.ones(1,1,16,16)\n",
        "conv1 = nn.ConvTranspose2d(1,1,kernel_size=4,stride=2,padding=1)\n",
        "out = conv1(test)\n",
        "print(out.size())\n",
        "\n",
        "conv1 = nn.ConvTranspose2d(1,1,kernel_size=3,stride=2,padding=1,output_padding=1)\n",
        "out = conv1(test)\n",
        "print(out.size())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnxEcKpSoUYf",
        "colab_type": "text"
      },
      "source": [
        "### 2. Hyperparameter setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwAriTt5obPp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set Hyperparameters\n",
        "# change num_gpu to the number of gpus you want to use\n",
        "\n",
        "epoch = 50\n",
        "batch_size = 512\n",
        "learning_rate = 0.0002\n",
        "num_gpus = 1\n",
        "z_size = 50\n",
        "middle_size = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5l55H8LTobWu",
        "colab_type": "text"
      },
      "source": [
        "### 3. Data Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0GRHENoobbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download Data\n",
        "\n",
        "mnist_train = dset.MNIST(\"./\", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\n",
        "\n",
        "# Set Data Loader(input pipeline)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,batch_size=batch_size,shuffle=True,drop_last=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2soOkVZ8obf7",
        "colab_type": "text"
      },
      "source": [
        "### 4. Generator "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2itRIh2objt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generator receives random noise z and create 1x28x28 image\n",
        "# OrderedDict를 사용해 해당 연산의 이름을 지정할 수 있습니다.\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator,self).__init__()\n",
        "        self.layer1 = nn.Sequential(OrderedDict([   # orderdict: 순서가 저장되는 dictionary\n",
        "                        ('fc1',nn.Linear(z_size,middle_size)),\n",
        "                        ('bn1',nn.BatchNorm1d(middle_size)),\n",
        "                        ('act1',nn.ReLU()),\n",
        "        ]))\n",
        "        self.layer2 = nn.Sequential(OrderedDict([\n",
        "                        ('fc2', nn.Linear(middle_size,784)),\n",
        "                        #('bn2', nn.BatchNorm1d(784)),\n",
        "                        ('tanh', nn.Tanh()),\n",
        "        ]))\n",
        "    def forward(self,z):\n",
        "        out = self.layer1(z)\n",
        "        out = self.layer2(out)\n",
        "        out = out.view(batch_size,1,28,28)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ric2eXWoboA",
        "colab_type": "text"
      },
      "source": [
        "### 5. Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm7sRmJbobsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Discriminator receives 1x28x28 image and returns a float number 0~1\n",
        "# we can name each layer using OrderedDict\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator,self).__init__()\n",
        "        self.layer1 = nn.Sequential(OrderedDict([\n",
        "                        ('fc1',nn.Linear(784,middle_size)),\n",
        "                        #('bn1',nn.BatchNorm1d(middle_size)),\n",
        "                        ('act1',nn.LeakyReLU()),  \n",
        "            \n",
        "        ]))\n",
        "        self.layer2 = nn.Sequential(OrderedDict([\n",
        "                        ('fc2', nn.Linear(middle_size,1)),\n",
        "                        ('bn2', nn.BatchNorm1d(1)),\n",
        "                        ('act2', nn.Sigmoid()),\n",
        "        ]))\n",
        "                                    \n",
        "    def forward(self,x):\n",
        "        out = x.view(batch_size, -1)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3xVMPDJobxN",
        "colab_type": "text"
      },
      "source": [
        "### 6. Put instances on Multi-gpu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "727e_0kkoUdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Put class objects on Multiple GPUs using \n",
        "# torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)\n",
        "# device_ids: default all devices / output_device: default device 0 \n",
        "# along with .cuda()\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "\n",
        "generator = nn.DataParallel(Generator()).to(device)\n",
        "discriminator = nn.DataParallel(Discriminator()).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPVRL_DGouFw",
        "colab_type": "text"
      },
      "source": [
        "### 7. Check layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlBS6SQIouKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get parameter list by using class.state_dict().keys()\n",
        "\n",
        "gen_params = generator.state_dict().keys()\n",
        "dis_params = discriminator.state_dict().keys()\n",
        "\n",
        "for i in gen_params:\n",
        "    print(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7MxWXi6ouXm",
        "colab_type": "text"
      },
      "source": [
        "### 8. Set Loss function & Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvYcok0fouep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss function, optimizers, and labels for training\n",
        "\n",
        "loss_func = nn.MSELoss()\n",
        "gen_optim = torch.optim.Adam(generator.parameters(), lr=learning_rate,betas=(0.5,0.999))\n",
        "dis_optim = torch.optim.Adam(discriminator.parameters(), lr=learning_rate,betas=(0.5,0.999))\n",
        "\n",
        "ones_label = torch.ones(batch_size,1).to(device)\n",
        "zeros_label = torch.zeros(batch_size,1).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVpnFFKFouj6",
        "colab_type": "text"
      },
      "source": [
        "### 9. Restore Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTeRbn8WoucY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model restore if any\n",
        "\n",
        "try:\n",
        "    generator, discriminator = torch.load('./model/vanilla_gan.pkl')\n",
        "    print(\"\\n--------model restored--------\\n\")\n",
        "except:\n",
        "    print(\"\\n--------model not restored--------\\n\")\n",
        "    pass\n",
        "  \n",
        "try:\n",
        "  os.mkdir(\"./model\")\n",
        "except:\n",
        "  pass\n",
        "\n",
        "try:\n",
        "  os.mkdir(\"./result\")\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNgMFjEWouVH",
        "colab_type": "text"
      },
      "source": [
        "### 10. Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN2DEAl6ouTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train\n",
        "\n",
        "for i in range(epoch):\n",
        "    for j,(image,label) in enumerate(train_loader):\n",
        "        image = image.to(device)\n",
        "        \n",
        "        # 구분자 학습\n",
        "        dis_optim.zero_grad()\n",
        "      \n",
        "        # Fake Data \n",
        "        # 랜덤한 z를 샘플링해줍니다.\n",
        "        z = init.normal_(torch.Tensor(batch_size,z_size),mean=0,std=0.1).to(device)\n",
        "        gen_fake = generator.forward(z)\n",
        "        dis_fake = discriminator.forward(gen_fake)\n",
        "        \n",
        "        # Real Data\n",
        "        dis_real = discriminator.forward(image)\n",
        "        \n",
        "        # 두 손실을 더해 최종손실에 대해 기울기 게산을 합니다.\n",
        "        dis_loss = torch.sum(loss_func(dis_fake,zeros_label)) + torch.sum(loss_func(dis_real,ones_label))\n",
        "        dis_loss.backward(retain_graph=True)\n",
        "        dis_optim.step()\n",
        "        \n",
        "        # 생성자 학습\n",
        "        gen_optim.zero_grad()\n",
        "        \n",
        "        # Fake Data\n",
        "        z = init.normal_(torch.Tensor(batch_size,z_size),mean=0,std=0.1).to(device)\n",
        "        gen_fake = generator.forward(z)\n",
        "        dis_fake = discriminator.forward(gen_fake)\n",
        "        \n",
        "        gen_loss = torch.sum(loss_func(dis_fake,ones_label)) # fake classified as real\n",
        "        gen_loss.backward()\n",
        "        gen_optim.step()\n",
        "    \n",
        "        # model save\n",
        "        if j % 100 == 0:\n",
        "            print(gen_loss,dis_loss)\n",
        "            torch.save([generator,discriminator],'./model/vanilla_gan.pkl')            \n",
        "            v_utils.save_image(gen_fake.cpu().data[0:25],\"./result/gen_{}_{}.png\".format(i,j), nrow=5)\n",
        "            print(\"{}th epoch gen_loss: {} dis_loss: {}\".format(i,gen_loss.data,dis_loss.data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBp5_6rasOTn",
        "colab_type": "text"
      },
      "source": [
        "## DCGAN (Deep Convolutional GAN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvXsm39jsTyo",
        "colab_type": "text"
      },
      "source": [
        "### DCGAN이란?\n",
        "- GAN 모델이 그냥 결과를 생성하는 것이 아니라 어떤 의미를 가지는 특성 (혹은 표현)을 학습하여 생성할 수 있다는 것을 보여줌\n",
        "> 학습을 잘 시키는 방법\n",
        " - Pooling 연산을 convolution 연산으로 대체하고 Generator network는 Deconvolution 연산을 사용\n",
        " - Generator와 Discriminator에 Batch normalizaton을 사용\n",
        " - Fully Connected network는 사용하지 않기\n",
        " - Generator network는 마지막에 사용되는 tanh 외에는 모든 activation function에 ReLU를 사용\n",
        " - Discriminator network의 모든 activation function로는 Leaky ReLU를 사용\n",
        "- word2vec에서 단어 벡터 간의 연산이 가능하였던 것처럼, DCGAN은 이미지 벡터 간 연산이 가능함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNAPlxk33Twh",
        "colab_type": "text"
      },
      "source": [
        "## SRGAN (Super-resolution GAN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29KZcCq33XV2",
        "colab_type": "text"
      },
      "source": [
        "### SRGAN이란?\n",
        "- Super-resolution 작업에 GAN을 적용한 모델\n",
        "- SRGAN의 경우 MSE에 추가적으로 생성된 이미지가 진짜 고화질 영상인지 아니면 super-resolution을 거친 이미지인지 구분하는 GAN 손실이 추가로 더해져 고화질 영상의 특성인 선명함이 생성에 영향을 줌"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwxiJbPy3XfV",
        "colab_type": "text"
      },
      "source": [
        "## Pix2Pix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghReEmKi422s",
        "colab_type": "text"
      },
      "source": [
        "### Pix2Pix란?\n",
        "- 이미지를 조건으로 주고 이미지를 생성하게 되는 모델\n",
        "- 실제 쌍 데이터가 필요하다는 단점이 있다.\n",
        "# 새 섹션"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcv9PqxP5MQz",
        "colab_type": "text"
      },
      "source": [
        "## CycleGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THOMYFgf5Oug",
        "colab_type": "text"
      },
      "source": [
        "### CycleGAN이란?\n",
        "- 꼭 상이 없더라도 Pix2Pix처럼 변환이 가능한 모델"
      ]
    }
  ]
}